{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 775,
     "status": "ok",
     "timestamp": 1626643190062,
     "user": {
      "displayName": "Ertuğrul Güngör",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicvIzQgyHfsqD4h979_wDnn11aI-XtLnxmnC_8=s64",
      "userId": "06165302180630202587"
     },
     "user_tz": -180
    },
    "id": "YBHMtNLN3GnS"
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class Identity(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Identity, self).__init__()\n",
    "      \n",
    "  def forward(self, x):\n",
    "    return x\n",
    "\n",
    "def get_vgg_model(num_outputs, init_scale):\n",
    "  vgg16 = models.vgg16()\n",
    "\n",
    "  # Remove avgpool since it's removed in the paper\n",
    "  vgg16.avgpool = Identity()\n",
    "\n",
    "  # In the paper, VGG16 model's 2 last FC layers are removed\n",
    "  # Output of last max pool layer is [batch_size,512,7,7] for 224x224 image\n",
    "  #vgg16.classifier = nn.Linear(512*7*7, num_outputs)\n",
    "\n",
    "  # For 32x32 image\n",
    "  vgg16.classifier = nn.Linear(512, num_outputs)\n",
    "\n",
    "  #for module in vgg16.modules():\n",
    "  #  if type(module) == nn.modules.pooling.MaxPool2d:\n",
    "  #    module = nn.MaxPool2d(2, stride=2, padding='same')\n",
    "    #if \"MaxPool2d\" is in str(type(module)):\n",
    "    #  print(\"module is : {}\".format(type(module)))\n",
    "  # Change weight initialization with a nested function\n",
    "  def init_weights(param):\n",
    "    if (type(param) == nn.Conv2d or type(param) == nn.Linear):\n",
    "      # Parameters are initialized with He initialization in the paper.\n",
    "\n",
    "      torch.nn.init.kaiming_normal_(param.weight, mode='fan_out', nonlinearity='relu')\n",
    "      # # ??\n",
    "      # param.weight = torch.nn.Parameter(param.weight * init_scale)\n",
    "\n",
    "      # print(type(param.weight))\n",
    "      # param.weight.data.fill_(param.weight * init_scale)\n",
    "      # torch.full(param.weight.size(), 3.141592)\n",
    "\n",
    "      # Default value of biases are already 0, just to be more verbose\n",
    "      # param.bias.data.fill_(0)\n",
    "\n",
    "      # print(param)\n",
    "      # print(type(param))\n",
    "\n",
    "  vgg16.apply(init_weights)\n",
    "  return vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7907,
     "status": "ok",
     "timestamp": 1626643197966,
     "user": {
      "displayName": "Ertuğrul Güngör",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicvIzQgyHfsqD4h979_wDnn11aI-XtLnxmnC_8=s64",
      "userId": "06165302180630202587"
     },
     "user_tz": -180
    },
    "id": "JZcqlcdLDtfM",
    "outputId": "7453edbd-e3ab-4b76-a8da-ed9b4634bdc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /opt/conda/include/python3.8/UNKNOWN\n",
      "sysconfig: /opt/conda/include/python3.8\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "Requirement already satisfied: torchinfo in /opt/conda/lib/python3.8/site-packages (1.5.2)\n",
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /opt/conda/include/python3.8/UNKNOWN\n",
      "sysconfig: /opt/conda/include/python3.8\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "VGG                                      --                        --\n",
      "├─Sequential: 1-1                        [1, 512, 1, 1]            --\n",
      "│    └─Conv2d: 2-1                       [1, 64, 32, 32]           1,792\n",
      "│    └─ReLU: 2-2                         [1, 64, 32, 32]           --\n",
      "│    └─Conv2d: 2-3                       [1, 64, 32, 32]           36,928\n",
      "│    └─ReLU: 2-4                         [1, 64, 32, 32]           --\n",
      "│    └─MaxPool2d: 2-5                    [1, 64, 16, 16]           --\n",
      "│    └─Conv2d: 2-6                       [1, 128, 16, 16]          73,856\n",
      "│    └─ReLU: 2-7                         [1, 128, 16, 16]          --\n",
      "│    └─Conv2d: 2-8                       [1, 128, 16, 16]          147,584\n",
      "│    └─ReLU: 2-9                         [1, 128, 16, 16]          --\n",
      "│    └─MaxPool2d: 2-10                   [1, 128, 8, 8]            --\n",
      "│    └─Conv2d: 2-11                      [1, 256, 8, 8]            295,168\n",
      "│    └─ReLU: 2-12                        [1, 256, 8, 8]            --\n",
      "│    └─Conv2d: 2-13                      [1, 256, 8, 8]            590,080\n",
      "│    └─ReLU: 2-14                        [1, 256, 8, 8]            --\n",
      "│    └─Conv2d: 2-15                      [1, 256, 8, 8]            590,080\n",
      "│    └─ReLU: 2-16                        [1, 256, 8, 8]            --\n",
      "│    └─MaxPool2d: 2-17                   [1, 256, 4, 4]            --\n",
      "│    └─Conv2d: 2-18                      [1, 512, 4, 4]            1,180,160\n",
      "│    └─ReLU: 2-19                        [1, 512, 4, 4]            --\n",
      "│    └─Conv2d: 2-20                      [1, 512, 4, 4]            2,359,808\n",
      "│    └─ReLU: 2-21                        [1, 512, 4, 4]            --\n",
      "│    └─Conv2d: 2-22                      [1, 512, 4, 4]            2,359,808\n",
      "│    └─ReLU: 2-23                        [1, 512, 4, 4]            --\n",
      "│    └─MaxPool2d: 2-24                   [1, 512, 2, 2]            --\n",
      "│    └─Conv2d: 2-25                      [1, 512, 2, 2]            2,359,808\n",
      "│    └─ReLU: 2-26                        [1, 512, 2, 2]            --\n",
      "│    └─Conv2d: 2-27                      [1, 512, 2, 2]            2,359,808\n",
      "│    └─ReLU: 2-28                        [1, 512, 2, 2]            --\n",
      "│    └─Conv2d: 2-29                      [1, 512, 2, 2]            2,359,808\n",
      "│    └─ReLU: 2-30                        [1, 512, 2, 2]            --\n",
      "│    └─MaxPool2d: 2-31                   [1, 512, 1, 1]            --\n",
      "├─Identity: 1-2                          [1, 512, 1, 1]            --\n",
      "├─Linear: 1-3                            [1, 13]                   6,669\n",
      "==========================================================================================\n",
      "Total params: 14,721,357\n",
      "Trainable params: 14,721,357\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 313.48\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 2.21\n",
      "Params size (MB): 58.89\n",
      "Estimated Total Size (MB): 61.11\n",
      "==========================================================================================\n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): Identity()\n",
      "  (classifier): Linear(in_features=512, out_features=13, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# install and import the torchinfo library\n",
    "!pip install torchinfo\n",
    "from torchinfo import summary\n",
    "\n",
    "vgg16 = get_vgg_model(13, 1)\n",
    "# batch_size = 256\n",
    "print(summary(vgg16, input_size=(1, 3, 32, 32)))\n",
    "#print(summary(vgg16, input_size=(1, 3, 224, 224)))\n",
    "\n",
    "# summary(vgg16, input_size=(batch_size, 3, 224, 224))\n",
    "# summary(models.vgg16(), input_size=(1, 3, 32, 32))\n",
    "print(vgg16)\n",
    "#vgg16 = models.vgg16()\n",
    "#print(vgg16)\n",
    "\n",
    "\n",
    "# print(vgg16)\n",
    "\n",
    "# vgg16.apply(init_weights)\n",
    "# print(\"dummy\")\n",
    "# for child in vgg16.named_children():\n",
    "#   # print(\"x\")\n",
    "#   print(child)\n",
    "\n",
    "# for name, param in vgg16.named_parameters():\n",
    "#   print(name)\n",
    "#   print(param.size())\n",
    "#   print(param[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1626643197966,
     "user": {
      "displayName": "Ertuğrul Güngör",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicvIzQgyHfsqD4h979_wDnn11aI-XtLnxmnC_8=s64",
      "userId": "06165302180630202587"
     },
     "user_tz": -180
    },
    "id": "6N3MbPVav4wt"
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "def get_cifar10_sets(num_examples_upstream, num_examples_downstream, num_classes_upstream, \\\n",
    "                     num_classes_downstream, is_downstream_random, batch_size):\n",
    "  # @TODO: Check if it's the same as paper\n",
    "  TF = transforms.Compose([\n",
    "      #transforms.Resize(256),\n",
    "      #transforms.CenterCrop(224),\n",
    "      transforms.ToTensor(),\n",
    "      # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "  ])\n",
    "\n",
    "  # num_examples_upstream = 20000\n",
    "  # num_examples_downstream = 10000\n",
    "  # num_classes_upstream = 30\n",
    "  # num_classes_downstream = 15\n",
    "  # is_downstream_random = False\n",
    "\n",
    "  trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=TF)\n",
    "  # test and transform??\n",
    "  testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=TF)\n",
    "\n",
    "  # Create random indices for splitting upstream and downstream tasks\n",
    "  indices = np.arange(0,len(trainset))\n",
    "  np.random.seed(12345)\n",
    "  np.random.shuffle(indices)\n",
    "  indices = indices.tolist()\n",
    "\n",
    "  # Make upstream labels random\n",
    "  temp = np.array(trainset.targets)\n",
    "  temp[indices[:num_examples_upstream]] = np.random.randint(num_classes_upstream, size=num_examples_upstream)\n",
    "\n",
    "  # If the downstream task uses random labels\n",
    "  if is_downstream_random:\n",
    "    temp[indices[-num_examples_downstream:]] = np.random.randint(num_classes_downstream, size=num_examples_downstream).tolist()\n",
    "\n",
    "  trainset.targets = [int(label) for label in temp]\n",
    "\n",
    "  # Build dataloaders for upstream and downstream tasks\n",
    "  train_upstream_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=False,\n",
    "                                                sampler=torch.utils.data.SubsetRandomSampler(indices[:num_examples_upstream]))\n",
    "\n",
    "  train_downstream_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=False,\n",
    "                                                sampler=torch.utils.data.SubsetRandomSampler(indices[-num_examples_downstream:]))\n",
    "\n",
    "  test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "  # To check the random labels\n",
    "  '''\n",
    "  u_batch = next(iter(train_upstream_loader))\n",
    "  d_batch = next(iter(train_downstream_loader))\n",
    "\n",
    "  u_labels = u_batch[1]\n",
    "  print(u_labels)\n",
    "  d_labels = d_batch[1]\n",
    "  print(d_labels)\n",
    "  '''\n",
    "  return (train_upstream_loader, train_downstream_loader, test_loader)\n",
    "\n",
    "\n",
    "def get_cifar10_orig():\n",
    "  transform = transforms.Compose(\n",
    "      [transforms.ToTensor(),\n",
    "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "  batch_size = 256\n",
    "\n",
    "\n",
    "  trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                          download=False, transform=transform)\n",
    "  indices = np.arange(0,len(trainset))\n",
    "  trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=2, \n",
    "                                            sampler=torch.utils.data.SubsetRandomSampler(indices[:5000]))\n",
    "\n",
    "  testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=False, transform=transform)\n",
    "  testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "  return trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1626643197967,
     "user": {
      "displayName": "Ertuğrul Güngör",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicvIzQgyHfsqD4h979_wDnn11aI-XtLnxmnC_8=s64",
      "userId": "06165302180630202587"
     },
     "user_tz": -180
    },
    "id": "oxh4UOW0g5rq"
   },
   "outputs": [],
   "source": [
    "def accuracy(model, data=\"train\"):\n",
    "  correct = 0\n",
    "  total = 0\n",
    "\n",
    "  if data== \"train\":\n",
    "    _, dataloader, __ = get_cifar10_sets(\n",
    "        num_examples_upstream = 5000,\n",
    "        num_examples_downstream = 20000,\n",
    "        num_classes_upstream = 5,\n",
    "        num_classes_downstream = 10,  \n",
    "        is_downstream_random = False,\n",
    "        batch_size = 20000\n",
    "    )\n",
    "  elif data == \"test\":\n",
    "    _, __, dataloader = get_cifar10_sets(\n",
    "        num_examples_upstream = 5000,\n",
    "        num_examples_downstream = 20000,\n",
    "        num_classes_upstream = 5,\n",
    "        num_classes_downstream = 10,  \n",
    "        is_downstream_random = False,\n",
    "        batch_size = 10000\n",
    "    )\n",
    "  else:\n",
    "    print(\"choose a proper data mode\")\n",
    "    return\n",
    "  with torch.no_grad():\n",
    "    for data in dataloader:\n",
    "      inputs, labels = data\n",
    "      outputs = model(inputs)\n",
    "      _, predicted = torch.max(outputs.data, 1)\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted == labels).sum().item()\n",
    "\n",
    "  return (correct / total)\n",
    "\n",
    "def train(model, criterion, optimizer, epochs, dataloader, device, scheduler=None, acc_mode=\"none\", verbose=True):\n",
    "  loss_history = []\n",
    "  train_accuracies = []\n",
    "  test_accuracies = []\n",
    "  for epoch in range(1,epochs+1):\n",
    "    count = 0\n",
    "    for i, data in enumerate(dataloader, 0):     \n",
    "      # print(\"{}.th batch\".format(i))\n",
    "      # Our batch:\n",
    "      inputs, labels = data\n",
    "      inputs = inputs.to(device)\n",
    "      labels = labels.to(device)\n",
    "      # print(\"batching!\")\n",
    "\n",
    "      # zero the gradients as PyTorch accumulates them\n",
    "      optimizer.zero_grad()\n",
    "      # print(\"zero grad!\")\n",
    "\n",
    "      # Obtain the scores\n",
    "      outputs = model(inputs)\n",
    "      # print(\"feedforward!\")\n",
    "\n",
    "      # Calculate loss\n",
    "      loss = criterion(outputs.to(device), labels)\n",
    "      # print(\"loss!\")\n",
    "\n",
    "      # Backpropagate\n",
    "      loss.backward()\n",
    "      # print(\"backward!\")\n",
    "\n",
    "      # Update the weights\n",
    "      optimizer.step()\n",
    "      # print(\"update!\")\n",
    "      \n",
    "      loss_history.append(loss.item())\n",
    "      count += 1\n",
    "      #print(\"mini batch counter: {}\".format(count))\n",
    "      if (count % 15 == 0):\n",
    "          if acc_mode==\"both\":\n",
    "            train_acc = accuracy(model, data=\"train\")\n",
    "            train_accuracies.append(train_acc)\n",
    "            test_acc = accuracy(model, data=\"test\")\n",
    "            test_accuracies.append(test_acc)\n",
    "          elif acc_mode==\"train\":\n",
    "            train_acc = accuracy(model, data=\"train\")\n",
    "            train_accuracies.append(train_acc)\n",
    "          elif acc_mode==\"test\":\n",
    "            test_acc = accuracy(model, data=\"test\")\n",
    "            test_accuracies.append(test_acc)\n",
    "    \n",
    "    if verbose: \n",
    "        print(f'Epoch {epoch} / {epochs}: avg. loss of last 5 iterations {np.sum(loss_history[:-6:-1])/5}')\n",
    "        if acc_mode==\"train\" or acc_mode ==\"both\":\n",
    "          print(f'Epoch {epoch} / {epochs}: Last train acc:{train_acc}')\n",
    "        if acc_mode==\"test\" or acc_mode ==\"both\":\n",
    "          print(f'Epoch {epoch} / {epochs}: Last test acc:{test_acc}')\n",
    "\n",
    "    if scheduler is not None:\n",
    "      scheduler.step()\n",
    "      # Print Learning Rate\n",
    "      print('Epoch:', epoch,'LR:', scheduler.get_last_lr())\n",
    "\n",
    "  if acc_mode==\"both\":\n",
    "    return loss_history, train_accuracies, test_accuracies\n",
    "  elif acc_mode==\"train\":\n",
    "    return loss_history, train_accuracies\n",
    "  elif acc_mode==\"test\":\n",
    "    return loss_history, test_accuracies\n",
    "  return loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1626643197968,
     "user": {
      "displayName": "Ertuğrul Güngör",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicvIzQgyHfsqD4h979_wDnn11aI-XtLnxmnC_8=s64",
      "userId": "06165302180630202587"
     },
     "user_tz": -180
    },
    "id": "3hhJodbVnyOi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def get_training_components(learnable_params, init_lr, step_size):\n",
    "  '''\n",
    "  Function to prepare components of the training. These are all the same\n",
    "  throughout the experiments made.\n",
    "  '''\n",
    "  # The problem always be a image classification\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  # Which parameters to be updated depends on if the training is done from scratch\n",
    "  # or with pre-trained model.\n",
    "  optimizer = optim.SGD(learnable_params, lr=init_lr, momentum=0.9)\n",
    "  # Step size depends on the # of epoch, gamma is contant\n",
    "  scheduler = StepLR(optimizer, step_size=step_size, gamma=1/3)\n",
    "  return (criterion, optimizer, scheduler)\n",
    "\n",
    "def get_device():\n",
    "  if torch.cuda.is_available():\n",
    "    print(\"Cuda (GPU support) is available and enabled!\")\n",
    "    device = torch.device(\"cuda\")\n",
    "  else:\n",
    "    print(\"Cuda (GPU support) is not available :(\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "def get_learnable_parameters(model):\n",
    "  params_to_update = []\n",
    "  for name, param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "      #print(name)\n",
    "      params_to_update.append(param)\n",
    "  return params_to_update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kEy0gUI0n6_p",
    "outputId": "bfb71388-9147-4cde-cda6-e32f82f6753f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda (GPU support) is available and enabled!\n",
      "Epoch 1 / 120: avg. loss of last 5 iterations 1.6133390665054321\n",
      "Epoch: 1 LR: [0.001]\n",
      "Epoch 2 / 120: avg. loss of last 5 iterations 1.6077460765838623\n",
      "Epoch: 2 LR: [0.001]\n",
      "Epoch 3 / 120: avg. loss of last 5 iterations 1.605721402168274\n",
      "Epoch: 3 LR: [0.001]\n",
      "Epoch 4 / 120: avg. loss of last 5 iterations 1.6093560218811036\n",
      "Epoch: 4 LR: [0.001]\n",
      "Epoch 5 / 120: avg. loss of last 5 iterations 1.6039263248443603\n",
      "Epoch: 5 LR: [0.001]\n",
      "Epoch 6 / 120: avg. loss of last 5 iterations 1.5962279558181762\n",
      "Epoch: 6 LR: [0.001]\n",
      "Epoch 7 / 120: avg. loss of last 5 iterations 1.606328511238098\n",
      "Epoch: 7 LR: [0.001]\n",
      "Epoch 8 / 120: avg. loss of last 5 iterations 1.5934141874313354\n",
      "Epoch: 8 LR: [0.001]\n",
      "Epoch 9 / 120: avg. loss of last 5 iterations 1.5921808719635009\n",
      "Epoch: 9 LR: [0.001]\n",
      "Epoch 10 / 120: avg. loss of last 5 iterations 1.5870641946792603\n",
      "Epoch: 10 LR: [0.001]\n",
      "Epoch 11 / 120: avg. loss of last 5 iterations 1.585596537590027\n",
      "Epoch: 11 LR: [0.001]\n",
      "Epoch 12 / 120: avg. loss of last 5 iterations 1.5687595844268798\n",
      "Epoch: 12 LR: [0.001]\n",
      "Epoch 13 / 120: avg. loss of last 5 iterations 1.5809925079345704\n",
      "Epoch: 13 LR: [0.001]\n",
      "Epoch 14 / 120: avg. loss of last 5 iterations 1.5783725023269652\n",
      "Epoch: 14 LR: [0.001]\n",
      "Epoch 15 / 120: avg. loss of last 5 iterations 1.5735558032989503\n",
      "Epoch: 15 LR: [0.001]\n",
      "Epoch 16 / 120: avg. loss of last 5 iterations 1.5668370008468628\n",
      "Epoch: 16 LR: [0.001]\n",
      "Epoch 17 / 120: avg. loss of last 5 iterations 1.5739986181259156\n",
      "Epoch: 17 LR: [0.001]\n",
      "Epoch 18 / 120: avg. loss of last 5 iterations 1.5552118301391602\n",
      "Epoch: 18 LR: [0.001]\n",
      "Epoch 19 / 120: avg. loss of last 5 iterations 1.5475941896438599\n",
      "Epoch: 19 LR: [0.001]\n",
      "Epoch 20 / 120: avg. loss of last 5 iterations 1.5235903978347778\n",
      "Epoch: 20 LR: [0.001]\n",
      "Epoch 21 / 120: avg. loss of last 5 iterations 1.5242175102233886\n",
      "Epoch: 21 LR: [0.001]\n",
      "Epoch 22 / 120: avg. loss of last 5 iterations 1.5049087285995484\n",
      "Epoch: 22 LR: [0.001]\n",
      "Epoch 23 / 120: avg. loss of last 5 iterations 1.4980645895004272\n",
      "Epoch: 23 LR: [0.001]\n",
      "Epoch 24 / 120: avg. loss of last 5 iterations 1.420781183242798\n",
      "Epoch: 24 LR: [0.001]\n",
      "Epoch 25 / 120: avg. loss of last 5 iterations 1.454353666305542\n",
      "Epoch: 25 LR: [0.001]\n",
      "Epoch 26 / 120: avg. loss of last 5 iterations 1.3920888662338258\n",
      "Epoch: 26 LR: [0.001]\n",
      "Epoch 27 / 120: avg. loss of last 5 iterations 1.3393589496612548\n",
      "Epoch: 27 LR: [0.001]\n",
      "Epoch 28 / 120: avg. loss of last 5 iterations 1.4434041738510133\n",
      "Epoch: 28 LR: [0.001]\n",
      "Epoch 29 / 120: avg. loss of last 5 iterations 1.3439536333084106\n",
      "Epoch: 29 LR: [0.001]\n",
      "Epoch 30 / 120: avg. loss of last 5 iterations 1.2786612033843994\n",
      "Epoch: 30 LR: [0.001]\n",
      "Epoch 31 / 120: avg. loss of last 5 iterations 1.1995983123779297\n",
      "Epoch: 31 LR: [0.001]\n",
      "Epoch 32 / 120: avg. loss of last 5 iterations 1.141989493370056\n",
      "Epoch: 32 LR: [0.001]\n",
      "Epoch 33 / 120: avg. loss of last 5 iterations 1.1191924333572387\n",
      "Epoch: 33 LR: [0.001]\n",
      "Epoch 34 / 120: avg. loss of last 5 iterations 0.941384243965149\n",
      "Epoch: 34 LR: [0.001]\n",
      "Epoch 35 / 120: avg. loss of last 5 iterations 1.0341204404830933\n",
      "Epoch: 35 LR: [0.001]\n",
      "Epoch 36 / 120: avg. loss of last 5 iterations 0.9813701748847962\n",
      "Epoch: 36 LR: [0.001]\n",
      "Epoch 37 / 120: avg. loss of last 5 iterations 0.9093497514724731\n",
      "Epoch: 37 LR: [0.001]\n",
      "Epoch 38 / 120: avg. loss of last 5 iterations 0.7521415710449219\n",
      "Epoch: 38 LR: [0.001]\n",
      "Epoch 39 / 120: avg. loss of last 5 iterations 0.7433053731918335\n",
      "Epoch: 39 LR: [0.001]\n",
      "Epoch 40 / 120: avg. loss of last 5 iterations 0.5083985984325409\n",
      "Epoch: 40 LR: [0.0003333333333333333]\n",
      "Epoch 41 / 120: avg. loss of last 5 iterations 0.16531588435173034\n",
      "Epoch: 41 LR: [0.0003333333333333333]\n",
      "Epoch 42 / 120: avg. loss of last 5 iterations 0.08771532624959946\n",
      "Epoch: 42 LR: [0.0003333333333333333]\n",
      "Epoch 43 / 120: avg. loss of last 5 iterations 0.06177257373929024\n",
      "Epoch: 43 LR: [0.0003333333333333333]\n",
      "Epoch 44 / 120: avg. loss of last 5 iterations 0.05253495424985886\n",
      "Epoch: 44 LR: [0.0003333333333333333]\n",
      "Epoch 45 / 120: avg. loss of last 5 iterations 0.0317880779504776\n",
      "Epoch: 45 LR: [0.0003333333333333333]\n",
      "Epoch 46 / 120: avg. loss of last 5 iterations 0.023367162607610226\n",
      "Epoch: 46 LR: [0.0003333333333333333]\n",
      "Epoch 47 / 120: avg. loss of last 5 iterations 0.020608646981418132\n",
      "Epoch: 47 LR: [0.0003333333333333333]\n",
      "Epoch 48 / 120: avg. loss of last 5 iterations 0.013434073887765408\n",
      "Epoch: 48 LR: [0.0003333333333333333]\n",
      "Epoch 49 / 120: avg. loss of last 5 iterations 0.010446204245090485\n",
      "Epoch: 49 LR: [0.0003333333333333333]\n",
      "Epoch 50 / 120: avg. loss of last 5 iterations 0.010063219256699086\n",
      "Epoch: 50 LR: [0.0003333333333333333]\n",
      "Epoch 51 / 120: avg. loss of last 5 iterations 0.005871854070574045\n",
      "Epoch: 51 LR: [0.0003333333333333333]\n",
      "Epoch 52 / 120: avg. loss of last 5 iterations 0.0058335836976766585\n",
      "Epoch: 52 LR: [0.0003333333333333333]\n",
      "Epoch 53 / 120: avg. loss of last 5 iterations 0.005639694817364216\n",
      "Epoch: 53 LR: [0.0003333333333333333]\n",
      "Epoch 54 / 120: avg. loss of last 5 iterations 0.00445226514711976\n",
      "Epoch: 54 LR: [0.0003333333333333333]\n",
      "Epoch 55 / 120: avg. loss of last 5 iterations 0.005265107844024897\n",
      "Epoch: 55 LR: [0.0003333333333333333]\n",
      "Epoch 56 / 120: avg. loss of last 5 iterations 0.003107399633154273\n",
      "Epoch: 56 LR: [0.0003333333333333333]\n",
      "Epoch 57 / 120: avg. loss of last 5 iterations 0.004198432713747024\n",
      "Epoch: 57 LR: [0.0003333333333333333]\n",
      "Epoch 58 / 120: avg. loss of last 5 iterations 0.004680575849488377\n",
      "Epoch: 58 LR: [0.0003333333333333333]\n",
      "Epoch 59 / 120: avg. loss of last 5 iterations 0.0037512749433517454\n",
      "Epoch: 59 LR: [0.0003333333333333333]\n",
      "Epoch 60 / 120: avg. loss of last 5 iterations 0.003122447314672172\n",
      "Epoch: 60 LR: [0.0003333333333333333]\n",
      "Epoch 61 / 120: avg. loss of last 5 iterations 0.002919738367199898\n",
      "Epoch: 61 LR: [0.0003333333333333333]\n",
      "Epoch 62 / 120: avg. loss of last 5 iterations 0.0022822163300588727\n",
      "Epoch: 62 LR: [0.0003333333333333333]\n",
      "Epoch 63 / 120: avg. loss of last 5 iterations 0.002941936859861016\n",
      "Epoch: 63 LR: [0.0003333333333333333]\n",
      "Epoch 64 / 120: avg. loss of last 5 iterations 0.0021619923179969193\n",
      "Epoch: 64 LR: [0.0003333333333333333]\n",
      "Epoch 65 / 120: avg. loss of last 5 iterations 0.0025573860853910445\n",
      "Epoch: 65 LR: [0.0003333333333333333]\n",
      "Epoch 66 / 120: avg. loss of last 5 iterations 0.0022911303443834186\n",
      "Epoch: 66 LR: [0.0003333333333333333]\n",
      "Epoch 67 / 120: avg. loss of last 5 iterations 0.0024990621022880077\n",
      "Epoch: 67 LR: [0.0003333333333333333]\n",
      "Epoch 68 / 120: avg. loss of last 5 iterations 0.0019228694960474967\n",
      "Epoch: 68 LR: [0.0003333333333333333]\n",
      "Epoch 69 / 120: avg. loss of last 5 iterations 0.0017832560581155122\n",
      "Epoch: 69 LR: [0.0003333333333333333]\n",
      "Epoch 70 / 120: avg. loss of last 5 iterations 0.0015876588295213878\n",
      "Epoch: 70 LR: [0.0003333333333333333]\n",
      "Epoch 71 / 120: avg. loss of last 5 iterations 0.0017819527070969342\n",
      "Epoch: 71 LR: [0.0003333333333333333]\n",
      "Epoch 72 / 120: avg. loss of last 5 iterations 0.0015759447356685995\n",
      "Epoch: 72 LR: [0.0003333333333333333]\n",
      "Epoch 73 / 120: avg. loss of last 5 iterations 0.0014344155322760343\n",
      "Epoch: 73 LR: [0.0003333333333333333]\n",
      "Epoch 74 / 120: avg. loss of last 5 iterations 0.001556929771322757\n",
      "Epoch: 74 LR: [0.0003333333333333333]\n",
      "Epoch 75 / 120: avg. loss of last 5 iterations 0.0014289276441559196\n",
      "Epoch: 75 LR: [0.0003333333333333333]\n",
      "Epoch 76 / 120: avg. loss of last 5 iterations 0.0013736643828451633\n",
      "Epoch: 76 LR: [0.0003333333333333333]\n",
      "Epoch 77 / 120: avg. loss of last 5 iterations 0.0015072840498760343\n",
      "Epoch: 77 LR: [0.0003333333333333333]\n",
      "Epoch 78 / 120: avg. loss of last 5 iterations 0.0012448810273781418\n",
      "Epoch: 78 LR: [0.0003333333333333333]\n",
      "Epoch 79 / 120: avg. loss of last 5 iterations 0.0011751772835850716\n",
      "Epoch: 79 LR: [0.0003333333333333333]\n",
      "Epoch 80 / 120: avg. loss of last 5 iterations 0.001176391076296568\n",
      "Epoch: 80 LR: [0.0001111111111111111]\n",
      "Epoch 81 / 120: avg. loss of last 5 iterations 0.0011303410166874528\n",
      "Epoch: 81 LR: [0.0001111111111111111]\n",
      "Epoch 82 / 120: avg. loss of last 5 iterations 0.0012265273137018085\n",
      "Epoch: 82 LR: [0.0001111111111111111]\n",
      "Epoch 83 / 120: avg. loss of last 5 iterations 0.0012022346490994096\n",
      "Epoch: 83 LR: [0.0001111111111111111]\n",
      "Epoch 84 / 120: avg. loss of last 5 iterations 0.0013010873226448894\n",
      "Epoch: 84 LR: [0.0001111111111111111]\n",
      "Epoch 85 / 120: avg. loss of last 5 iterations 0.0012377497972920538\n",
      "Epoch: 85 LR: [0.0001111111111111111]\n",
      "Epoch 86 / 120: avg. loss of last 5 iterations 0.0012621544301509857\n",
      "Epoch: 86 LR: [0.0001111111111111111]\n",
      "Epoch 87 / 120: avg. loss of last 5 iterations 0.001103963830973953\n",
      "Epoch: 87 LR: [0.0001111111111111111]\n",
      "Epoch 88 / 120: avg. loss of last 5 iterations 0.0010658274055458604\n",
      "Epoch: 88 LR: [0.0001111111111111111]\n",
      "Epoch 89 / 120: avg. loss of last 5 iterations 0.0009763428883161396\n",
      "Epoch: 89 LR: [0.0001111111111111111]\n",
      "Epoch 90 / 120: avg. loss of last 5 iterations 0.000948563520796597\n",
      "Epoch: 90 LR: [0.0001111111111111111]\n",
      "Epoch 91 / 120: avg. loss of last 5 iterations 0.0010439090081490576\n",
      "Epoch: 91 LR: [0.0001111111111111111]\n",
      "Epoch 92 / 120: avg. loss of last 5 iterations 0.0011610556743107737\n",
      "Epoch: 92 LR: [0.0001111111111111111]\n",
      "Epoch 93 / 120: avg. loss of last 5 iterations 0.0009884675848297775\n",
      "Epoch: 93 LR: [0.0001111111111111111]\n",
      "Epoch 94 / 120: avg. loss of last 5 iterations 0.000958879035897553\n",
      "Epoch: 94 LR: [0.0001111111111111111]\n",
      "Epoch 95 / 120: avg. loss of last 5 iterations 0.0009473009500652552\n",
      "Epoch: 95 LR: [0.0001111111111111111]\n",
      "Epoch 96 / 120: avg. loss of last 5 iterations 0.001093560189474374\n",
      "Epoch: 96 LR: [0.0001111111111111111]\n",
      "Epoch 97 / 120: avg. loss of last 5 iterations 0.0009911023429594935\n",
      "Epoch: 97 LR: [0.0001111111111111111]\n",
      "Epoch 98 / 120: avg. loss of last 5 iterations 0.0012914520455524325\n",
      "Epoch: 98 LR: [0.0001111111111111111]\n",
      "Epoch 99 / 120: avg. loss of last 5 iterations 0.0009948133258149028\n",
      "Epoch: 99 LR: [0.0001111111111111111]\n",
      "Epoch 100 / 120: avg. loss of last 5 iterations 0.000973532721400261\n",
      "Epoch: 100 LR: [0.0001111111111111111]\n",
      "Epoch 101 / 120: avg. loss of last 5 iterations 0.0009923173813149333\n",
      "Epoch: 101 LR: [0.0001111111111111111]\n",
      "Epoch 102 / 120: avg. loss of last 5 iterations 0.0009507615119218827\n",
      "Epoch: 102 LR: [0.0001111111111111111]\n",
      "Epoch 103 / 120: avg. loss of last 5 iterations 0.0010904768481850625\n",
      "Epoch: 103 LR: [0.0001111111111111111]\n",
      "Epoch 104 / 120: avg. loss of last 5 iterations 0.0009849007590673863\n",
      "Epoch: 104 LR: [0.0001111111111111111]\n",
      "Epoch 105 / 120: avg. loss of last 5 iterations 0.0008487807004712522\n",
      "Epoch: 105 LR: [0.0001111111111111111]\n",
      "Epoch 106 / 120: avg. loss of last 5 iterations 0.000931233202572912\n",
      "Epoch: 106 LR: [0.0001111111111111111]\n",
      "Epoch 107 / 120: avg. loss of last 5 iterations 0.001007501222193241\n",
      "Epoch: 107 LR: [0.0001111111111111111]\n",
      "Epoch 108 / 120: avg. loss of last 5 iterations 0.0008692001225426793\n",
      "Epoch: 108 LR: [0.0001111111111111111]\n",
      "Epoch 109 / 120: avg. loss of last 5 iterations 0.0009375131106935441\n",
      "Epoch: 109 LR: [0.0001111111111111111]\n",
      "Epoch 110 / 120: avg. loss of last 5 iterations 0.0010114225442521274\n",
      "Epoch: 110 LR: [0.0001111111111111111]\n",
      "Epoch 111 / 120: avg. loss of last 5 iterations 0.0008923195651732385\n",
      "Epoch: 111 LR: [0.0001111111111111111]\n",
      "Epoch 112 / 120: avg. loss of last 5 iterations 0.000938081310596317\n",
      "Epoch: 112 LR: [0.0001111111111111111]\n",
      "Epoch 113 / 120: avg. loss of last 5 iterations 0.0008143839775584638\n",
      "Epoch: 113 LR: [0.0001111111111111111]\n",
      "Epoch 114 / 120: avg. loss of last 5 iterations 0.000898011855315417\n",
      "Epoch: 114 LR: [0.0001111111111111111]\n",
      "Epoch 115 / 120: avg. loss of last 5 iterations 0.0008408130262978375\n",
      "Epoch: 115 LR: [0.0001111111111111111]\n",
      "Epoch 116 / 120: avg. loss of last 5 iterations 0.0008666026871651411\n",
      "Epoch: 116 LR: [0.0001111111111111111]\n",
      "Epoch 117 / 120: avg. loss of last 5 iterations 0.000956130085978657\n",
      "Epoch: 117 LR: [0.0001111111111111111]\n",
      "Epoch 118 / 120: avg. loss of last 5 iterations 0.0008709275512956083\n",
      "Epoch: 118 LR: [0.0001111111111111111]\n",
      "Epoch 119 / 120: avg. loss of last 5 iterations 0.0009223309927619994\n",
      "Epoch: 119 LR: [0.0001111111111111111]\n",
      "Epoch 120 / 120: avg. loss of last 5 iterations 0.0007970271864905954\n",
      "Epoch: 120 LR: [3.703703703703703e-05]\n"
     ]
    }
   ],
   "source": [
    "from torch import linalg as LA\n",
    "\n",
    "train_upstream_loader, train_downstream_loader, test_loader = get_cifar10_sets(\n",
    "    num_examples_upstream = 20000,\n",
    "    num_examples_downstream = 20000,\n",
    "    num_classes_upstream = 5,\n",
    "    num_classes_downstream = 5,\n",
    "    is_downstream_random = False,\n",
    "    batch_size = 256\n",
    "  )\n",
    "\n",
    "model = get_vgg_model(5, 1)\n",
    "norm_dict = {}\n",
    "for name, param in model.named_parameters():\n",
    "  norm_dict[name] = LA.norm(param)\n",
    "\n",
    "epochs = 120\n",
    "learnable_parameters = get_learnable_parameters(model)\n",
    "criterion, optimizer, scheduler = get_training_components(learnable_parameters, 0.001, (int)(epochs/3))\n",
    "\n",
    "device = get_device()\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "loss_history = train(model, criterion, optimizer, epochs, train_upstream_loader, device, scheduler=scheduler)\n",
    "\n",
    "# scale norms back to initial values after upstream training\n",
    "for name, param in model.named_parameters():\n",
    "  scale = norm_dict[name]\n",
    "  param.data.copy_(param*(scale.item()/LA.norm(param)))\n",
    "\n",
    "torch.save(model.state_dict(), \"./pretrained_model_dict_upstream_1_2.pt\")\n",
    "torch.save(model, \"./pretrained_model_upstream_1_2.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def setup_and_train_downstream(epochs, lr, step_size, num_examples_upstream, num_examples_downstream,\n",
    "                   num_classes_upstream, num_classes_downstream, is_downstream_random,\n",
    "                   batch_size, num_outputs, upstream_model_path=None):\n",
    "    \n",
    "    train_upstream_loader, train_downstream_loader, test_loader = get_cifar10_sets(\n",
    "        num_examples_upstream, num_examples_downstream, num_classes_upstream,\n",
    "        num_classes_downstream, is_downstream_random, batch_size)\n",
    "\n",
    "    model = get_vgg_model(num_classes_downstream, 1)\n",
    "    if upstream_model_path is not None:\n",
    "        model.load_state_dict(torch.load(upstream_model_path), strict=False)\n",
    "        model.classifier = nn.Linear(512, num_outputs)\n",
    "        torch.nn.init.kaiming_normal_(model.classifier.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    learnable_parameters = get_learnable_parameters(model)\n",
    "    criterion, optimizer, scheduler = get_training_components(learnable_parameters, lr, step_size)\n",
    "    device = get_device()\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    if not is_downstream_random:\n",
    "      loss_history, train_accuracies, test_accuracies = train(model, criterion, optimizer, epochs, train_downstream_loader, \n",
    "                                                              device, scheduler=scheduler, acc_mode=\"both\")\n",
    "      return loss_history, train_accuracies, test_accuracies\n",
    "    loss_history, train_accuracies = train(model, criterion, optimizer, epochs, train_downstream_loader, \n",
    "                                           device, scheduler=scheduler, acc_mode=\"train\")\n",
    "    return loss_history, train_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda (GPU support) is available and enabled!\n",
      "Epoch 1 / 40: avg. loss of last 5 iterations 1.9065651416778564\n",
      "Epoch 1 / 40: Last train acc:0.29745\n",
      "Epoch 1 / 40: Last test acc:0.3024\n",
      "Epoch: 1 LR: [0.001]\n",
      "Epoch 2 / 40: avg. loss of last 5 iterations 1.6211234331130981\n",
      "Epoch 2 / 40: Last train acc:0.42845\n",
      "Epoch 2 / 40: Last test acc:0.428\n",
      "Epoch: 2 LR: [0.001]\n",
      "Epoch 3 / 40: avg. loss of last 5 iterations 1.5648954391479493\n",
      "Epoch 3 / 40: Last train acc:0.42495\n",
      "Epoch 3 / 40: Last test acc:0.4159\n",
      "Epoch: 3 LR: [0.001]\n",
      "Epoch 4 / 40: avg. loss of last 5 iterations 1.3323709487915039\n",
      "Epoch 4 / 40: Last train acc:0.51525\n",
      "Epoch 4 / 40: Last test acc:0.4925\n",
      "Epoch: 4 LR: [0.001]\n",
      "Epoch 5 / 40: avg. loss of last 5 iterations 1.3845954179763793\n",
      "Epoch 5 / 40: Last train acc:0.5284\n",
      "Epoch 5 / 40: Last test acc:0.5056\n",
      "Epoch: 5 LR: [0.001]\n",
      "Epoch 6 / 40: avg. loss of last 5 iterations 1.308006715774536\n",
      "Epoch 6 / 40: Last train acc:0.5417\n",
      "Epoch 6 / 40: Last test acc:0.5077\n",
      "Epoch: 6 LR: [0.001]\n",
      "Epoch 7 / 40: avg. loss of last 5 iterations 1.258984589576721\n",
      "Epoch 7 / 40: Last train acc:0.5807\n",
      "Epoch 7 / 40: Last test acc:0.5383\n",
      "Epoch: 7 LR: [0.001]\n",
      "Epoch 8 / 40: avg. loss of last 5 iterations 1.1498520851135254\n",
      "Epoch 8 / 40: Last train acc:0.607\n",
      "Epoch 8 / 40: Last test acc:0.5521\n",
      "Epoch: 8 LR: [0.001]\n",
      "Epoch 9 / 40: avg. loss of last 5 iterations 1.0485374808311463\n",
      "Epoch 9 / 40: Last train acc:0.5965\n",
      "Epoch 9 / 40: Last test acc:0.5424\n",
      "Epoch: 9 LR: [0.001]\n",
      "Epoch 10 / 40: avg. loss of last 5 iterations 1.167632246017456\n",
      "Epoch 10 / 40: Last train acc:0.6491\n",
      "Epoch 10 / 40: Last test acc:0.5827\n",
      "Epoch: 10 LR: [0.001]\n",
      "Epoch 11 / 40: avg. loss of last 5 iterations 1.0720869302749634\n",
      "Epoch 11 / 40: Last train acc:0.6713\n",
      "Epoch 11 / 40: Last test acc:0.5856\n",
      "Epoch: 11 LR: [0.001]\n",
      "Epoch 12 / 40: avg. loss of last 5 iterations 0.9823662519454956\n",
      "Epoch 12 / 40: Last train acc:0.7016\n",
      "Epoch 12 / 40: Last test acc:0.6023\n",
      "Epoch: 12 LR: [0.001]\n",
      "Epoch 13 / 40: avg. loss of last 5 iterations 0.9022251963615417\n",
      "Epoch 13 / 40: Last train acc:0.6841\n",
      "Epoch 13 / 40: Last test acc:0.5808\n",
      "Epoch: 13 LR: [0.0003333333333333333]\n",
      "Epoch 14 / 40: avg. loss of last 5 iterations 0.7242270946502686\n",
      "Epoch 14 / 40: Last train acc:0.77335\n",
      "Epoch 14 / 40: Last test acc:0.6211\n",
      "Epoch: 14 LR: [0.0003333333333333333]\n",
      "Epoch 15 / 40: avg. loss of last 5 iterations 0.6802717924118042\n",
      "Epoch 15 / 40: Last train acc:0.78345\n",
      "Epoch 15 / 40: Last test acc:0.6227\n",
      "Epoch: 15 LR: [0.0003333333333333333]\n",
      "Epoch 16 / 40: avg. loss of last 5 iterations 0.5834593951702118\n",
      "Epoch 16 / 40: Last train acc:0.7901\n",
      "Epoch 16 / 40: Last test acc:0.6122\n",
      "Epoch: 16 LR: [0.0003333333333333333]\n",
      "Epoch 17 / 40: avg. loss of last 5 iterations 0.6310210227966309\n",
      "Epoch 17 / 40: Last train acc:0.829\n",
      "Epoch 17 / 40: Last test acc:0.631\n",
      "Epoch: 17 LR: [0.0003333333333333333]\n",
      "Epoch 18 / 40: avg. loss of last 5 iterations 0.5477145910263062\n",
      "Epoch 18 / 40: Last train acc:0.82415\n",
      "Epoch 18 / 40: Last test acc:0.6269\n",
      "Epoch: 18 LR: [0.0003333333333333333]\n",
      "Epoch 19 / 40: avg. loss of last 5 iterations 0.521478682756424\n",
      "Epoch 19 / 40: Last train acc:0.8443\n",
      "Epoch 19 / 40: Last test acc:0.6231\n",
      "Epoch: 19 LR: [0.0003333333333333333]\n",
      "Epoch 20 / 40: avg. loss of last 5 iterations 0.5333707571029663\n",
      "Epoch 20 / 40: Last train acc:0.8461\n",
      "Epoch 20 / 40: Last test acc:0.6124\n",
      "Epoch: 20 LR: [0.0003333333333333333]\n",
      "Epoch 21 / 40: avg. loss of last 5 iterations 0.43401015996932985\n",
      "Epoch 21 / 40: Last train acc:0.89015\n",
      "Epoch 21 / 40: Last test acc:0.6281\n",
      "Epoch: 21 LR: [0.0003333333333333333]\n",
      "Epoch 22 / 40: avg. loss of last 5 iterations 0.42203956842422485\n",
      "Epoch 22 / 40: Last train acc:0.87955\n",
      "Epoch 22 / 40: Last test acc:0.6147\n",
      "Epoch: 22 LR: [0.0003333333333333333]\n",
      "Epoch 23 / 40: avg. loss of last 5 iterations 0.3800545632839203\n",
      "Epoch 23 / 40: Last train acc:0.90195\n",
      "Epoch 23 / 40: Last test acc:0.6223\n",
      "Epoch: 23 LR: [0.0003333333333333333]\n",
      "Epoch 24 / 40: avg. loss of last 5 iterations 0.3354245722293854\n",
      "Epoch 24 / 40: Last train acc:0.91765\n",
      "Epoch 24 / 40: Last test acc:0.6273\n",
      "Epoch: 24 LR: [0.0003333333333333333]\n",
      "Epoch 25 / 40: avg. loss of last 5 iterations 0.31047523617744444\n",
      "Epoch 25 / 40: Last train acc:0.925\n",
      "Epoch 25 / 40: Last test acc:0.6224\n",
      "Epoch: 25 LR: [0.0003333333333333333]\n",
      "Epoch 26 / 40: avg. loss of last 5 iterations 0.26400187611579895\n",
      "Epoch 26 / 40: Last train acc:0.94325\n",
      "Epoch 26 / 40: Last test acc:0.6229\n",
      "Epoch: 26 LR: [0.0001111111111111111]\n",
      "Epoch 27 / 40: avg. loss of last 5 iterations 0.14208401441574098\n",
      "Epoch 27 / 40: Last train acc:0.9743\n",
      "Epoch 27 / 40: Last test acc:0.6314\n",
      "Epoch: 27 LR: [0.0001111111111111111]\n",
      "Epoch 28 / 40: avg. loss of last 5 iterations 0.1685919314622879\n",
      "Epoch 28 / 40: Last train acc:0.97575\n",
      "Epoch 28 / 40: Last test acc:0.6338\n",
      "Epoch: 28 LR: [0.0001111111111111111]\n",
      "Epoch 29 / 40: avg. loss of last 5 iterations 0.1237382173538208\n",
      "Epoch 29 / 40: Last train acc:0.98205\n",
      "Epoch 29 / 40: Last test acc:0.6325\n",
      "Epoch: 29 LR: [0.0001111111111111111]\n",
      "Epoch 30 / 40: avg. loss of last 5 iterations 0.13877274543046952\n",
      "Epoch 30 / 40: Last train acc:0.9806\n",
      "Epoch 30 / 40: Last test acc:0.6305\n",
      "Epoch: 30 LR: [0.0001111111111111111]\n",
      "Epoch 31 / 40: avg. loss of last 5 iterations 0.10527764111757279\n",
      "Epoch 31 / 40: Last train acc:0.9854\n",
      "Epoch 31 / 40: Last test acc:0.6311\n",
      "Epoch: 31 LR: [0.0001111111111111111]\n",
      "Epoch 32 / 40: avg. loss of last 5 iterations 0.11101250797510147\n",
      "Epoch 32 / 40: Last train acc:0.9877\n",
      "Epoch 32 / 40: Last test acc:0.6308\n",
      "Epoch: 32 LR: [0.0001111111111111111]\n",
      "Epoch 33 / 40: avg. loss of last 5 iterations 0.1126952663064003\n",
      "Epoch 33 / 40: Last train acc:0.9877\n",
      "Epoch 33 / 40: Last test acc:0.6291\n",
      "Epoch: 33 LR: [0.0001111111111111111]\n",
      "Epoch 34 / 40: avg. loss of last 5 iterations 0.11471676826477051\n",
      "Epoch 34 / 40: Last train acc:0.99165\n",
      "Epoch 34 / 40: Last test acc:0.6286\n",
      "Epoch: 34 LR: [0.0001111111111111111]\n",
      "Epoch 35 / 40: avg. loss of last 5 iterations 0.08657175526022912\n",
      "Epoch 35 / 40: Last train acc:0.9931\n",
      "Epoch 35 / 40: Last test acc:0.6286\n",
      "Epoch: 35 LR: [0.0001111111111111111]\n",
      "Epoch 36 / 40: avg. loss of last 5 iterations 0.07375832945108414\n",
      "Epoch 36 / 40: Last train acc:0.9936\n",
      "Epoch 36 / 40: Last test acc:0.6304\n",
      "Epoch: 36 LR: [0.0001111111111111111]\n",
      "Epoch 37 / 40: avg. loss of last 5 iterations 0.08441999405622483\n",
      "Epoch 37 / 40: Last train acc:0.99555\n",
      "Epoch 37 / 40: Last test acc:0.6292\n",
      "Epoch: 37 LR: [0.0001111111111111111]\n",
      "Epoch 38 / 40: avg. loss of last 5 iterations 0.06580742746591568\n",
      "Epoch 38 / 40: Last train acc:0.9961\n",
      "Epoch 38 / 40: Last test acc:0.6289\n",
      "Epoch: 38 LR: [0.0001111111111111111]\n",
      "Epoch 39 / 40: avg. loss of last 5 iterations 0.07080544009804726\n",
      "Epoch 39 / 40: Last train acc:0.9972\n",
      "Epoch 39 / 40: Last test acc:0.6309\n",
      "Epoch: 39 LR: [3.703703703703703e-05]\n",
      "Epoch 40 / 40: avg. loss of last 5 iterations 0.06409489661455155\n",
      "Epoch 40 / 40: Last train acc:0.9975\n",
      "Epoch 40 / 40: Last test acc:0.6313\n",
      "Epoch: 40 LR: [3.703703703703703e-05]\n"
     ]
    }
   ],
   "source": [
    "# From scratch with real labels\n",
    "loss_scratch_real, accs_scratch_real_train, accs_scratch_real_test = setup_and_train_downstream(\n",
    "    epochs=40, lr=0.001, step_size=(int)(40/3), num_examples_upstream=20000, \n",
    "    num_examples_downstream=20000, num_classes_upstream = 5, num_classes_downstream = 10,\n",
    "    is_downstream_random = False, batch_size = 256, num_outputs=10\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda (GPU support) is available and enabled!\n",
      "Epoch 1 / 40: avg. loss of last 5 iterations 1.8364093780517579\n",
      "Epoch 1 / 40: Last train acc:0.35295\n",
      "Epoch 1 / 40: Last test acc:0.3466\n",
      "Epoch: 1 LR: [0.001]\n",
      "Epoch 2 / 40: avg. loss of last 5 iterations 1.5792887926101684\n",
      "Epoch 2 / 40: Last train acc:0.44275\n",
      "Epoch 2 / 40: Last test acc:0.4279\n",
      "Epoch: 2 LR: [0.001]\n",
      "Epoch 3 / 40: avg. loss of last 5 iterations 1.4937051296234132\n",
      "Epoch 3 / 40: Last train acc:0.49845\n",
      "Epoch 3 / 40: Last test acc:0.466\n",
      "Epoch: 3 LR: [0.001]\n",
      "Epoch 4 / 40: avg. loss of last 5 iterations 1.3794410705566407\n",
      "Epoch 4 / 40: Last train acc:0.5465\n",
      "Epoch 4 / 40: Last test acc:0.4972\n",
      "Epoch: 4 LR: [0.001]\n",
      "Epoch 5 / 40: avg. loss of last 5 iterations 1.3808815956115723\n",
      "Epoch 5 / 40: Last train acc:0.54975\n",
      "Epoch 5 / 40: Last test acc:0.4885\n",
      "Epoch: 5 LR: [0.001]\n",
      "Epoch 6 / 40: avg. loss of last 5 iterations 1.1478238105773926\n",
      "Epoch 6 / 40: Last train acc:0.61995\n",
      "Epoch 6 / 40: Last test acc:0.5242\n",
      "Epoch: 6 LR: [0.001]\n",
      "Epoch 7 / 40: avg. loss of last 5 iterations 1.154151201248169\n",
      "Epoch 7 / 40: Last train acc:0.60165\n",
      "Epoch 7 / 40: Last test acc:0.5123\n",
      "Epoch: 7 LR: [0.001]\n",
      "Epoch 8 / 40: avg. loss of last 5 iterations 1.151511788368225\n",
      "Epoch 8 / 40: Last train acc:0.6624\n",
      "Epoch 8 / 40: Last test acc:0.5438\n",
      "Epoch: 8 LR: [0.001]\n",
      "Epoch 9 / 40: avg. loss of last 5 iterations 0.9794971585273743\n",
      "Epoch 9 / 40: Last train acc:0.68015\n",
      "Epoch 9 / 40: Last test acc:0.5362\n",
      "Epoch: 9 LR: [0.001]\n",
      "Epoch 10 / 40: avg. loss of last 5 iterations 0.884083890914917\n",
      "Epoch 10 / 40: Last train acc:0.7287\n",
      "Epoch 10 / 40: Last test acc:0.5514\n",
      "Epoch: 10 LR: [0.001]\n",
      "Epoch 11 / 40: avg. loss of last 5 iterations 0.9371485471725464\n",
      "Epoch 11 / 40: Last train acc:0.7417\n",
      "Epoch 11 / 40: Last test acc:0.5533\n",
      "Epoch: 11 LR: [0.001]\n",
      "Epoch 12 / 40: avg. loss of last 5 iterations 0.7716424584388732\n",
      "Epoch 12 / 40: Last train acc:0.82575\n",
      "Epoch 12 / 40: Last test acc:0.5702\n",
      "Epoch: 12 LR: [0.001]\n",
      "Epoch 13 / 40: avg. loss of last 5 iterations 0.617200493812561\n",
      "Epoch 13 / 40: Last train acc:0.84095\n",
      "Epoch 13 / 40: Last test acc:0.5646\n",
      "Epoch: 13 LR: [0.0003333333333333333]\n",
      "Epoch 14 / 40: avg. loss of last 5 iterations 0.33164559602737426\n",
      "Epoch 14 / 40: Last train acc:0.91665\n",
      "Epoch 14 / 40: Last test acc:0.5793\n",
      "Epoch: 14 LR: [0.0003333333333333333]\n",
      "Epoch 15 / 40: avg. loss of last 5 iterations 0.2649250328540802\n",
      "Epoch 15 / 40: Last train acc:0.94335\n",
      "Epoch 15 / 40: Last test acc:0.581\n",
      "Epoch: 15 LR: [0.0003333333333333333]\n",
      "Epoch 16 / 40: avg. loss of last 5 iterations 0.22374641448259353\n",
      "Epoch 16 / 40: Last train acc:0.9569\n",
      "Epoch 16 / 40: Last test acc:0.5791\n",
      "Epoch: 16 LR: [0.0003333333333333333]\n",
      "Epoch 17 / 40: avg. loss of last 5 iterations 0.19537097513675689\n",
      "Epoch 17 / 40: Last train acc:0.9744\n",
      "Epoch 17 / 40: Last test acc:0.579\n",
      "Epoch: 17 LR: [0.0003333333333333333]\n",
      "Epoch 18 / 40: avg. loss of last 5 iterations 0.13590787202119828\n",
      "Epoch 18 / 40: Last train acc:0.98375\n",
      "Epoch 18 / 40: Last test acc:0.5787\n",
      "Epoch: 18 LR: [0.0003333333333333333]\n",
      "Epoch 19 / 40: avg. loss of last 5 iterations 0.10450277924537658\n",
      "Epoch 19 / 40: Last train acc:0.98985\n",
      "Epoch 19 / 40: Last test acc:0.5791\n",
      "Epoch: 19 LR: [0.0003333333333333333]\n",
      "Epoch 20 / 40: avg. loss of last 5 iterations 0.1075274258852005\n",
      "Epoch 20 / 40: Last train acc:0.9937\n",
      "Epoch 20 / 40: Last test acc:0.5745\n",
      "Epoch: 20 LR: [0.0003333333333333333]\n",
      "Epoch 21 / 40: avg. loss of last 5 iterations 0.05451536253094673\n",
      "Epoch 21 / 40: Last train acc:0.9983\n",
      "Epoch 21 / 40: Last test acc:0.578\n",
      "Epoch: 21 LR: [0.0003333333333333333]\n",
      "Epoch 22 / 40: avg. loss of last 5 iterations 0.04254300221800804\n",
      "Epoch 22 / 40: Last train acc:0.99885\n",
      "Epoch 22 / 40: Last test acc:0.5797\n",
      "Epoch: 22 LR: [0.0003333333333333333]\n",
      "Epoch 23 / 40: avg. loss of last 5 iterations 0.03193826451897621\n",
      "Epoch 23 / 40: Last train acc:0.9996\n",
      "Epoch 23 / 40: Last test acc:0.5767\n",
      "Epoch: 23 LR: [0.0003333333333333333]\n",
      "Epoch 24 / 40: avg. loss of last 5 iterations 0.02372908964753151\n",
      "Epoch 24 / 40: Last train acc:0.9997\n",
      "Epoch 24 / 40: Last test acc:0.5778\n",
      "Epoch: 24 LR: [0.0003333333333333333]\n",
      "Epoch 25 / 40: avg. loss of last 5 iterations 0.023943078145384787\n",
      "Epoch 25 / 40: Last train acc:0.9998\n",
      "Epoch 25 / 40: Last test acc:0.5773\n",
      "Epoch: 25 LR: [0.0003333333333333333]\n",
      "Epoch 26 / 40: avg. loss of last 5 iterations 0.018114637397229672\n",
      "Epoch 26 / 40: Last train acc:1.0\n",
      "Epoch 26 / 40: Last test acc:0.5772\n",
      "Epoch: 26 LR: [0.0001111111111111111]\n",
      "Epoch 27 / 40: avg. loss of last 5 iterations 0.011991365440189838\n",
      "Epoch 27 / 40: Last train acc:1.0\n",
      "Epoch 27 / 40: Last test acc:0.578\n",
      "Epoch: 27 LR: [0.0001111111111111111]\n",
      "Epoch 28 / 40: avg. loss of last 5 iterations 0.01159639861434698\n",
      "Epoch 28 / 40: Last train acc:1.0\n",
      "Epoch 28 / 40: Last test acc:0.5777\n",
      "Epoch: 28 LR: [0.0001111111111111111]\n",
      "Epoch 29 / 40: avg. loss of last 5 iterations 0.016239523701369763\n",
      "Epoch 29 / 40: Last train acc:1.0\n",
      "Epoch 29 / 40: Last test acc:0.5775\n",
      "Epoch: 29 LR: [0.0001111111111111111]\n",
      "Epoch 30 / 40: avg. loss of last 5 iterations 0.01087412890046835\n",
      "Epoch 30 / 40: Last train acc:1.0\n",
      "Epoch 30 / 40: Last test acc:0.578\n",
      "Epoch: 30 LR: [0.0001111111111111111]\n",
      "Epoch 31 / 40: avg. loss of last 5 iterations 0.010993512906134128\n",
      "Epoch 31 / 40: Last train acc:1.0\n",
      "Epoch 31 / 40: Last test acc:0.5771\n",
      "Epoch: 31 LR: [0.0001111111111111111]\n",
      "Epoch 32 / 40: avg. loss of last 5 iterations 0.010375968925654889\n",
      "Epoch 32 / 40: Last train acc:1.0\n",
      "Epoch 32 / 40: Last test acc:0.5774\n",
      "Epoch: 32 LR: [0.0001111111111111111]\n",
      "Epoch 33 / 40: avg. loss of last 5 iterations 0.010259741265326739\n",
      "Epoch 33 / 40: Last train acc:1.0\n",
      "Epoch 33 / 40: Last test acc:0.5778\n",
      "Epoch: 33 LR: [0.0001111111111111111]\n",
      "Epoch 34 / 40: avg. loss of last 5 iterations 0.008895833510905504\n",
      "Epoch 34 / 40: Last train acc:1.0\n",
      "Epoch 34 / 40: Last test acc:0.5767\n",
      "Epoch: 34 LR: [0.0001111111111111111]\n",
      "Epoch 35 / 40: avg. loss of last 5 iterations 0.009220062382519245\n",
      "Epoch 35 / 40: Last train acc:1.0\n",
      "Epoch 35 / 40: Last test acc:0.5771\n",
      "Epoch: 35 LR: [0.0001111111111111111]\n",
      "Epoch 36 / 40: avg. loss of last 5 iterations 0.00788457915186882\n",
      "Epoch 36 / 40: Last train acc:1.0\n",
      "Epoch 36 / 40: Last test acc:0.577\n",
      "Epoch: 36 LR: [0.0001111111111111111]\n",
      "Epoch 37 / 40: avg. loss of last 5 iterations 0.008642307668924331\n",
      "Epoch 37 / 40: Last train acc:1.0\n",
      "Epoch 37 / 40: Last test acc:0.5769\n",
      "Epoch: 37 LR: [0.0001111111111111111]\n",
      "Epoch 38 / 40: avg. loss of last 5 iterations 0.008994876593351363\n",
      "Epoch 38 / 40: Last train acc:1.0\n",
      "Epoch 38 / 40: Last test acc:0.5758\n",
      "Epoch: 38 LR: [0.0001111111111111111]\n",
      "Epoch 39 / 40: avg. loss of last 5 iterations 0.007816429436206817\n",
      "Epoch 39 / 40: Last train acc:1.0\n",
      "Epoch 39 / 40: Last test acc:0.5757\n",
      "Epoch: 39 LR: [3.703703703703703e-05]\n",
      "Epoch 40 / 40: avg. loss of last 5 iterations 0.008312130533158778\n",
      "Epoch 40 / 40: Last train acc:1.0\n",
      "Epoch 40 / 40: Last test acc:0.5772\n",
      "Epoch: 40 LR: [3.703703703703703e-05]\n"
     ]
    }
   ],
   "source": [
    "# Using upstream training's weights with real labels\n",
    "loss_upstream_real, accs_upstream_real_train, accs_upstream_real_test = setup_and_train_downstream(\n",
    "    epochs=40, lr=0.001, step_size=(int)(40/3), num_examples_upstream=20000, \n",
    "    num_examples_downstream=20000, num_classes_upstream = 5, num_classes_downstream = 5,\n",
    "    is_downstream_random = False, batch_size = 256, num_outputs=10,\n",
    "    upstream_model_path = \"pretrained_model_dict_upstream_1_2.pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "orange = \"#fcba03\" # from scr\n",
    "light_blue = \"#03bafc\" # pre\n",
    "\n",
    "plt.plot(accs_scratch_real_train, linestyle=\"-\", color=orange)\n",
    "plt.plot(accs_upstream_real_train, linestyle=\"-\", color=light_blue)\n",
    "plt.plot(accs_scratch_real_test, linestyle=\"--\", color=orange)\n",
    "plt.plot(accs_upstream_real_test, linestyle=\"--\", color=light_blue)\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of figure1.ipynb",
   "provenance": [
    {
     "file_id": "1BNwYcPcE3j7kX8PsVSfBpXp1s6Dwmfne",
     "timestamp": 1626623850743
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
