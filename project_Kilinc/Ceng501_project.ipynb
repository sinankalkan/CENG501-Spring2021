{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ceng501_project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJL68Go3rHJ3"
      },
      "source": [
        "# Libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWvdt4PKQCoF"
      },
      "source": [
        "# 1. Data Generation \n",
        "\n",
        "In this section you can create different data types in each cell as a class structure "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icJU9GZZMilt"
      },
      "source": [
        "# Sinusoidal Data Generation \n",
        "\n",
        "class Sinusoidal_data:\n",
        "    def __init__(self,batch_size = 100, noise=0.01,tr_size=100,val_size=100,te_size=20000):\n",
        "\n",
        "        # Train / Validation / Test Size \n",
        "        self.tr_size = tr_size\n",
        "        self.val_size = val_size\n",
        "        self.te_size = te_size \n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Noise percetage \n",
        "        # (Low : %1 = 0.01 , Middle : %5 = 0.05 , High %10 = 0.1)\n",
        "        self.noise = noise\n",
        "\n",
        "\n",
        "    def get_data(self):\n",
        "\n",
        "        # Train data \n",
        "        x_train = 2*np.random.randn(self.tr_size, 2)\n",
        "        u = np.zeros(shape=(1, 2))\n",
        "        u[0, 0] = 1\n",
        "        v = np.zeros(shape=(1, 2))\n",
        "        v[0, 1] = 1\n",
        "        y_train = np.sign(u.dot(x_train.T) + np.sin(v.dot(x_train.T))).astype(np.int32).reshape(self.tr_size)\n",
        "\n",
        "        # Flip some output labels randomly as a noise \n",
        "        idx = np.random.permutation(self.tr_size)[:int(self.tr_size*self.noise)]\n",
        "        y_train[idx] *= -1\n",
        "        y_train = (y_train+1)//2\n",
        "        #y_train = np.reshape(y_train,(y_train.shape[0],1))\n",
        "        \n",
        "\n",
        "        # validation data \n",
        "        x_val = 2*np.random.randn(self.val_size, 2)\n",
        "        u = np.zeros(shape=(1, 2))\n",
        "        u[0, 0] = 1\n",
        "        v = np.zeros(shape=(1, 2))\n",
        "        v[0, 1] = 1\n",
        "        y_val = np.sign(u.dot(x_val.T) + np.sin(v.dot(x_val.T))).astype(np.int32).reshape(self.val_size)\n",
        "\n",
        "        # Flip some output labels randomly as a noise \n",
        "        idx = np.random.permutation(self.val_size)[:int(self.val_size*self.noise)]\n",
        "        y_val[idx] *= -1\n",
        "        y_val = (y_val+1)//2\n",
        "        #y_val = np.reshape(y_val,(y_val.shape[0],1))\n",
        "\n",
        "        # Test data \n",
        "        x_test = 2*np.random.randn(self.te_size, 2)\n",
        "        u = np.zeros(shape=(1, 2))\n",
        "        u[0, 0] = 1\n",
        "        v = np.zeros(shape=(1, 2))\n",
        "        v[0, 1] = 1\n",
        "        y_test = np.sign(u.dot(x_test.T) + np.sin(v.dot(x_test.T))).astype(np.int32).reshape(self.te_size)\n",
        "\n",
        "        # Flip some output labels randomly as a noise \n",
        "        idx = np.random.permutation(self.te_size)[:int(self.te_size*self.noise)]\n",
        "        y_test[idx] *= -1\n",
        "        y_test = (y_test+1)//2\n",
        "        #y_test = np.reshape(y_test,(y_test.shape[0],1))\n",
        "\n",
        "        x_train = torch.Tensor(x_train)\n",
        "        x_val = torch.Tensor(x_val)\n",
        "        x_test = torch.Tensor(x_test)\n",
        "        \n",
        "        y_train = torch.Tensor(y_train)\n",
        "        y_val = torch.Tensor(y_val)\n",
        "        y_test = torch.Tensor(y_test)\n",
        "\n",
        "        train_data = torch.utils.data.TensorDataset(x_train, y_train)\n",
        "        val_data = torch.utils.data.TensorDataset(x_val, y_val)\n",
        "        test_data = torch.utils.data.TensorDataset(x_test, y_test)\n",
        "\n",
        "        trainloader = torch.utils.data.DataLoader(train_data,batch_size=self.batch_size,shuffle=True, pin_memory=True)\n",
        "        valloader = torch.utils.data.DataLoader(val_data,batch_size=self.batch_size, shuffle=True, pin_memory=True)   \n",
        "        testloader = torch.utils.data.DataLoader(test_data,batch_size=self.batch_size, shuffle=True, pin_memory=True) \n",
        "\n",
        "        return trainloader,valloader,testloader\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABtKEY8pXySq"
      },
      "source": [
        "# Gaussian Data\n",
        "\n",
        "class Gaussian_data:\n",
        "    def __init__(self,batch_size = 100, noise=0.01,tr_size=100,val_size=100,te_size=20000):\n",
        "\n",
        "        # Train / Validation / Test Size \n",
        "        self.tr_size = tr_size\n",
        "        self.val_size = val_size\n",
        "        self.te_size = te_size \n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Noise percetage \n",
        "        # (Low : %1 = 0.01 , Middle : %5 = 0.05 , High %10 = 0.1)\n",
        "        self.noise = noise\n",
        "\n",
        "        self.pos_mean = torch.zeros(10)\n",
        "        self.neg_mean = torch.ones(10)\n",
        "\n",
        "    def get_data(self):\n",
        "\n",
        "        # Train data \n",
        "        pos_dist = torch.distributions.MultivariateNormal(self.pos_mean, torch.eye(10))\n",
        "        neg_dist = torch.distributions.MultivariateNormal(self.neg_mean, torch.eye(10))\n",
        "        pos_tr = pos_dist.sample((self.tr_size,))\n",
        "        neg_tr = neg_dist.sample((self.tr_size,))\n",
        "        x_train = torch.cat((pos_tr,neg_tr))\n",
        "        y_train = torch.cat((-torch.ones(self.tr_size), torch.ones(self.tr_size)))\n",
        "\n",
        "        n = y_train.shape[0]\n",
        "        idx = np.random.permutation(n)[:int(n*self.noise)]\n",
        "        y_train[idx] *= -1\n",
        "        y_train = (y_train+1)//2      \n",
        "\n",
        "        # validation data \n",
        "        pos_val = pos_dist.sample((self.val_size,))\n",
        "        neg_val = neg_dist.sample((self.val_size,))\n",
        "        x_val = torch.cat((pos_val,neg_val))\n",
        "        y_val = torch.cat((-torch.ones(self.val_size), torch.ones(self.val_size)))\n",
        "\n",
        "        n = y_val.shape[0]\n",
        "        idx = np.random.permutation(n)[:int(n*self.noise)]\n",
        "        y_val[idx] *= -1\n",
        "        y_val = (y_val+1)//2\n",
        "\n",
        "        # Test data \n",
        "        pos_te = pos_dist.sample((self.te_size,))\n",
        "        neg_te = neg_dist.sample((self.te_size,))\n",
        "        x_test = torch.cat((pos_te,neg_te))\n",
        "        y_test = torch.cat((-torch.ones(self.te_size), torch.ones(self.te_size)))\n",
        "\n",
        "        n = y_test.shape[0]\n",
        "        idx = np.random.permutation(n)[:int(n*self.noise)]\n",
        "        y_test[idx] *= -1\n",
        "        y_test = (y_test+1)//2\n",
        "\n",
        "        train_data = torch.utils.data.TensorDataset(x_train, y_train)\n",
        "        val_data = torch.utils.data.TensorDataset(x_val, y_val)\n",
        "        test_data = torch.utils.data.TensorDataset(x_test, y_test)\n",
        "\n",
        "        trainloader = torch.utils.data.DataLoader(train_data,batch_size=self.batch_size,shuffle=True, pin_memory=True)\n",
        "        valloader = torch.utils.data.DataLoader(val_data,batch_size=self.batch_size, shuffle=True, pin_memory=True)   \n",
        "        testloader = torch.utils.data.DataLoader(test_data,batch_size=self.batch_size, shuffle=True, pin_memory=True) \n",
        "\n",
        "        return trainloader,valloader,testloader\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XkoGCXgjtST"
      },
      "source": [
        "# Spiral Data Generation \n",
        "\n",
        "class Spiral_data:\n",
        "    def __init__(self,batch_size = 100, noise=0.01,tr_size=100,val_size=100,te_size=20000):\n",
        "\n",
        "        # Train / Validation / Test Size \n",
        "        self.tr_size = tr_size\n",
        "        self.val_size = val_size\n",
        "        self.te_size = te_size \n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Noise percetage \n",
        "        # (Low : %1 = 0.01 , Middle : %5 = 0.05 , High %10 = 0.1)\n",
        "        self.noise = noise\n",
        "\n",
        "\n",
        "    def get_data(self):\n",
        "\n",
        "        # Train data \n",
        "        a1, a2 = np.linspace(0, 4*np.pi, self.tr_size//2), np.linspace(0, 4*np.pi, self.tr_size//2)\n",
        "        y_train, x_train = np.zeros(shape=(self.tr_size,)), np.zeros(shape=(self.tr_size, 2))\n",
        "\n",
        "        y_train[:self.tr_size//2], y_train[self.tr_size//2:] = 1, -1\n",
        "        x_train[y_train == 1, 0] = a1 * np.cos(a1) + np.random.randn(self.tr_size//2)\n",
        "        x_train[y_train == 1, 1] = a1 * np.sin(a1) + np.random.randn(self.tr_size//2)\n",
        "        x_train[y_train == -1, 0] = (a2 + np.pi) * np.cos(a2) + np.random.randn(self.tr_size//2)\n",
        "        x_train[y_train == -1, 1] = (a2 + np.pi) * np.sin(a2) + np.random.randn(self.tr_size//2) \n",
        "\n",
        "\n",
        "        # Flip some output labels randomly as a noise \n",
        "        idx = np.random.permutation(self.tr_size)[:int(self.tr_size*self.noise)]\n",
        "        y_train[idx] *= -1\n",
        "        y_train = (y_train+1)//2\n",
        "        #y_train = np.reshape(y_train,(y_train.shape[0],1))\n",
        "        \n",
        "\n",
        "        # validation data \n",
        "        a1, a2 = np.linspace(0, 4*np.pi, self.val_size//2), np.linspace(0, 4*np.pi, self.val_size//2)\n",
        "        y_val, x_val = np.zeros(shape=(self.val_size,)), np.zeros(shape=(self.val_size, 2))\n",
        "\n",
        "        y_val[:self.val_size//2], y_val[self.val_size//2:] = 1, -1\n",
        "        x_val[y_val == 1, 0] = a1 * np.cos(a1) + np.random.randn(self.val_size//2)\n",
        "        x_val[y_val == 1, 1] = a1 * np.sin(a1) + np.random.randn(self.val_size//2)\n",
        "        x_val[y_val == -1, 0] = (a2 + np.pi) * np.cos(a2) + np.random.randn(self.val_size//2)\n",
        "        x_val[y_val == -1, 1] = (a2 + np.pi) * np.sin(a2) + np.random.randn(self.val_size//2) \n",
        "\n",
        "        # Flip some output labels randomly as a noise \n",
        "        idx = np.random.permutation(self.val_size)[:int(self.val_size*self.noise)]\n",
        "        y_val[idx] *= -1\n",
        "        y_val = (y_val+1)//2\n",
        "        #y_val = np.reshape(y_val,(y_val.shape[0],1))\n",
        "\n",
        "        # Test data \n",
        "        a1, a2 = np.linspace(0, 4*np.pi, self.te_size//2), np.linspace(0, 4*np.pi, self.te_size//2)\n",
        "        y_test, x_test = np.zeros(shape=(self.te_size,)), np.zeros(shape=(self.te_size, 2))\n",
        "\n",
        "        y_test[:self.te_size//2], y_test[self.te_size//2:] = 1, -1\n",
        "        x_test[y_test == 1, 0] = a1 * np.cos(a1) + np.random.randn(self.te_size//2)\n",
        "        x_test[y_test == 1, 1] = a1 * np.sin(a1) + np.random.randn(self.te_size//2)\n",
        "        x_test[y_test == -1, 0] = (a2 + np.pi) * np.cos(a2) + np.random.randn(self.te_size//2)\n",
        "        x_test[y_test == -1, 1] = (a2 + np.pi) * np.sin(a2) + np.random.randn(self.te_size//2) \n",
        "\n",
        "        # Flip some output labels randomly as a noise \n",
        "        idx = np.random.permutation(self.te_size)[:int(self.te_size*self.noise)]\n",
        "        y_test[idx] *= -1\n",
        "        y_test = (y_test+1)//2\n",
        "        #y_test = np.reshape(y_test,(y_test.shape[0],1))\n",
        "\n",
        "        x_train = torch.Tensor(x_train)\n",
        "        x_val = torch.Tensor(x_val)\n",
        "        x_test = torch.Tensor(x_test)\n",
        "        \n",
        "        y_train = torch.Tensor(y_train)\n",
        "        y_val = torch.Tensor(y_val)\n",
        "        y_test = torch.Tensor(y_test)\n",
        "\n",
        "        train_data = torch.utils.data.TensorDataset(x_train, y_train)\n",
        "        val_data = torch.utils.data.TensorDataset(x_val, y_val)\n",
        "        test_data = torch.utils.data.TensorDataset(x_test, y_test)\n",
        "\n",
        "        trainloader = torch.utils.data.DataLoader(train_data,batch_size=self.batch_size,shuffle=True, pin_memory=True)\n",
        "        valloader = torch.utils.data.DataLoader(val_data,batch_size=self.batch_size, shuffle=True, pin_memory=True)   \n",
        "        testloader = torch.utils.data.DataLoader(test_data,batch_size=self.batch_size, shuffle=True, pin_memory=True) \n",
        "\n",
        "        return trainloader,valloader,testloader\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYPqIPQqs-1q"
      },
      "source": [
        "# MNIST Data Generation \n",
        "import numpy as np\n",
        "\n",
        "class MNIST_data:\n",
        "    def __init__(self,batch_size=100):\n",
        "\n",
        "        # Train / Validation split \n",
        "        self.tr_size = 0.8\n",
        "        self.val_size = 0.2\n",
        "        self.batch_size = batch_size \n",
        "\n",
        "\n",
        "\n",
        "    def get_data(self):\n",
        "\n",
        "        normalize = transforms.Normalize((0.1307,), (0.3081,))  # MNIST\n",
        "\n",
        "        transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                normalize\n",
        "            ])\n",
        "        \n",
        "        # load the dataset\n",
        "        train_data = datasets.MNIST(root='./data', train=True, \n",
        "                    download=True, transform=transform)\n",
        "\n",
        "        val_data = datasets.MNIST(root='./data', train=True, \n",
        "                    download=True, transform=transform)\n",
        "\n",
        "        # load the test dataset\n",
        "        test_data = datasets.MNIST(root='./data', train=False, \n",
        "                    download=True, transform=transform)\n",
        "\n",
        "        num_train = len(train_data)\n",
        "        indices = list(range(num_train))\n",
        "        split = int(np.floor(self.val_size * num_train))\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "        train_sampler = SubsetRandomSampler(train_idx)\n",
        "        valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "        trainloader = torch.utils.data.DataLoader(train_data, \n",
        "                        batch_size=self.batch_size, sampler=train_sampler, \n",
        "                        pin_memory=True)\n",
        "\n",
        "        valloader = torch.utils.data.DataLoader(val_data, \n",
        "                        batch_size=self.batch_size, sampler=valid_sampler, \n",
        "                        pin_memory=True)\n",
        "\n",
        "        testloader = torch.utils.data.DataLoader(test_data, \n",
        "                        batch_size=self.batch_size , shuffle=True)\n",
        "\n",
        "        return trainloader, valloader, testloader"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1UH_AeSPCaA"
      },
      "source": [
        "# Cifar10 Data Generation \n",
        "import numpy as np\n",
        "\n",
        "class Cifar10_data:\n",
        "    def __init__(self,batch_size=100):\n",
        "\n",
        "        # Train / Validation split \n",
        "        self.tr_size = 0.8\n",
        "        self.val_size = 0.2\n",
        "        self.batch_size = batch_size \n",
        "\n",
        "\n",
        "\n",
        "    def get_data(self):\n",
        "\n",
        "\n",
        "        transform = transforms.Compose(\n",
        "            [transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "               \n",
        "        # load the dataset\n",
        "        train_data = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "        val_data = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "        # load the test dataset\n",
        "        test_data = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "        num_train = len(train_data)\n",
        "        indices = list(range(num_train))\n",
        "        split = int(np.floor(self.val_size * num_train))\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "        train_sampler = SubsetRandomSampler(train_idx)\n",
        "        valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "        trainloader = torch.utils.data.DataLoader(train_data, \n",
        "                        batch_size=self.batch_size, sampler=train_sampler, \n",
        "                        pin_memory=True)\n",
        "\n",
        "        valloader = torch.utils.data.DataLoader(val_data, \n",
        "                        batch_size=self.batch_size, sampler=valid_sampler, \n",
        "                        pin_memory=True)\n",
        "\n",
        "        testloader = torch.utils.data.DataLoader(test_data, \n",
        "                        batch_size=self.batch_size , shuffle=True)\n",
        "\n",
        "        self.classes = ('plane', 'car', 'bird', 'cat',\n",
        "                'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "        return trainloader, valloader, testloader"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_0rs48FQyTE"
      },
      "source": [
        "# Cifar100 Data Generation \n",
        "import numpy as np\n",
        "\n",
        "class Cifar100_data:\n",
        "    def __init__(self,batch_size=100):\n",
        "\n",
        "        # Train / Validation split \n",
        "        self.tr_size = 0.8\n",
        "        self.val_size = 0.2\n",
        "        self.batch_size = batch_size \n",
        "\n",
        "\n",
        "\n",
        "    def get_data(self):\n",
        "\n",
        "\n",
        "        transform = transforms.Compose(\n",
        "            [transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "               \n",
        "        # load the dataset\n",
        "        train_data = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "        val_data = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "        # load the test dataset\n",
        "        test_data = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "        num_train = len(train_data)\n",
        "        indices = list(range(num_train))\n",
        "        split = int(np.floor(self.val_size * num_train))\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "        train_sampler = SubsetRandomSampler(train_idx)\n",
        "        valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "        trainloader = torch.utils.data.DataLoader(train_data, \n",
        "                        batch_size=self.batch_size, sampler=train_sampler, \n",
        "                        pin_memory=True)\n",
        "\n",
        "        valloader = torch.utils.data.DataLoader(val_data, \n",
        "                        batch_size=self.batch_size, sampler=valid_sampler, \n",
        "                        pin_memory=True)\n",
        "\n",
        "        testloader = torch.utils.data.DataLoader(test_data, \n",
        "                        batch_size=self.batch_size , shuffle=True)\n",
        "\n",
        "\n",
        "        return trainloader, valloader, testloader"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLtofj5CRUrE"
      },
      "source": [
        "# SVHN Data Generation \n",
        "import numpy as np\n",
        "\n",
        "class SVHN_data:\n",
        "    def __init__(self,batch_size=100):\n",
        "\n",
        "        # Train / Validation split \n",
        "        self.tr_size = 0.8\n",
        "        self.val_size = 0.2\n",
        "        self.batch_size = batch_size \n",
        "\n",
        "\n",
        "\n",
        "    def get_data(self):\n",
        "\n",
        "\n",
        "        transform = transforms.Compose(\n",
        "            [transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "               \n",
        "        # load the dataset\n",
        "        train_data = torchvision.datasets.SVHN(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "        val_data = torchvision.datasets.SVHN(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "        # load the test dataset\n",
        "        test_data = torchvision.datasets.SVHN(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "        num_train = len(train_data)\n",
        "        indices = list(range(num_train))\n",
        "        split = int(np.floor(self.val_size * num_train))\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "        train_sampler = SubsetRandomSampler(train_idx)\n",
        "        valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "        trainloader = torch.utils.data.DataLoader(train_data, \n",
        "                        batch_size=self.batch_size, sampler=train_sampler, \n",
        "                        pin_memory=True)\n",
        "\n",
        "        valloader = torch.utils.data.DataLoader(val_data, \n",
        "                        batch_size=self.batch_size, sampler=valid_sampler, \n",
        "                        pin_memory=True)\n",
        "\n",
        "        testloader = torch.utils.data.DataLoader(test_data, \n",
        "                        batch_size=self.batch_size , shuffle=True)\n",
        "\n",
        "\n",
        "        return trainloader, valloader, testloader"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQVZtJX1S1UX"
      },
      "source": [
        "# Kuzushiji Data Generation \n",
        "import numpy as np\n",
        "\n",
        "class Kuzushiji_data:\n",
        "    def __init__(self,batch_size=100):\n",
        "\n",
        "        # Train / Validation split \n",
        "        self.tr_size = 0.8\n",
        "        self.val_size = 0.2\n",
        "        self.batch_size = batch_size \n",
        "\n",
        "\n",
        "\n",
        "    def get_data(self):\n",
        "\n",
        "        normalize = transforms.Normalize((0.1307,), (0.3081,))  # MNIST\n",
        "\n",
        "        transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                normalize\n",
        "            ])\n",
        "        \n",
        "        # load the dataset\n",
        "        train_data = datasets.KMNIST(root='./data', train=True, \n",
        "                    download=True, transform=transform)\n",
        "\n",
        "        val_data = datasets.KMNIST(root='./data', train=True, \n",
        "                    download=True, transform=transform)\n",
        "\n",
        "        # load the test dataset\n",
        "        test_data = datasets.KMNIST(root='./data', train=False, \n",
        "                    download=True, transform=transform)\n",
        "\n",
        "        num_train = len(train_data)\n",
        "        indices = list(range(num_train))\n",
        "        split = int(np.floor(self.val_size * num_train))\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "        train_sampler = SubsetRandomSampler(train_idx)\n",
        "        valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "        trainloader = torch.utils.data.DataLoader(train_data, \n",
        "                        batch_size=self.batch_size, sampler=train_sampler, \n",
        "                        pin_memory=True)\n",
        "\n",
        "        valloader = torch.utils.data.DataLoader(val_data, \n",
        "                        batch_size=self.batch_size, sampler=valid_sampler, \n",
        "                        pin_memory=True)\n",
        "\n",
        "        testloader = torch.utils.data.DataLoader(test_data, \n",
        "                        batch_size=self.batch_size , shuffle=True)\n",
        "\n",
        "        return trainloader, valloader, testloader"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHDIM-r2QW_G"
      },
      "source": [
        "# 2. Neural network Structures\n",
        "\n",
        "In this section you can see each neural network structure from the paper. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3zrpRYuAyRq"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MLP_Net(nn.Module):\n",
        "    def __init__(self,input_size,out_size):\n",
        "        super(MLP_Net, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.out_size = out_size\n",
        "        self.flatten = nn.Flatten()        \n",
        "        self.fc1 = nn.Linear(self.input_size, 500)\n",
        "        self.fc2 = nn.Linear(500, 500)\n",
        "        self.fc3 = nn.Linear(500, 500)\n",
        "        self.fc4 = nn.Linear(500, 500)\n",
        "        self.fc5 = nn.Linear(500, self.out_size)\n",
        "        self.bn1 = nn.BatchNorm1d(500)\n",
        "        self.bn2 = nn.BatchNorm1d(500)\n",
        "        self.bn3 = nn.BatchNorm1d(500)\n",
        "        self.bn4 = nn.BatchNorm1d(500)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if len(x.shape) > 2:\n",
        "            x = self.flatten(x)\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        x = F.relu(self.bn2(self.fc2(x)))\n",
        "        #x = F.relu(self.bn3(self.fc3(x)))\n",
        "        #x = F.relu(self.bn4(self.fc4(x)))\n",
        "        x = self.fc5(x)\n",
        "        return x"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRPRBUALaePn"
      },
      "source": [
        "class Resnet_18():\n",
        "    def __init__(self,input_size,out_size):\n",
        "        self.input_size = input_size\n",
        "        self.out_size = out_size\n",
        "        self.ResNet18 = torchvision.models.resnet18( pretrained=False)\n",
        "        num_ftrs = self.ResNet18.fc.in_features\n",
        "        self.ResNet18.fc = nn.Linear(num_ftrs, 10)\n",
        "        self.ResNet18.fc = self.ResNet18.fc.cuda() if torch.cuda.is_available() else self.ResNet18.fc"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AKz-DhyQmCj"
      },
      "source": [
        "# 3. Training Algorithm\n",
        "\n",
        "You can train a model with datasets to validate the results "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98d65yBbo928",
        "outputId": "772557d5-a401-4d8d-a55e-feba5f00767c"
      },
      "source": [
        "# import libraries\n",
        "import random\n",
        "import numpy as np \n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "# Seed\n",
        "random.seed(501)\n",
        "np.random.seed(501)\n",
        "torch.manual_seed(501)\n",
        "\n",
        "# Flood Level\n",
        "flood_level = 0.0\n",
        "\n",
        "# Donot forget to change input shape \n",
        "# Sinus/gauss/spiral = 2/10/2\n",
        "#Mnist = 28*28\n",
        "#Cifar = 3072\n",
        "input_dim = 10\n",
        "\n",
        "# Do not forget to change class number\n",
        "# Sinus/gauss/spiral = 1\n",
        "#mnist/cifar10 = 10\n",
        "class_number = 1\n",
        "\n",
        "# Choose dataset class  \n",
        "batch_size = 100\n",
        "# Noise Low/Middle/High = 0.01/0.05/0.1                                     \n",
        "dataset = Gaussian_data(batch_size=batch_size,noise=0.01)\n",
        "#dataset = MNIST_data(batch_size=batch_size)\n",
        "#dataset = Cifar10_data(batch_size=batch_size)\n",
        "\n",
        "# Choose neural network \n",
        "#model = Resnet_18(input_dim, class_number).ResNet18\n",
        "model = MLP_Net(input_dim, class_number)\n",
        "\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Cuda (GPU support) is available and enabled!\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"Cuda (GPU support) is not available :(\")\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "def binary_acc(y_pred, y_test):\n",
        "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
        "\n",
        "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
        "    acc = correct_results_sum/y_test.shape[0]\n",
        "    acc = torch.round(acc * 100)\n",
        "    \n",
        "    return acc\n",
        "\n",
        "def multi_class_acc(y_pred, y_test):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    _, predicted = torch.max(y_pred.data, 1)\n",
        "    total = y_test.shape[0]\n",
        "    correct = (predicted == labels).sum().float()\n",
        "\n",
        "    return torch.round(100 * correct / total)\n",
        "\n",
        "# Main Loop \n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    trainloader , valloader , testloader = dataset.get_data() \n",
        "\n",
        "\n",
        "    if class_number == 1:\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "        \n",
        "    else:    \n",
        "        criterion = nn.CrossEntropyLoss(reduction='mean')\n",
        "\n",
        "    \n",
        "    optimizer = optim.Adam(model.parameters(), weight_decay=0, lr=0.001)\n",
        "\n",
        "    train_loss_history = []\n",
        "    test_loss_history = []  \n",
        "    for epoch in range(500):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "        for  data in trainloader:    \n",
        "            \n",
        "            # Our batch:\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the gradients as PyTorch accumulates them\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Obtain the scores\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Calculate loss\n",
        "            #print(labels.shape)\n",
        "            if class_number == 1:\n",
        "                loss = criterion(outputs.to(device), labels.unsqueeze(1))\n",
        "                acc = binary_acc(outputs.to(device), labels.unsqueeze(1))\n",
        "            else:\n",
        "                loss = criterion(outputs.to(device), labels)\n",
        "                #acc = multi_class_acc(outputs.to(device), labels)\n",
        "\n",
        "            if flood_level > 0:\n",
        "                loss_corrected = abs(loss-flood_level) + flood_level\n",
        "            else:\n",
        "                loss_corrected = loss\n",
        "         \n",
        "\n",
        "            # Backpropagate\n",
        "            loss_corrected.backward()\n",
        "\n",
        "            # Update the weights\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss_history.append(loss.item())\n",
        "            epoch_loss += loss_corrected.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "            for data in testloader:\n",
        "                test_loss_list = []\n",
        "                images, labels = data\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(images)\n",
        "                if class_number == 1:\n",
        "                    test_acc = binary_acc(outputs,labels.unsqueeze(1))                \n",
        "                    test_loss =  criterion(outputs.to(device), labels.unsqueeze(1))\n",
        "                else:\n",
        "                    test_acc = multi_class_acc(outputs.to(device),labels)                \n",
        "                    test_loss =  criterion(outputs.to(device), labels)\n",
        "                test_loss_list.append(test_loss.item())                   \n",
        "            test_loss_history.append(np.mean(test_loss_list))\n",
        "\n",
        "\n",
        "        #print(f'Epoch {epoch} / {500}: avg. loss of last 5 iterations {np.sum(loss_history[:-6:-1])/5}')\n",
        "        #print(f'Epoch {epoch+0:03}: | Loss: {epoch_loss/len(trainloader):.5f} | Train Acc: {epoch_acc/len(trainloader):.3f} | Val Acc: {val_acc:.3f}')\n",
        "        print(f'Epoch {epoch+0:03}: | Loss: {epoch_loss/len(trainloader):.5f} ')\n",
        "\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cuda (GPU support) is available and enabled!\n",
            "Epoch 000: | Loss: 0.46396 \n",
            "Epoch 001: | Loss: 0.19274 \n",
            "Epoch 002: | Loss: 0.15133 \n",
            "Epoch 003: | Loss: 0.13437 \n",
            "Epoch 004: | Loss: 0.11978 \n",
            "Epoch 005: | Loss: 0.10759 \n",
            "Epoch 006: | Loss: 0.09928 \n",
            "Epoch 007: | Loss: 0.09175 \n",
            "Epoch 008: | Loss: 0.09833 \n",
            "Epoch 009: | Loss: 0.07732 \n",
            "Epoch 010: | Loss: 0.07977 \n",
            "Epoch 011: | Loss: 0.06175 \n",
            "Epoch 012: | Loss: 0.05920 \n",
            "Epoch 013: | Loss: 0.05202 \n",
            "Epoch 014: | Loss: 0.04794 \n",
            "Epoch 015: | Loss: 0.05506 \n",
            "Epoch 016: | Loss: 0.05203 \n",
            "Epoch 017: | Loss: 0.03705 \n",
            "Epoch 018: | Loss: 0.03838 \n",
            "Epoch 019: | Loss: 0.02822 \n",
            "Epoch 020: | Loss: 0.02728 \n",
            "Epoch 021: | Loss: 0.02304 \n",
            "Epoch 022: | Loss: 0.02226 \n",
            "Epoch 023: | Loss: 0.01834 \n",
            "Epoch 024: | Loss: 0.01685 \n",
            "Epoch 025: | Loss: 0.01680 \n",
            "Epoch 026: | Loss: 0.01576 \n",
            "Epoch 027: | Loss: 0.03529 \n",
            "Epoch 028: | Loss: 0.01180 \n",
            "Epoch 029: | Loss: 0.00993 \n",
            "Epoch 030: | Loss: 0.01258 \n",
            "Epoch 031: | Loss: 0.01155 \n",
            "Epoch 032: | Loss: 0.00820 \n",
            "Epoch 033: | Loss: 0.00785 \n",
            "Epoch 034: | Loss: 0.00641 \n",
            "Epoch 035: | Loss: 0.00664 \n",
            "Epoch 036: | Loss: 0.00647 \n",
            "Epoch 037: | Loss: 0.00593 \n",
            "Epoch 038: | Loss: 0.00554 \n",
            "Epoch 039: | Loss: 0.00554 \n",
            "Epoch 040: | Loss: 0.00628 \n",
            "Epoch 041: | Loss: 0.00443 \n",
            "Epoch 042: | Loss: 0.00384 \n",
            "Epoch 043: | Loss: 0.00397 \n",
            "Epoch 044: | Loss: 0.00448 \n",
            "Epoch 045: | Loss: 0.00308 \n",
            "Epoch 046: | Loss: 0.00702 \n",
            "Epoch 047: | Loss: 0.00572 \n",
            "Epoch 048: | Loss: 0.00642 \n",
            "Epoch 049: | Loss: 0.00246 \n",
            "Epoch 050: | Loss: 0.00284 \n",
            "Epoch 051: | Loss: 0.00247 \n",
            "Epoch 052: | Loss: 0.00245 \n",
            "Epoch 053: | Loss: 0.00245 \n",
            "Epoch 054: | Loss: 0.00224 \n",
            "Epoch 055: | Loss: 0.00204 \n",
            "Epoch 056: | Loss: 0.00199 \n",
            "Epoch 057: | Loss: 0.00208 \n",
            "Epoch 058: | Loss: 0.00304 \n",
            "Epoch 059: | Loss: 0.00205 \n",
            "Epoch 060: | Loss: 0.00675 \n",
            "Epoch 061: | Loss: 0.00156 \n",
            "Epoch 062: | Loss: 0.00153 \n",
            "Epoch 063: | Loss: 0.00192 \n",
            "Epoch 064: | Loss: 0.00155 \n",
            "Epoch 065: | Loss: 0.00134 \n",
            "Epoch 066: | Loss: 0.00173 \n",
            "Epoch 067: | Loss: 0.00427 \n",
            "Epoch 068: | Loss: 0.00142 \n",
            "Epoch 069: | Loss: 0.00309 \n",
            "Epoch 070: | Loss: 0.00134 \n",
            "Epoch 071: | Loss: 0.00125 \n",
            "Epoch 072: | Loss: 0.00150 \n",
            "Epoch 073: | Loss: 0.00110 \n",
            "Epoch 074: | Loss: 0.00105 \n",
            "Epoch 075: | Loss: 0.00171 \n",
            "Epoch 076: | Loss: 0.00096 \n",
            "Epoch 077: | Loss: 0.00172 \n",
            "Epoch 078: | Loss: 0.00108 \n",
            "Epoch 079: | Loss: 0.00109 \n",
            "Epoch 080: | Loss: 0.00106 \n",
            "Epoch 081: | Loss: 0.00091 \n",
            "Epoch 082: | Loss: 0.00145 \n",
            "Epoch 083: | Loss: 0.00088 \n",
            "Epoch 084: | Loss: 0.00095 \n",
            "Epoch 085: | Loss: 0.00116 \n",
            "Epoch 086: | Loss: 0.00089 \n",
            "Epoch 087: | Loss: 0.00081 \n",
            "Epoch 088: | Loss: 0.00064 \n",
            "Epoch 089: | Loss: 0.00091 \n",
            "Epoch 090: | Loss: 0.00124 \n",
            "Epoch 091: | Loss: 0.00078 \n",
            "Epoch 092: | Loss: 0.00119 \n",
            "Epoch 093: | Loss: 0.00111 \n",
            "Epoch 094: | Loss: 0.00101 \n",
            "Epoch 095: | Loss: 0.00131 \n",
            "Epoch 096: | Loss: 0.00080 \n",
            "Epoch 097: | Loss: 0.00143 \n",
            "Epoch 098: | Loss: 0.00114 \n",
            "Epoch 099: | Loss: 0.00069 \n",
            "Epoch 100: | Loss: 0.00104 \n",
            "Epoch 101: | Loss: 0.00086 \n",
            "Epoch 102: | Loss: 0.00054 \n",
            "Epoch 103: | Loss: 0.00067 \n",
            "Epoch 104: | Loss: 0.00059 \n",
            "Epoch 105: | Loss: 0.00065 \n",
            "Epoch 106: | Loss: 0.00065 \n",
            "Epoch 107: | Loss: 0.00135 \n",
            "Epoch 108: | Loss: 0.00063 \n",
            "Epoch 109: | Loss: 0.00055 \n",
            "Epoch 110: | Loss: 0.00087 \n",
            "Epoch 111: | Loss: 0.00052 \n",
            "Epoch 112: | Loss: 0.00046 \n",
            "Epoch 113: | Loss: 0.00045 \n",
            "Epoch 114: | Loss: 0.00049 \n",
            "Epoch 115: | Loss: 0.00053 \n",
            "Epoch 116: | Loss: 0.00047 \n",
            "Epoch 117: | Loss: 0.00039 \n",
            "Epoch 118: | Loss: 0.00073 \n",
            "Epoch 119: | Loss: 0.00070 \n",
            "Epoch 120: | Loss: 0.00096 \n",
            "Epoch 121: | Loss: 0.00049 \n",
            "Epoch 122: | Loss: 0.00048 \n",
            "Epoch 123: | Loss: 0.00051 \n",
            "Epoch 124: | Loss: 0.00047 \n",
            "Epoch 125: | Loss: 0.00083 \n",
            "Epoch 126: | Loss: 0.00051 \n",
            "Epoch 127: | Loss: 0.00030 \n",
            "Epoch 128: | Loss: 0.00035 \n",
            "Epoch 129: | Loss: 0.00046 \n",
            "Epoch 130: | Loss: 0.00032 \n",
            "Epoch 131: | Loss: 0.00116 \n",
            "Epoch 132: | Loss: 0.00052 \n",
            "Epoch 133: | Loss: 0.00075 \n",
            "Epoch 134: | Loss: 0.00034 \n",
            "Epoch 135: | Loss: 0.00028 \n",
            "Epoch 136: | Loss: 0.00032 \n",
            "Epoch 137: | Loss: 0.00038 \n",
            "Epoch 138: | Loss: 0.00037 \n",
            "Epoch 139: | Loss: 0.00025 \n",
            "Epoch 140: | Loss: 0.00034 \n",
            "Epoch 141: | Loss: 0.00032 \n",
            "Epoch 142: | Loss: 0.00037 \n",
            "Epoch 143: | Loss: 0.00052 \n",
            "Epoch 144: | Loss: 0.00071 \n",
            "Epoch 145: | Loss: 0.00062 \n",
            "Epoch 146: | Loss: 0.00024 \n",
            "Epoch 147: | Loss: 0.00033 \n",
            "Epoch 148: | Loss: 0.00037 \n",
            "Epoch 149: | Loss: 0.00030 \n",
            "Epoch 150: | Loss: 0.00026 \n",
            "Epoch 151: | Loss: 0.00032 \n",
            "Epoch 152: | Loss: 0.00046 \n",
            "Epoch 153: | Loss: 0.00026 \n",
            "Epoch 154: | Loss: 0.00066 \n",
            "Epoch 155: | Loss: 0.00206 \n",
            "Epoch 156: | Loss: 0.00025 \n",
            "Epoch 157: | Loss: 0.00036 \n",
            "Epoch 158: | Loss: 0.00031 \n",
            "Epoch 159: | Loss: 0.00048 \n",
            "Epoch 160: | Loss: 0.00027 \n",
            "Epoch 161: | Loss: 0.00037 \n",
            "Epoch 162: | Loss: 0.00026 \n",
            "Epoch 163: | Loss: 0.00032 \n",
            "Epoch 164: | Loss: 0.00024 \n",
            "Epoch 165: | Loss: 0.00024 \n",
            "Epoch 166: | Loss: 0.00025 \n",
            "Epoch 167: | Loss: 0.00034 \n",
            "Epoch 168: | Loss: 0.00029 \n",
            "Epoch 169: | Loss: 0.00018 \n",
            "Epoch 170: | Loss: 0.00020 \n",
            "Epoch 171: | Loss: 0.00040 \n",
            "Epoch 172: | Loss: 0.00027 \n",
            "Epoch 173: | Loss: 0.00021 \n",
            "Epoch 174: | Loss: 0.00022 \n",
            "Epoch 175: | Loss: 0.00017 \n",
            "Epoch 176: | Loss: 0.00036 \n",
            "Epoch 177: | Loss: 0.00034 \n",
            "Epoch 178: | Loss: 0.00019 \n",
            "Epoch 179: | Loss: 0.00025 \n",
            "Epoch 180: | Loss: 0.00020 \n",
            "Epoch 181: | Loss: 0.00033 \n",
            "Epoch 182: | Loss: 0.00037 \n",
            "Epoch 183: | Loss: 0.00035 \n",
            "Epoch 184: | Loss: 0.00016 \n",
            "Epoch 185: | Loss: 0.00020 \n",
            "Epoch 186: | Loss: 0.00024 \n",
            "Epoch 187: | Loss: 0.00028 \n",
            "Epoch 188: | Loss: 0.00028 \n",
            "Epoch 189: | Loss: 0.00019 \n",
            "Epoch 190: | Loss: 0.00016 \n",
            "Epoch 191: | Loss: 0.00014 \n",
            "Epoch 192: | Loss: 0.00024 \n",
            "Epoch 193: | Loss: 0.00053 \n",
            "Epoch 194: | Loss: 0.00026 \n",
            "Epoch 195: | Loss: 0.00030 \n",
            "Epoch 196: | Loss: 0.00017 \n",
            "Epoch 197: | Loss: 0.00033 \n",
            "Epoch 198: | Loss: 0.00017 \n",
            "Epoch 199: | Loss: 0.00019 \n",
            "Epoch 200: | Loss: 0.00016 \n",
            "Epoch 201: | Loss: 0.00013 \n",
            "Epoch 202: | Loss: 0.00017 \n",
            "Epoch 203: | Loss: 0.00014 \n",
            "Epoch 204: | Loss: 0.00013 \n",
            "Epoch 205: | Loss: 0.00019 \n",
            "Epoch 206: | Loss: 0.00040 \n",
            "Epoch 207: | Loss: 0.00016 \n",
            "Epoch 208: | Loss: 0.00026 \n",
            "Epoch 209: | Loss: 0.00014 \n",
            "Epoch 210: | Loss: 0.00016 \n",
            "Epoch 211: | Loss: 0.00012 \n",
            "Epoch 212: | Loss: 0.00019 \n",
            "Epoch 213: | Loss: 0.00019 \n",
            "Epoch 214: | Loss: 0.00012 \n",
            "Epoch 215: | Loss: 0.00017 \n",
            "Epoch 216: | Loss: 0.00012 \n",
            "Epoch 217: | Loss: 0.00017 \n",
            "Epoch 218: | Loss: 0.00014 \n",
            "Epoch 219: | Loss: 0.00020 \n",
            "Epoch 220: | Loss: 0.00016 \n",
            "Epoch 221: | Loss: 0.00025 \n",
            "Epoch 222: | Loss: 0.00023 \n",
            "Epoch 223: | Loss: 0.00012 \n",
            "Epoch 224: | Loss: 0.00014 \n",
            "Epoch 225: | Loss: 0.00011 \n",
            "Epoch 226: | Loss: 0.00016 \n",
            "Epoch 227: | Loss: 0.00015 \n",
            "Epoch 228: | Loss: 0.00011 \n",
            "Epoch 229: | Loss: 0.00015 \n",
            "Epoch 230: | Loss: 0.00011 \n",
            "Epoch 231: | Loss: 0.00011 \n",
            "Epoch 232: | Loss: 0.00016 \n",
            "Epoch 233: | Loss: 0.00010 \n",
            "Epoch 234: | Loss: 0.00009 \n",
            "Epoch 235: | Loss: 0.00013 \n",
            "Epoch 236: | Loss: 0.00009 \n",
            "Epoch 237: | Loss: 0.00010 \n",
            "Epoch 238: | Loss: 0.00011 \n",
            "Epoch 239: | Loss: 0.00009 \n",
            "Epoch 240: | Loss: 0.00023 \n",
            "Epoch 241: | Loss: 0.00011 \n",
            "Epoch 242: | Loss: 0.00032 \n",
            "Epoch 243: | Loss: 0.00019 \n",
            "Epoch 244: | Loss: 0.00009 \n",
            "Epoch 245: | Loss: 0.00010 \n",
            "Epoch 246: | Loss: 0.00009 \n",
            "Epoch 247: | Loss: 0.00014 \n",
            "Epoch 248: | Loss: 0.00012 \n",
            "Epoch 249: | Loss: 0.00011 \n",
            "Epoch 250: | Loss: 0.00009 \n",
            "Epoch 251: | Loss: 0.00010 \n",
            "Epoch 252: | Loss: 0.00008 \n",
            "Epoch 253: | Loss: 0.00009 \n",
            "Epoch 254: | Loss: 0.00024 \n",
            "Epoch 255: | Loss: 0.00008 \n",
            "Epoch 256: | Loss: 0.00009 \n",
            "Epoch 257: | Loss: 0.00013 \n",
            "Epoch 258: | Loss: 0.00014 \n",
            "Epoch 259: | Loss: 0.00013 \n",
            "Epoch 260: | Loss: 0.00014 \n",
            "Epoch 261: | Loss: 0.00009 \n",
            "Epoch 262: | Loss: 0.00010 \n",
            "Epoch 263: | Loss: 0.00046 \n",
            "Epoch 264: | Loss: 0.00010 \n",
            "Epoch 265: | Loss: 0.00074 \n",
            "Epoch 266: | Loss: 0.00010 \n",
            "Epoch 267: | Loss: 0.00012 \n",
            "Epoch 268: | Loss: 0.00008 \n",
            "Epoch 269: | Loss: 0.00019 \n",
            "Epoch 270: | Loss: 0.00011 \n",
            "Epoch 271: | Loss: 0.00009 \n",
            "Epoch 272: | Loss: 0.00040 \n",
            "Epoch 273: | Loss: 0.00010 \n",
            "Epoch 274: | Loss: 0.00008 \n",
            "Epoch 275: | Loss: 0.00008 \n",
            "Epoch 276: | Loss: 0.00009 \n",
            "Epoch 277: | Loss: 0.00008 \n",
            "Epoch 278: | Loss: 0.00008 \n",
            "Epoch 279: | Loss: 0.00011 \n",
            "Epoch 280: | Loss: 0.00031 \n",
            "Epoch 281: | Loss: 0.00009 \n",
            "Epoch 282: | Loss: 0.00011 \n",
            "Epoch 283: | Loss: 0.00010 \n",
            "Epoch 284: | Loss: 0.00009 \n",
            "Epoch 285: | Loss: 0.00012 \n",
            "Epoch 286: | Loss: 0.00007 \n",
            "Epoch 287: | Loss: 0.00016 \n",
            "Epoch 288: | Loss: 0.00008 \n",
            "Epoch 289: | Loss: 0.00009 \n",
            "Epoch 290: | Loss: 0.00024 \n",
            "Epoch 291: | Loss: 0.00006 \n",
            "Epoch 292: | Loss: 0.00008 \n",
            "Epoch 293: | Loss: 0.00008 \n",
            "Epoch 294: | Loss: 0.00016 \n",
            "Epoch 295: | Loss: 0.00007 \n",
            "Epoch 296: | Loss: 0.00016 \n",
            "Epoch 297: | Loss: 0.00007 \n",
            "Epoch 298: | Loss: 0.00007 \n",
            "Epoch 299: | Loss: 0.00009 \n",
            "Epoch 300: | Loss: 0.00006 \n",
            "Epoch 301: | Loss: 0.00011 \n",
            "Epoch 302: | Loss: 0.00033 \n",
            "Epoch 303: | Loss: 0.00006 \n",
            "Epoch 304: | Loss: 0.00007 \n",
            "Epoch 305: | Loss: 0.00007 \n",
            "Epoch 306: | Loss: 0.00006 \n",
            "Epoch 307: | Loss: 0.00007 \n",
            "Epoch 308: | Loss: 0.00007 \n",
            "Epoch 309: | Loss: 0.00005 \n",
            "Epoch 310: | Loss: 0.00008 \n",
            "Epoch 311: | Loss: 0.00007 \n",
            "Epoch 312: | Loss: 0.00006 \n",
            "Epoch 313: | Loss: 0.00007 \n",
            "Epoch 314: | Loss: 0.00006 \n",
            "Epoch 315: | Loss: 0.00008 \n",
            "Epoch 316: | Loss: 0.00009 \n",
            "Epoch 317: | Loss: 0.00024 \n",
            "Epoch 318: | Loss: 0.00008 \n",
            "Epoch 319: | Loss: 0.00005 \n",
            "Epoch 320: | Loss: 0.00005 \n",
            "Epoch 321: | Loss: 0.00007 \n",
            "Epoch 322: | Loss: 0.00005 \n",
            "Epoch 323: | Loss: 0.00007 \n",
            "Epoch 324: | Loss: 0.00008 \n",
            "Epoch 325: | Loss: 0.00008 \n",
            "Epoch 326: | Loss: 0.00013 \n",
            "Epoch 327: | Loss: 0.00010 \n",
            "Epoch 328: | Loss: 0.00011 \n",
            "Epoch 329: | Loss: 0.00012 \n",
            "Epoch 330: | Loss: 0.00006 \n",
            "Epoch 331: | Loss: 0.00004 \n",
            "Epoch 332: | Loss: 0.00007 \n",
            "Epoch 333: | Loss: 0.00008 \n",
            "Epoch 334: | Loss: 0.00009 \n",
            "Epoch 335: | Loss: 0.00011 \n",
            "Epoch 336: | Loss: 0.00006 \n",
            "Epoch 337: | Loss: 0.00005 \n",
            "Epoch 338: | Loss: 0.00023 \n",
            "Epoch 339: | Loss: 0.00004 \n",
            "Epoch 340: | Loss: 0.00005 \n",
            "Epoch 341: | Loss: 0.00011 \n",
            "Epoch 342: | Loss: 0.00005 \n",
            "Epoch 343: | Loss: 0.00006 \n",
            "Epoch 344: | Loss: 0.00008 \n",
            "Epoch 345: | Loss: 0.00006 \n",
            "Epoch 346: | Loss: 0.00005 \n",
            "Epoch 347: | Loss: 0.00005 \n",
            "Epoch 348: | Loss: 0.00008 \n",
            "Epoch 349: | Loss: 0.00005 \n",
            "Epoch 350: | Loss: 0.00005 \n",
            "Epoch 351: | Loss: 0.00006 \n",
            "Epoch 352: | Loss: 0.00024 \n",
            "Epoch 353: | Loss: 0.00006 \n",
            "Epoch 354: | Loss: 0.00006 \n",
            "Epoch 355: | Loss: 0.00007 \n",
            "Epoch 356: | Loss: 0.00011 \n",
            "Epoch 357: | Loss: 0.00004 \n",
            "Epoch 358: | Loss: 0.00006 \n",
            "Epoch 359: | Loss: 0.00005 \n",
            "Epoch 360: | Loss: 0.00004 \n",
            "Epoch 361: | Loss: 0.00005 \n",
            "Epoch 362: | Loss: 0.00016 \n",
            "Epoch 363: | Loss: 0.00005 \n",
            "Epoch 364: | Loss: 0.00004 \n",
            "Epoch 365: | Loss: 0.00007 \n",
            "Epoch 366: | Loss: 0.00011 \n",
            "Epoch 367: | Loss: 0.00006 \n",
            "Epoch 368: | Loss: 0.00005 \n",
            "Epoch 369: | Loss: 0.00009 \n",
            "Epoch 370: | Loss: 0.00004 \n",
            "Epoch 371: | Loss: 0.00005 \n",
            "Epoch 372: | Loss: 0.00004 \n",
            "Epoch 373: | Loss: 0.00005 \n",
            "Epoch 374: | Loss: 0.00004 \n",
            "Epoch 375: | Loss: 0.00005 \n",
            "Epoch 376: | Loss: 0.00007 \n",
            "Epoch 377: | Loss: 0.00004 \n",
            "Epoch 378: | Loss: 0.00005 \n",
            "Epoch 379: | Loss: 0.00007 \n",
            "Epoch 380: | Loss: 0.00006 \n",
            "Epoch 381: | Loss: 0.00004 \n",
            "Epoch 382: | Loss: 0.00005 \n",
            "Epoch 383: | Loss: 0.00007 \n",
            "Epoch 384: | Loss: 0.00020 \n",
            "Epoch 385: | Loss: 0.00004 \n",
            "Epoch 386: | Loss: 0.00004 \n",
            "Epoch 387: | Loss: 0.00005 \n",
            "Epoch 388: | Loss: 0.00009 \n",
            "Epoch 389: | Loss: 0.00006 \n",
            "Epoch 390: | Loss: 0.00007 \n",
            "Epoch 391: | Loss: 0.00004 \n",
            "Epoch 392: | Loss: 0.00004 \n",
            "Epoch 393: | Loss: 0.00004 \n",
            "Epoch 394: | Loss: 0.00034 \n",
            "Epoch 395: | Loss: 0.00010 \n",
            "Epoch 396: | Loss: 0.00005 \n",
            "Epoch 397: | Loss: 0.00008 \n",
            "Epoch 398: | Loss: 0.00013 \n",
            "Epoch 399: | Loss: 0.00008 \n",
            "Epoch 400: | Loss: 0.00017 \n",
            "Epoch 401: | Loss: 0.00013 \n",
            "Epoch 402: | Loss: 0.00005 \n",
            "Epoch 403: | Loss: 0.00004 \n",
            "Epoch 404: | Loss: 0.00005 \n",
            "Epoch 405: | Loss: 0.00005 \n",
            "Epoch 406: | Loss: 0.00004 \n",
            "Epoch 407: | Loss: 0.00007 \n",
            "Epoch 408: | Loss: 0.00012 \n",
            "Epoch 409: | Loss: 0.00004 \n",
            "Epoch 410: | Loss: 0.00004 \n",
            "Epoch 411: | Loss: 0.00006 \n",
            "Epoch 412: | Loss: 0.00008 \n",
            "Epoch 413: | Loss: 0.00004 \n",
            "Epoch 414: | Loss: 0.00005 \n",
            "Epoch 415: | Loss: 0.00021 \n",
            "Epoch 416: | Loss: 0.00004 \n",
            "Epoch 417: | Loss: 0.00004 \n",
            "Epoch 418: | Loss: 0.00004 \n",
            "Epoch 419: | Loss: 0.00005 \n",
            "Epoch 420: | Loss: 0.00006 \n",
            "Epoch 421: | Loss: 0.00005 \n",
            "Epoch 422: | Loss: 0.00004 \n",
            "Epoch 423: | Loss: 0.00004 \n",
            "Epoch 424: | Loss: 0.00003 \n",
            "Epoch 425: | Loss: 0.00008 \n",
            "Epoch 426: | Loss: 0.00004 \n",
            "Epoch 427: | Loss: 0.00003 \n",
            "Epoch 428: | Loss: 0.00005 \n",
            "Epoch 429: | Loss: 0.00004 \n",
            "Epoch 430: | Loss: 0.00003 \n",
            "Epoch 431: | Loss: 0.00003 \n",
            "Epoch 432: | Loss: 0.00003 \n",
            "Epoch 433: | Loss: 0.00004 \n",
            "Epoch 434: | Loss: 0.00004 \n",
            "Epoch 435: | Loss: 0.00007 \n",
            "Epoch 436: | Loss: 0.00003 \n",
            "Epoch 437: | Loss: 0.00004 \n",
            "Epoch 438: | Loss: 0.00003 \n",
            "Epoch 439: | Loss: 0.00003 \n",
            "Epoch 440: | Loss: 0.00003 \n",
            "Epoch 441: | Loss: 0.00007 \n",
            "Epoch 442: | Loss: 0.00003 \n",
            "Epoch 443: | Loss: 0.00003 \n",
            "Epoch 444: | Loss: 0.00003 \n",
            "Epoch 445: | Loss: 0.00003 \n",
            "Epoch 446: | Loss: 0.00006 \n",
            "Epoch 447: | Loss: 0.00003 \n",
            "Epoch 448: | Loss: 0.00004 \n",
            "Epoch 449: | Loss: 0.00004 \n",
            "Epoch 450: | Loss: 0.00003 \n",
            "Epoch 451: | Loss: 0.00003 \n",
            "Epoch 452: | Loss: 0.00005 \n",
            "Epoch 453: | Loss: 0.00010 \n",
            "Epoch 454: | Loss: 0.00008 \n",
            "Epoch 455: | Loss: 0.00006 \n",
            "Epoch 456: | Loss: 0.00004 \n",
            "Epoch 457: | Loss: 0.00004 \n",
            "Epoch 458: | Loss: 0.00003 \n",
            "Epoch 459: | Loss: 0.00005 \n",
            "Epoch 460: | Loss: 0.00003 \n",
            "Epoch 461: | Loss: 0.00003 \n",
            "Epoch 462: | Loss: 0.00015 \n",
            "Epoch 463: | Loss: 0.00003 \n",
            "Epoch 464: | Loss: 0.00003 \n",
            "Epoch 465: | Loss: 0.00003 \n",
            "Epoch 466: | Loss: 0.00004 \n",
            "Epoch 467: | Loss: 0.00003 \n",
            "Epoch 468: | Loss: 0.00006 \n",
            "Epoch 469: | Loss: 0.00002 \n",
            "Epoch 470: | Loss: 0.00007 \n",
            "Epoch 471: | Loss: 0.00002 \n",
            "Epoch 472: | Loss: 0.00003 \n",
            "Epoch 473: | Loss: 0.00004 \n",
            "Epoch 474: | Loss: 0.00003 \n",
            "Epoch 475: | Loss: 0.00003 \n",
            "Epoch 476: | Loss: 0.00004 \n",
            "Epoch 477: | Loss: 0.00003 \n",
            "Epoch 478: | Loss: 0.00002 \n",
            "Epoch 479: | Loss: 0.00004 \n",
            "Epoch 480: | Loss: 0.00002 \n",
            "Epoch 481: | Loss: 0.00003 \n",
            "Epoch 482: | Loss: 0.00003 \n",
            "Epoch 483: | Loss: 0.00003 \n",
            "Epoch 484: | Loss: 0.00003 \n",
            "Epoch 485: | Loss: 0.00003 \n",
            "Epoch 486: | Loss: 0.00002 \n",
            "Epoch 487: | Loss: 0.00006 \n",
            "Epoch 488: | Loss: 0.00003 \n",
            "Epoch 489: | Loss: 0.00002 \n",
            "Epoch 490: | Loss: 0.00004 \n",
            "Epoch 491: | Loss: 0.00003 \n",
            "Epoch 492: | Loss: 0.00002 \n",
            "Epoch 493: | Loss: 0.00003 \n",
            "Epoch 494: | Loss: 0.00002 \n",
            "Epoch 495: | Loss: 0.00002 \n",
            "Epoch 496: | Loss: 0.00005 \n",
            "Epoch 497: | Loss: 0.00012 \n",
            "Epoch 498: | Loss: 0.00003 \n",
            "Epoch 499: | Loss: 0.00004 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVvivh1q_lVk",
        "outputId": "ca363f88-6d0d-4078-f54a-bb20b4e33d10"
      },
      "source": [
        "# FOR READY Datasets\n",
        "# import libraries\n",
        "import random\n",
        "import numpy as np \n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "# Seed\n",
        "random.seed(501)\n",
        "np.random.seed(501)\n",
        "torch.manual_seed(501)\n",
        "\n",
        "# Flood Level\n",
        "flood_level = 0.05\n",
        "\n",
        "# Donot forget to change input shape \n",
        "# Sinus/gauss/spiral = 2/10/2\n",
        "#Mnist = 28*28\n",
        "#Cifar = 3072\n",
        "input_dim = 2\n",
        "\n",
        "# Do not forget to change class number\n",
        "# Sinus/gauss/spiral = 1\n",
        "#mnist/cifar10 = 10\n",
        "class_number = 1\n",
        "\n",
        "# Choose dataset class  \n",
        "batch_size = 100\n",
        "# Noise Low/Middle/High = 0.01/0.05/0.1                                     \n",
        "dataset = Sinusoidal_data(batch_size=batch_size,noise=0.05)\n",
        "#dataset = MNIST_data(batch_size=batch_size)\n",
        "#dataset = Cifar10_data(batch_size=batch_size)\n",
        "\n",
        "# Choose neural network \n",
        "#model = Resnet_18(input_dim, class_number).ResNet18\n",
        "model = MLP_Net(input_dim, class_number)\n",
        "\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Cuda (GPU support) is available and enabled!\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"Cuda (GPU support) is not available :(\")\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "def binary_acc(y_pred, y_test):\n",
        "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
        "\n",
        "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
        "    acc = correct_results_sum/y_test.shape[0]\n",
        "    acc = torch.round(acc * 100)\n",
        "    \n",
        "    return acc\n",
        "\n",
        "def multi_class_acc(y_pred, y_test):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    _, predicted = torch.max(y_pred.data, 1)\n",
        "    total = y_test.shape[0]\n",
        "    correct = (predicted == labels).sum().float()\n",
        "\n",
        "    return torch.round(100 * correct / total)\n",
        "\n",
        "# Main Loop \n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    trainloader , valloader , testloader = dataset.get_data() \n",
        "\n",
        "\n",
        "    if class_number == 1:\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "        \n",
        "    else:    \n",
        "        criterion = nn.CrossEntropyLoss(reduction='mean')\n",
        "\n",
        "    \n",
        "    optimizer = optim.Adam(model.parameters(), weight_decay=0, lr=0.001)\n",
        "\n",
        "    train_loss_history = []\n",
        "    val_loss_history = []  \n",
        "    for epoch in range(500):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "        for  data in trainloader:    \n",
        "            \n",
        "            # Our batch:\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the gradients as PyTorch accumulates them\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Obtain the scores\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Calculate loss\n",
        "            #print(labels.shape)\n",
        "            if class_number == 1:\n",
        "                loss = criterion(outputs.to(device), labels.unsqueeze(1))\n",
        "                acc = binary_acc(outputs.to(device), labels.unsqueeze(1))\n",
        "            else:\n",
        "                loss = criterion(outputs.to(device), labels)\n",
        "                #acc = multi_class_acc(outputs.to(device), labels)\n",
        "\n",
        "            if flood_level > 0:\n",
        "                loss_corrected = abs(loss-flood_level) + flood_level\n",
        "            else:\n",
        "                loss_corrected = loss\n",
        "         \n",
        "\n",
        "            # Backpropagate\n",
        "            loss_corrected.backward()\n",
        "\n",
        "            # Update the weights\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss_history.append(loss.item())\n",
        "            epoch_loss += loss_corrected.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #print(f'Epoch {epoch} / {500}: avg. loss of last 5 iterations {np.sum(loss_history[:-6:-1])/5}')\n",
        "        #print(f'Epoch {epoch+0:03}: | Loss: {epoch_loss/len(trainloader):.5f} | Train Acc: {epoch_acc/len(trainloader):.3f} | Val Acc: {val_acc:.3f}')\n",
        "        print(f'Epoch {epoch+0:03}: | Loss: {epoch_loss/len(trainloader):.5f} | Train Acc: {epoch_acc/len(trainloader):.3f} ')\n",
        "\n",
        "\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cuda (GPU support) is available and enabled!\n",
            "Epoch 000: | Loss: 0.75330 | Train Acc: 45.000 \n",
            "Epoch 001: | Loss: 0.32376 | Train Acc: 88.000 \n",
            "Epoch 002: | Loss: 0.28539 | Train Acc: 90.000 \n",
            "Epoch 003: | Loss: 0.27080 | Train Acc: 90.000 \n",
            "Epoch 004: | Loss: 0.26145 | Train Acc: 90.000 \n",
            "Epoch 005: | Loss: 0.25377 | Train Acc: 91.000 \n",
            "Epoch 006: | Loss: 0.24671 | Train Acc: 91.000 \n",
            "Epoch 007: | Loss: 0.24020 | Train Acc: 92.000 \n",
            "Epoch 008: | Loss: 0.23442 | Train Acc: 92.000 \n",
            "Epoch 009: | Loss: 0.22973 | Train Acc: 92.000 \n",
            "Epoch 010: | Loss: 0.22635 | Train Acc: 93.000 \n",
            "Epoch 011: | Loss: 0.22378 | Train Acc: 93.000 \n",
            "Epoch 012: | Loss: 0.22171 | Train Acc: 93.000 \n",
            "Epoch 013: | Loss: 0.21976 | Train Acc: 93.000 \n",
            "Epoch 014: | Loss: 0.21777 | Train Acc: 93.000 \n",
            "Epoch 015: | Loss: 0.21565 | Train Acc: 93.000 \n",
            "Epoch 016: | Loss: 0.21349 | Train Acc: 93.000 \n",
            "Epoch 017: | Loss: 0.21130 | Train Acc: 93.000 \n",
            "Epoch 018: | Loss: 0.20917 | Train Acc: 94.000 \n",
            "Epoch 019: | Loss: 0.20706 | Train Acc: 95.000 \n",
            "Epoch 020: | Loss: 0.20501 | Train Acc: 95.000 \n",
            "Epoch 021: | Loss: 0.20307 | Train Acc: 95.000 \n",
            "Epoch 022: | Loss: 0.20130 | Train Acc: 95.000 \n",
            "Epoch 023: | Loss: 0.19963 | Train Acc: 95.000 \n",
            "Epoch 024: | Loss: 0.19803 | Train Acc: 95.000 \n",
            "Epoch 025: | Loss: 0.19653 | Train Acc: 95.000 \n",
            "Epoch 026: | Loss: 0.19509 | Train Acc: 95.000 \n",
            "Epoch 027: | Loss: 0.19364 | Train Acc: 95.000 \n",
            "Epoch 028: | Loss: 0.19214 | Train Acc: 95.000 \n",
            "Epoch 029: | Loss: 0.19061 | Train Acc: 95.000 \n",
            "Epoch 030: | Loss: 0.18913 | Train Acc: 95.000 \n",
            "Epoch 031: | Loss: 0.18769 | Train Acc: 95.000 \n",
            "Epoch 032: | Loss: 0.18630 | Train Acc: 95.000 \n",
            "Epoch 033: | Loss: 0.18492 | Train Acc: 96.000 \n",
            "Epoch 034: | Loss: 0.18359 | Train Acc: 96.000 \n",
            "Epoch 035: | Loss: 0.18228 | Train Acc: 96.000 \n",
            "Epoch 036: | Loss: 0.18097 | Train Acc: 96.000 \n",
            "Epoch 037: | Loss: 0.17967 | Train Acc: 96.000 \n",
            "Epoch 038: | Loss: 0.17843 | Train Acc: 96.000 \n",
            "Epoch 039: | Loss: 0.17723 | Train Acc: 96.000 \n",
            "Epoch 040: | Loss: 0.17603 | Train Acc: 96.000 \n",
            "Epoch 041: | Loss: 0.17483 | Train Acc: 96.000 \n",
            "Epoch 042: | Loss: 0.17368 | Train Acc: 96.000 \n",
            "Epoch 043: | Loss: 0.17255 | Train Acc: 96.000 \n",
            "Epoch 044: | Loss: 0.17144 | Train Acc: 96.000 \n",
            "Epoch 045: | Loss: 0.17031 | Train Acc: 96.000 \n",
            "Epoch 046: | Loss: 0.16920 | Train Acc: 96.000 \n",
            "Epoch 047: | Loss: 0.16812 | Train Acc: 96.000 \n",
            "Epoch 048: | Loss: 0.16704 | Train Acc: 96.000 \n",
            "Epoch 049: | Loss: 0.16598 | Train Acc: 96.000 \n",
            "Epoch 050: | Loss: 0.16497 | Train Acc: 96.000 \n",
            "Epoch 051: | Loss: 0.16395 | Train Acc: 96.000 \n",
            "Epoch 052: | Loss: 0.16293 | Train Acc: 96.000 \n",
            "Epoch 053: | Loss: 0.16191 | Train Acc: 96.000 \n",
            "Epoch 054: | Loss: 0.16090 | Train Acc: 96.000 \n",
            "Epoch 055: | Loss: 0.15993 | Train Acc: 96.000 \n",
            "Epoch 056: | Loss: 0.15894 | Train Acc: 96.000 \n",
            "Epoch 057: | Loss: 0.15796 | Train Acc: 96.000 \n",
            "Epoch 058: | Loss: 0.15701 | Train Acc: 96.000 \n",
            "Epoch 059: | Loss: 0.15605 | Train Acc: 96.000 \n",
            "Epoch 060: | Loss: 0.15513 | Train Acc: 96.000 \n",
            "Epoch 061: | Loss: 0.15422 | Train Acc: 96.000 \n",
            "Epoch 062: | Loss: 0.15333 | Train Acc: 96.000 \n",
            "Epoch 063: | Loss: 0.15246 | Train Acc: 96.000 \n",
            "Epoch 064: | Loss: 0.15148 | Train Acc: 96.000 \n",
            "Epoch 065: | Loss: 0.15050 | Train Acc: 96.000 \n",
            "Epoch 066: | Loss: 0.14959 | Train Acc: 96.000 \n",
            "Epoch 067: | Loss: 0.14873 | Train Acc: 96.000 \n",
            "Epoch 068: | Loss: 0.14786 | Train Acc: 96.000 \n",
            "Epoch 069: | Loss: 0.14689 | Train Acc: 96.000 \n",
            "Epoch 070: | Loss: 0.14601 | Train Acc: 96.000 \n",
            "Epoch 071: | Loss: 0.14509 | Train Acc: 96.000 \n",
            "Epoch 072: | Loss: 0.14413 | Train Acc: 96.000 \n",
            "Epoch 073: | Loss: 0.14302 | Train Acc: 96.000 \n",
            "Epoch 074: | Loss: 0.14194 | Train Acc: 96.000 \n",
            "Epoch 075: | Loss: 0.14177 | Train Acc: 96.000 \n",
            "Epoch 076: | Loss: 0.14075 | Train Acc: 96.000 \n",
            "Epoch 077: | Loss: 0.14003 | Train Acc: 96.000 \n",
            "Epoch 078: | Loss: 0.13918 | Train Acc: 96.000 \n",
            "Epoch 079: | Loss: 0.13836 | Train Acc: 96.000 \n",
            "Epoch 080: | Loss: 0.13747 | Train Acc: 96.000 \n",
            "Epoch 081: | Loss: 0.13648 | Train Acc: 96.000 \n",
            "Epoch 082: | Loss: 0.13555 | Train Acc: 96.000 \n",
            "Epoch 083: | Loss: 0.13463 | Train Acc: 96.000 \n",
            "Epoch 084: | Loss: 0.13371 | Train Acc: 96.000 \n",
            "Epoch 085: | Loss: 0.13259 | Train Acc: 97.000 \n",
            "Epoch 086: | Loss: 0.13175 | Train Acc: 97.000 \n",
            "Epoch 087: | Loss: 0.13087 | Train Acc: 97.000 \n",
            "Epoch 088: | Loss: 0.12995 | Train Acc: 97.000 \n",
            "Epoch 089: | Loss: 0.12877 | Train Acc: 97.000 \n",
            "Epoch 090: | Loss: 0.12786 | Train Acc: 97.000 \n",
            "Epoch 091: | Loss: 0.12681 | Train Acc: 97.000 \n",
            "Epoch 092: | Loss: 0.12579 | Train Acc: 97.000 \n",
            "Epoch 093: | Loss: 0.12501 | Train Acc: 97.000 \n",
            "Epoch 094: | Loss: 0.12378 | Train Acc: 97.000 \n",
            "Epoch 095: | Loss: 0.12283 | Train Acc: 97.000 \n",
            "Epoch 096: | Loss: 0.12259 | Train Acc: 97.000 \n",
            "Epoch 097: | Loss: 0.12170 | Train Acc: 97.000 \n",
            "Epoch 098: | Loss: 0.12071 | Train Acc: 97.000 \n",
            "Epoch 099: | Loss: 0.11943 | Train Acc: 97.000 \n",
            "Epoch 100: | Loss: 0.11854 | Train Acc: 97.000 \n",
            "Epoch 101: | Loss: 0.11737 | Train Acc: 97.000 \n",
            "Epoch 102: | Loss: 0.11660 | Train Acc: 97.000 \n",
            "Epoch 103: | Loss: 0.11604 | Train Acc: 97.000 \n",
            "Epoch 104: | Loss: 0.11536 | Train Acc: 97.000 \n",
            "Epoch 105: | Loss: 0.11454 | Train Acc: 97.000 \n",
            "Epoch 106: | Loss: 0.11352 | Train Acc: 97.000 \n",
            "Epoch 107: | Loss: 0.11263 | Train Acc: 97.000 \n",
            "Epoch 108: | Loss: 0.11170 | Train Acc: 97.000 \n",
            "Epoch 109: | Loss: 0.11105 | Train Acc: 97.000 \n",
            "Epoch 110: | Loss: 0.10970 | Train Acc: 97.000 \n",
            "Epoch 111: | Loss: 0.10886 | Train Acc: 97.000 \n",
            "Epoch 112: | Loss: 0.10768 | Train Acc: 97.000 \n",
            "Epoch 113: | Loss: 0.10672 | Train Acc: 97.000 \n",
            "Epoch 114: | Loss: 0.10553 | Train Acc: 97.000 \n",
            "Epoch 115: | Loss: 0.10498 | Train Acc: 97.000 \n",
            "Epoch 116: | Loss: 0.10427 | Train Acc: 97.000 \n",
            "Epoch 117: | Loss: 0.10460 | Train Acc: 97.000 \n",
            "Epoch 118: | Loss: 0.10343 | Train Acc: 97.000 \n",
            "Epoch 119: | Loss: 0.10272 | Train Acc: 97.000 \n",
            "Epoch 120: | Loss: 0.10201 | Train Acc: 97.000 \n",
            "Epoch 121: | Loss: 0.10070 | Train Acc: 97.000 \n",
            "Epoch 122: | Loss: 0.10032 | Train Acc: 97.000 \n",
            "Epoch 123: | Loss: 0.09960 | Train Acc: 97.000 \n",
            "Epoch 124: | Loss: 0.09898 | Train Acc: 97.000 \n",
            "Epoch 125: | Loss: 0.09842 | Train Acc: 97.000 \n",
            "Epoch 126: | Loss: 0.09697 | Train Acc: 97.000 \n",
            "Epoch 127: | Loss: 0.09669 | Train Acc: 97.000 \n",
            "Epoch 128: | Loss: 0.09587 | Train Acc: 97.000 \n",
            "Epoch 129: | Loss: 0.09469 | Train Acc: 97.000 \n",
            "Epoch 130: | Loss: 0.09389 | Train Acc: 97.000 \n",
            "Epoch 131: | Loss: 0.09415 | Train Acc: 97.000 \n",
            "Epoch 132: | Loss: 0.09371 | Train Acc: 97.000 \n",
            "Epoch 133: | Loss: 0.09506 | Train Acc: 97.000 \n",
            "Epoch 134: | Loss: 0.09300 | Train Acc: 97.000 \n",
            "Epoch 135: | Loss: 0.09180 | Train Acc: 97.000 \n",
            "Epoch 136: | Loss: 0.09210 | Train Acc: 97.000 \n",
            "Epoch 137: | Loss: 0.09166 | Train Acc: 97.000 \n",
            "Epoch 138: | Loss: 0.09080 | Train Acc: 97.000 \n",
            "Epoch 139: | Loss: 0.08926 | Train Acc: 97.000 \n",
            "Epoch 140: | Loss: 0.08948 | Train Acc: 97.000 \n",
            "Epoch 141: | Loss: 0.08919 | Train Acc: 97.000 \n",
            "Epoch 142: | Loss: 0.08771 | Train Acc: 97.000 \n",
            "Epoch 143: | Loss: 0.08684 | Train Acc: 97.000 \n",
            "Epoch 144: | Loss: 0.08679 | Train Acc: 97.000 \n",
            "Epoch 145: | Loss: 0.08534 | Train Acc: 97.000 \n",
            "Epoch 146: | Loss: 0.08462 | Train Acc: 97.000 \n",
            "Epoch 147: | Loss: 0.08363 | Train Acc: 97.000 \n",
            "Epoch 148: | Loss: 0.08315 | Train Acc: 97.000 \n",
            "Epoch 149: | Loss: 0.08508 | Train Acc: 97.000 \n",
            "Epoch 150: | Loss: 0.08804 | Train Acc: 98.000 \n",
            "Epoch 151: | Loss: 0.08764 | Train Acc: 97.000 \n",
            "Epoch 152: | Loss: 0.08397 | Train Acc: 97.000 \n",
            "Epoch 153: | Loss: 0.08529 | Train Acc: 97.000 \n",
            "Epoch 154: | Loss: 0.08233 | Train Acc: 97.000 \n",
            "Epoch 155: | Loss: 0.08335 | Train Acc: 97.000 \n",
            "Epoch 156: | Loss: 0.08025 | Train Acc: 97.000 \n",
            "Epoch 157: | Loss: 0.07958 | Train Acc: 97.000 \n",
            "Epoch 158: | Loss: 0.08076 | Train Acc: 97.000 \n",
            "Epoch 159: | Loss: 0.08049 | Train Acc: 97.000 \n",
            "Epoch 160: | Loss: 0.07830 | Train Acc: 97.000 \n",
            "Epoch 161: | Loss: 0.07732 | Train Acc: 97.000 \n",
            "Epoch 162: | Loss: 0.07658 | Train Acc: 97.000 \n",
            "Epoch 163: | Loss: 0.07600 | Train Acc: 97.000 \n",
            "Epoch 164: | Loss: 0.07701 | Train Acc: 97.000 \n",
            "Epoch 165: | Loss: 0.08107 | Train Acc: 97.000 \n",
            "Epoch 166: | Loss: 0.07792 | Train Acc: 97.000 \n",
            "Epoch 167: | Loss: 0.07630 | Train Acc: 97.000 \n",
            "Epoch 168: | Loss: 0.07804 | Train Acc: 97.000 \n",
            "Epoch 169: | Loss: 0.07668 | Train Acc: 97.000 \n",
            "Epoch 170: | Loss: 0.07430 | Train Acc: 97.000 \n",
            "Epoch 171: | Loss: 0.07426 | Train Acc: 97.000 \n",
            "Epoch 172: | Loss: 0.07650 | Train Acc: 98.000 \n",
            "Epoch 173: | Loss: 0.07992 | Train Acc: 97.000 \n",
            "Epoch 174: | Loss: 0.07261 | Train Acc: 97.000 \n",
            "Epoch 175: | Loss: 0.07544 | Train Acc: 98.000 \n",
            "Epoch 176: | Loss: 0.07546 | Train Acc: 97.000 \n",
            "Epoch 177: | Loss: 0.07234 | Train Acc: 97.000 \n",
            "Epoch 178: | Loss: 0.07314 | Train Acc: 98.000 \n",
            "Epoch 179: | Loss: 0.07349 | Train Acc: 97.000 \n",
            "Epoch 180: | Loss: 0.07026 | Train Acc: 97.000 \n",
            "Epoch 181: | Loss: 0.06944 | Train Acc: 97.000 \n",
            "Epoch 182: | Loss: 0.06881 | Train Acc: 97.000 \n",
            "Epoch 183: | Loss: 0.06793 | Train Acc: 97.000 \n",
            "Epoch 184: | Loss: 0.06727 | Train Acc: 97.000 \n",
            "Epoch 185: | Loss: 0.07046 | Train Acc: 97.000 \n",
            "Epoch 186: | Loss: 0.07422 | Train Acc: 97.000 \n",
            "Epoch 187: | Loss: 0.07505 | Train Acc: 97.000 \n",
            "Epoch 188: | Loss: 0.07032 | Train Acc: 97.000 \n",
            "Epoch 189: | Loss: 0.07357 | Train Acc: 99.000 \n",
            "Epoch 190: | Loss: 0.07034 | Train Acc: 97.000 \n",
            "Epoch 191: | Loss: 0.06976 | Train Acc: 97.000 \n",
            "Epoch 192: | Loss: 0.06830 | Train Acc: 97.000 \n",
            "Epoch 193: | Loss: 0.06647 | Train Acc: 97.000 \n",
            "Epoch 194: | Loss: 0.06712 | Train Acc: 97.000 \n",
            "Epoch 195: | Loss: 0.06945 | Train Acc: 98.000 \n",
            "Epoch 196: | Loss: 0.07264 | Train Acc: 97.000 \n",
            "Epoch 197: | Loss: 0.06741 | Train Acc: 97.000 \n",
            "Epoch 198: | Loss: 0.06807 | Train Acc: 97.000 \n",
            "Epoch 199: | Loss: 0.07039 | Train Acc: 97.000 \n",
            "Epoch 200: | Loss: 0.06700 | Train Acc: 97.000 \n",
            "Epoch 201: | Loss: 0.06568 | Train Acc: 97.000 \n",
            "Epoch 202: | Loss: 0.06719 | Train Acc: 97.000 \n",
            "Epoch 203: | Loss: 0.06639 | Train Acc: 98.000 \n",
            "Epoch 204: | Loss: 0.06539 | Train Acc: 97.000 \n",
            "Epoch 205: | Loss: 0.06457 | Train Acc: 98.000 \n",
            "Epoch 206: | Loss: 0.06380 | Train Acc: 97.000 \n",
            "Epoch 207: | Loss: 0.06272 | Train Acc: 97.000 \n",
            "Epoch 208: | Loss: 0.06299 | Train Acc: 98.000 \n",
            "Epoch 209: | Loss: 0.06277 | Train Acc: 97.000 \n",
            "Epoch 210: | Loss: 0.06857 | Train Acc: 98.000 \n",
            "Epoch 211: | Loss: 0.08712 | Train Acc: 97.000 \n",
            "Epoch 212: | Loss: 0.06627 | Train Acc: 97.000 \n",
            "Epoch 213: | Loss: 0.08427 | Train Acc: 96.000 \n",
            "Epoch 214: | Loss: 0.07195 | Train Acc: 97.000 \n",
            "Epoch 215: | Loss: 0.07718 | Train Acc: 97.000 \n",
            "Epoch 216: | Loss: 0.07320 | Train Acc: 97.000 \n",
            "Epoch 217: | Loss: 0.07221 | Train Acc: 97.000 \n",
            "Epoch 218: | Loss: 0.07097 | Train Acc: 97.000 \n",
            "Epoch 219: | Loss: 0.07075 | Train Acc: 97.000 \n",
            "Epoch 220: | Loss: 0.06941 | Train Acc: 97.000 \n",
            "Epoch 221: | Loss: 0.06917 | Train Acc: 97.000 \n",
            "Epoch 222: | Loss: 0.06589 | Train Acc: 97.000 \n",
            "Epoch 223: | Loss: 0.06653 | Train Acc: 97.000 \n",
            "Epoch 224: | Loss: 0.06413 | Train Acc: 97.000 \n",
            "Epoch 225: | Loss: 0.06511 | Train Acc: 97.000 \n",
            "Epoch 226: | Loss: 0.06354 | Train Acc: 97.000 \n",
            "Epoch 227: | Loss: 0.06389 | Train Acc: 98.000 \n",
            "Epoch 228: | Loss: 0.06347 | Train Acc: 97.000 \n",
            "Epoch 229: | Loss: 0.06247 | Train Acc: 97.000 \n",
            "Epoch 230: | Loss: 0.06087 | Train Acc: 97.000 \n",
            "Epoch 231: | Loss: 0.06115 | Train Acc: 97.000 \n",
            "Epoch 232: | Loss: 0.06114 | Train Acc: 98.000 \n",
            "Epoch 233: | Loss: 0.06041 | Train Acc: 97.000 \n",
            "Epoch 234: | Loss: 0.05982 | Train Acc: 97.000 \n",
            "Epoch 235: | Loss: 0.05955 | Train Acc: 97.000 \n",
            "Epoch 236: | Loss: 0.05887 | Train Acc: 97.000 \n",
            "Epoch 237: | Loss: 0.05803 | Train Acc: 97.000 \n",
            "Epoch 238: | Loss: 0.05719 | Train Acc: 97.000 \n",
            "Epoch 239: | Loss: 0.05669 | Train Acc: 97.000 \n",
            "Epoch 240: | Loss: 0.05625 | Train Acc: 98.000 \n",
            "Epoch 241: | Loss: 0.05675 | Train Acc: 97.000 \n",
            "Epoch 242: | Loss: 0.05874 | Train Acc: 99.000 \n",
            "Epoch 243: | Loss: 0.06516 | Train Acc: 97.000 \n",
            "Epoch 244: | Loss: 0.05928 | Train Acc: 98.000 \n",
            "Epoch 245: | Loss: 0.05725 | Train Acc: 98.000 \n",
            "Epoch 246: | Loss: 0.05930 | Train Acc: 97.000 \n",
            "Epoch 247: | Loss: 0.05832 | Train Acc: 98.000 \n",
            "Epoch 248: | Loss: 0.05603 | Train Acc: 98.000 \n",
            "Epoch 249: | Loss: 0.05975 | Train Acc: 98.000 \n",
            "Epoch 250: | Loss: 0.05829 | Train Acc: 98.000 \n",
            "Epoch 251: | Loss: 0.05896 | Train Acc: 97.000 \n",
            "Epoch 252: | Loss: 0.05455 | Train Acc: 98.000 \n",
            "Epoch 253: | Loss: 0.05559 | Train Acc: 98.000 \n",
            "Epoch 254: | Loss: 0.05703 | Train Acc: 97.000 \n",
            "Epoch 255: | Loss: 0.05744 | Train Acc: 98.000 \n",
            "Epoch 256: | Loss: 0.06636 | Train Acc: 97.000 \n",
            "Epoch 257: | Loss: 0.05943 | Train Acc: 98.000 \n",
            "Epoch 258: | Loss: 0.05650 | Train Acc: 97.000 \n",
            "Epoch 259: | Loss: 0.05697 | Train Acc: 98.000 \n",
            "Epoch 260: | Loss: 0.05775 | Train Acc: 98.000 \n",
            "Epoch 261: | Loss: 0.06096 | Train Acc: 97.000 \n",
            "Epoch 262: | Loss: 0.05420 | Train Acc: 98.000 \n",
            "Epoch 263: | Loss: 0.05422 | Train Acc: 99.000 \n",
            "Epoch 264: | Loss: 0.05646 | Train Acc: 97.000 \n",
            "Epoch 265: | Loss: 0.05388 | Train Acc: 98.000 \n",
            "Epoch 266: | Loss: 0.05234 | Train Acc: 97.000 \n",
            "Epoch 267: | Loss: 0.05092 | Train Acc: 98.000 \n",
            "Epoch 268: | Loss: 0.05218 | Train Acc: 99.000 \n",
            "Epoch 269: | Loss: 0.06114 | Train Acc: 97.000 \n",
            "Epoch 270: | Loss: 0.06227 | Train Acc: 97.000 \n",
            "Epoch 271: | Loss: 0.06659 | Train Acc: 98.000 \n",
            "Epoch 272: | Loss: 0.05773 | Train Acc: 97.000 \n",
            "Epoch 273: | Loss: 0.06707 | Train Acc: 97.000 \n",
            "Epoch 274: | Loss: 0.05632 | Train Acc: 97.000 \n",
            "Epoch 275: | Loss: 0.05994 | Train Acc: 97.000 \n",
            "Epoch 276: | Loss: 0.05608 | Train Acc: 99.000 \n",
            "Epoch 277: | Loss: 0.05679 | Train Acc: 99.000 \n",
            "Epoch 278: | Loss: 0.05818 | Train Acc: 98.000 \n",
            "Epoch 279: | Loss: 0.05547 | Train Acc: 97.000 \n",
            "Epoch 280: | Loss: 0.05853 | Train Acc: 99.000 \n",
            "Epoch 281: | Loss: 0.06132 | Train Acc: 98.000 \n",
            "Epoch 282: | Loss: 0.05295 | Train Acc: 98.000 \n",
            "Epoch 283: | Loss: 0.05333 | Train Acc: 98.000 \n",
            "Epoch 284: | Loss: 0.05576 | Train Acc: 97.000 \n",
            "Epoch 285: | Loss: 0.05495 | Train Acc: 98.000 \n",
            "Epoch 286: | Loss: 0.05239 | Train Acc: 97.000 \n",
            "Epoch 287: | Loss: 0.05089 | Train Acc: 98.000 \n",
            "Epoch 288: | Loss: 0.05098 | Train Acc: 99.000 \n",
            "Epoch 289: | Loss: 0.05248 | Train Acc: 97.000 \n",
            "Epoch 290: | Loss: 0.05101 | Train Acc: 98.000 \n",
            "Epoch 291: | Loss: 0.05019 | Train Acc: 97.000 \n",
            "Epoch 292: | Loss: 0.07620 | Train Acc: 98.000 \n",
            "Epoch 293: | Loss: 0.05565 | Train Acc: 98.000 \n",
            "Epoch 294: | Loss: 0.06844 | Train Acc: 97.000 \n",
            "Epoch 295: | Loss: 0.06663 | Train Acc: 97.000 \n",
            "Epoch 296: | Loss: 0.06515 | Train Acc: 97.000 \n",
            "Epoch 297: | Loss: 0.06158 | Train Acc: 97.000 \n",
            "Epoch 298: | Loss: 0.06214 | Train Acc: 98.000 \n",
            "Epoch 299: | Loss: 0.06054 | Train Acc: 98.000 \n",
            "Epoch 300: | Loss: 0.05917 | Train Acc: 98.000 \n",
            "Epoch 301: | Loss: 0.05770 | Train Acc: 97.000 \n",
            "Epoch 302: | Loss: 0.05887 | Train Acc: 97.000 \n",
            "Epoch 303: | Loss: 0.05591 | Train Acc: 97.000 \n",
            "Epoch 304: | Loss: 0.05701 | Train Acc: 98.000 \n",
            "Epoch 305: | Loss: 0.05491 | Train Acc: 98.000 \n",
            "Epoch 306: | Loss: 0.05356 | Train Acc: 98.000 \n",
            "Epoch 307: | Loss: 0.05351 | Train Acc: 98.000 \n",
            "Epoch 308: | Loss: 0.05205 | Train Acc: 97.000 \n",
            "Epoch 309: | Loss: 0.05183 | Train Acc: 98.000 \n",
            "Epoch 310: | Loss: 0.05008 | Train Acc: 98.000 \n",
            "Epoch 311: | Loss: 0.05033 | Train Acc: 98.000 \n",
            "Epoch 312: | Loss: 0.05056 | Train Acc: 98.000 \n",
            "Epoch 313: | Loss: 0.05198 | Train Acc: 98.000 \n",
            "Epoch 314: | Loss: 0.05234 | Train Acc: 97.000 \n",
            "Epoch 315: | Loss: 0.05168 | Train Acc: 98.000 \n",
            "Epoch 316: | Loss: 0.05122 | Train Acc: 97.000 \n",
            "Epoch 317: | Loss: 0.05052 | Train Acc: 98.000 \n",
            "Epoch 318: | Loss: 0.06053 | Train Acc: 97.000 \n",
            "Epoch 319: | Loss: 0.07895 | Train Acc: 97.000 \n",
            "Epoch 320: | Loss: 0.06239 | Train Acc: 97.000 \n",
            "Epoch 321: | Loss: 0.06388 | Train Acc: 97.000 \n",
            "Epoch 322: | Loss: 0.05597 | Train Acc: 98.000 \n",
            "Epoch 323: | Loss: 0.05943 | Train Acc: 98.000 \n",
            "Epoch 324: | Loss: 0.05774 | Train Acc: 97.000 \n",
            "Epoch 325: | Loss: 0.05603 | Train Acc: 98.000 \n",
            "Epoch 326: | Loss: 0.05392 | Train Acc: 99.000 \n",
            "Epoch 327: | Loss: 0.05586 | Train Acc: 98.000 \n",
            "Epoch 328: | Loss: 0.05191 | Train Acc: 98.000 \n",
            "Epoch 329: | Loss: 0.05440 | Train Acc: 99.000 \n",
            "Epoch 330: | Loss: 0.05195 | Train Acc: 97.000 \n",
            "Epoch 331: | Loss: 0.05127 | Train Acc: 97.000 \n",
            "Epoch 332: | Loss: 0.05092 | Train Acc: 98.000 \n",
            "Epoch 333: | Loss: 0.05057 | Train Acc: 97.000 \n",
            "Epoch 334: | Loss: 0.07112 | Train Acc: 97.000 \n",
            "Epoch 335: | Loss: 0.05260 | Train Acc: 97.000 \n",
            "Epoch 336: | Loss: 0.06855 | Train Acc: 97.000 \n",
            "Epoch 337: | Loss: 0.06151 | Train Acc: 97.000 \n",
            "Epoch 338: | Loss: 0.06221 | Train Acc: 97.000 \n",
            "Epoch 339: | Loss: 0.05729 | Train Acc: 97.000 \n",
            "Epoch 340: | Loss: 0.05980 | Train Acc: 98.000 \n",
            "Epoch 341: | Loss: 0.05886 | Train Acc: 97.000 \n",
            "Epoch 342: | Loss: 0.05547 | Train Acc: 98.000 \n",
            "Epoch 343: | Loss: 0.05569 | Train Acc: 99.000 \n",
            "Epoch 344: | Loss: 0.05480 | Train Acc: 98.000 \n",
            "Epoch 345: | Loss: 0.05292 | Train Acc: 97.000 \n",
            "Epoch 346: | Loss: 0.05266 | Train Acc: 99.000 \n",
            "Epoch 347: | Loss: 0.05265 | Train Acc: 99.000 \n",
            "Epoch 348: | Loss: 0.05077 | Train Acc: 98.000 \n",
            "Epoch 349: | Loss: 0.05023 | Train Acc: 97.000 \n",
            "Epoch 350: | Loss: 0.06534 | Train Acc: 97.000 \n",
            "Epoch 351: | Loss: 0.05019 | Train Acc: 98.000 \n",
            "Epoch 352: | Loss: 0.05769 | Train Acc: 98.000 \n",
            "Epoch 353: | Loss: 0.05474 | Train Acc: 97.000 \n",
            "Epoch 354: | Loss: 0.05267 | Train Acc: 97.000 \n",
            "Epoch 355: | Loss: 0.05450 | Train Acc: 99.000 \n",
            "Epoch 356: | Loss: 0.05034 | Train Acc: 98.000 \n",
            "Epoch 357: | Loss: 0.05155 | Train Acc: 98.000 \n",
            "Epoch 358: | Loss: 0.05000 | Train Acc: 98.000 \n",
            "Epoch 359: | Loss: 0.05839 | Train Acc: 98.000 \n",
            "Epoch 360: | Loss: 0.05233 | Train Acc: 98.000 \n",
            "Epoch 361: | Loss: 0.05340 | Train Acc: 99.000 \n",
            "Epoch 362: | Loss: 0.05326 | Train Acc: 97.000 \n",
            "Epoch 363: | Loss: 0.05142 | Train Acc: 97.000 \n",
            "Epoch 364: | Loss: 0.05178 | Train Acc: 98.000 \n",
            "Epoch 365: | Loss: 0.05355 | Train Acc: 98.000 \n",
            "Epoch 366: | Loss: 0.05855 | Train Acc: 97.000 \n",
            "Epoch 367: | Loss: 0.05150 | Train Acc: 98.000 \n",
            "Epoch 368: | Loss: 0.07364 | Train Acc: 97.000 \n",
            "Epoch 369: | Loss: 0.06094 | Train Acc: 97.000 \n",
            "Epoch 370: | Loss: 0.06828 | Train Acc: 97.000 \n",
            "Epoch 371: | Loss: 0.05637 | Train Acc: 98.000 \n",
            "Epoch 372: | Loss: 0.06230 | Train Acc: 97.000 \n",
            "Epoch 373: | Loss: 0.05617 | Train Acc: 98.000 \n",
            "Epoch 374: | Loss: 0.05720 | Train Acc: 97.000 \n",
            "Epoch 375: | Loss: 0.05957 | Train Acc: 97.000 \n",
            "Epoch 376: | Loss: 0.05165 | Train Acc: 97.000 \n",
            "Epoch 377: | Loss: 0.05642 | Train Acc: 98.000 \n",
            "Epoch 378: | Loss: 0.05087 | Train Acc: 98.000 \n",
            "Epoch 379: | Loss: 0.05276 | Train Acc: 97.000 \n",
            "Epoch 380: | Loss: 0.05071 | Train Acc: 97.000 \n",
            "Epoch 381: | Loss: 0.05047 | Train Acc: 99.000 \n",
            "Epoch 382: | Loss: 0.08478 | Train Acc: 96.000 \n",
            "Epoch 383: | Loss: 0.05578 | Train Acc: 97.000 \n",
            "Epoch 384: | Loss: 0.07533 | Train Acc: 97.000 \n",
            "Epoch 385: | Loss: 0.05567 | Train Acc: 97.000 \n",
            "Epoch 386: | Loss: 0.05791 | Train Acc: 98.000 \n",
            "Epoch 387: | Loss: 0.05827 | Train Acc: 98.000 \n",
            "Epoch 388: | Loss: 0.05432 | Train Acc: 97.000 \n",
            "Epoch 389: | Loss: 0.05790 | Train Acc: 97.000 \n",
            "Epoch 390: | Loss: 0.05168 | Train Acc: 98.000 \n",
            "Epoch 391: | Loss: 0.05700 | Train Acc: 99.000 \n",
            "Epoch 392: | Loss: 0.05155 | Train Acc: 98.000 \n",
            "Epoch 393: | Loss: 0.05355 | Train Acc: 97.000 \n",
            "Epoch 394: | Loss: 0.05062 | Train Acc: 97.000 \n",
            "Epoch 395: | Loss: 0.05084 | Train Acc: 99.000 \n",
            "Epoch 396: | Loss: 0.05186 | Train Acc: 98.000 \n",
            "Epoch 397: | Loss: 0.05183 | Train Acc: 98.000 \n",
            "Epoch 398: | Loss: 0.05077 | Train Acc: 97.000 \n",
            "Epoch 399: | Loss: 0.06165 | Train Acc: 97.000 \n",
            "Epoch 400: | Loss: 0.05019 | Train Acc: 99.000 \n",
            "Epoch 401: | Loss: 0.05631 | Train Acc: 98.000 \n",
            "Epoch 402: | Loss: 0.05175 | Train Acc: 97.000 \n",
            "Epoch 403: | Loss: 0.05196 | Train Acc: 97.000 \n",
            "Epoch 404: | Loss: 0.05010 | Train Acc: 99.000 \n",
            "Epoch 405: | Loss: 0.05104 | Train Acc: 99.000 \n",
            "Epoch 406: | Loss: 0.06633 | Train Acc: 97.000 \n",
            "Epoch 407: | Loss: 0.05653 | Train Acc: 97.000 \n",
            "Epoch 408: | Loss: 0.05984 | Train Acc: 97.000 \n",
            "Epoch 409: | Loss: 0.05419 | Train Acc: 97.000 \n",
            "Epoch 410: | Loss: 0.05658 | Train Acc: 98.000 \n",
            "Epoch 411: | Loss: 0.05319 | Train Acc: 99.000 \n",
            "Epoch 412: | Loss: 0.05400 | Train Acc: 97.000 \n",
            "Epoch 413: | Loss: 0.05019 | Train Acc: 97.000 \n",
            "Epoch 414: | Loss: 0.06576 | Train Acc: 97.000 \n",
            "Epoch 415: | Loss: 0.06187 | Train Acc: 97.000 \n",
            "Epoch 416: | Loss: 0.05142 | Train Acc: 97.000 \n",
            "Epoch 417: | Loss: 0.05779 | Train Acc: 98.000 \n",
            "Epoch 418: | Loss: 0.05516 | Train Acc: 99.000 \n",
            "Epoch 419: | Loss: 0.05072 | Train Acc: 98.000 \n",
            "Epoch 420: | Loss: 0.06235 | Train Acc: 98.000 \n",
            "Epoch 421: | Loss: 0.05693 | Train Acc: 97.000 \n",
            "Epoch 422: | Loss: 0.05160 | Train Acc: 98.000 \n",
            "Epoch 423: | Loss: 0.05380 | Train Acc: 97.000 \n",
            "Epoch 424: | Loss: 0.05499 | Train Acc: 98.000 \n",
            "Epoch 425: | Loss: 0.05028 | Train Acc: 98.000 \n",
            "Epoch 426: | Loss: 0.05481 | Train Acc: 99.000 \n",
            "Epoch 427: | Loss: 0.05294 | Train Acc: 97.000 \n",
            "Epoch 428: | Loss: 0.05136 | Train Acc: 97.000 \n",
            "Epoch 429: | Loss: 0.05625 | Train Acc: 97.000 \n",
            "Epoch 430: | Loss: 0.05052 | Train Acc: 99.000 \n",
            "Epoch 431: | Loss: 0.05229 | Train Acc: 98.000 \n",
            "Epoch 432: | Loss: 0.05093 | Train Acc: 97.000 \n",
            "Epoch 433: | Loss: 0.05080 | Train Acc: 97.000 \n",
            "Epoch 434: | Loss: 0.06738 | Train Acc: 97.000 \n",
            "Epoch 435: | Loss: 0.05182 | Train Acc: 97.000 \n",
            "Epoch 436: | Loss: 0.05203 | Train Acc: 98.000 \n",
            "Epoch 437: | Loss: 0.05028 | Train Acc: 97.000 \n",
            "Epoch 438: | Loss: 0.05077 | Train Acc: 98.000 \n",
            "Epoch 439: | Loss: 0.05025 | Train Acc: 98.000 \n",
            "Epoch 440: | Loss: 0.05027 | Train Acc: 98.000 \n",
            "Epoch 441: | Loss: 0.05226 | Train Acc: 97.000 \n",
            "Epoch 442: | Loss: 0.05017 | Train Acc: 98.000 \n",
            "Epoch 443: | Loss: 0.05052 | Train Acc: 99.000 \n",
            "Epoch 444: | Loss: 0.05065 | Train Acc: 97.000 \n",
            "Epoch 445: | Loss: 0.05557 | Train Acc: 97.000 \n",
            "Epoch 446: | Loss: 0.05029 | Train Acc: 97.000 \n",
            "Epoch 447: | Loss: 0.05137 | Train Acc: 97.000 \n",
            "Epoch 448: | Loss: 0.05219 | Train Acc: 98.000 \n",
            "Epoch 449: | Loss: 0.05068 | Train Acc: 98.000 \n",
            "Epoch 450: | Loss: 0.06934 | Train Acc: 97.000 \n",
            "Epoch 451: | Loss: 0.05733 | Train Acc: 97.000 \n",
            "Epoch 452: | Loss: 0.06478 | Train Acc: 97.000 \n",
            "Epoch 453: | Loss: 0.05695 | Train Acc: 97.000 \n",
            "Epoch 454: | Loss: 0.05866 | Train Acc: 98.000 \n",
            "Epoch 455: | Loss: 0.05672 | Train Acc: 98.000 \n",
            "Epoch 456: | Loss: 0.05361 | Train Acc: 97.000 \n",
            "Epoch 457: | Loss: 0.05780 | Train Acc: 97.000 \n",
            "Epoch 458: | Loss: 0.05029 | Train Acc: 97.000 \n",
            "Epoch 459: | Loss: 0.05538 | Train Acc: 97.000 \n",
            "Epoch 460: | Loss: 0.05519 | Train Acc: 97.000 \n",
            "Epoch 461: | Loss: 0.05004 | Train Acc: 98.000 \n",
            "Epoch 462: | Loss: 0.05000 | Train Acc: 99.000 \n",
            "Epoch 463: | Loss: 0.05164 | Train Acc: 98.000 \n",
            "Epoch 464: | Loss: 0.05392 | Train Acc: 98.000 \n",
            "Epoch 465: | Loss: 0.05099 | Train Acc: 98.000 \n",
            "Epoch 466: | Loss: 0.08119 | Train Acc: 96.000 \n",
            "Epoch 467: | Loss: 0.06814 | Train Acc: 97.000 \n",
            "Epoch 468: | Loss: 0.07946 | Train Acc: 97.000 \n",
            "Epoch 469: | Loss: 0.06564 | Train Acc: 98.000 \n",
            "Epoch 470: | Loss: 0.05357 | Train Acc: 98.000 \n",
            "Epoch 471: | Loss: 0.07089 | Train Acc: 97.000 \n",
            "Epoch 472: | Loss: 0.05501 | Train Acc: 98.000 \n",
            "Epoch 473: | Loss: 0.05741 | Train Acc: 97.000 \n",
            "Epoch 474: | Loss: 0.06267 | Train Acc: 97.000 \n",
            "Epoch 475: | Loss: 0.05472 | Train Acc: 97.000 \n",
            "Epoch 476: | Loss: 0.05199 | Train Acc: 98.000 \n",
            "Epoch 477: | Loss: 0.05391 | Train Acc: 98.000 \n",
            "Epoch 478: | Loss: 0.05037 | Train Acc: 98.000 \n",
            "Epoch 479: | Loss: 0.05284 | Train Acc: 97.000 \n",
            "Epoch 480: | Loss: 0.05058 | Train Acc: 97.000 \n",
            "Epoch 481: | Loss: 0.05147 | Train Acc: 97.000 \n",
            "Epoch 482: | Loss: 0.05003 | Train Acc: 98.000 \n",
            "Epoch 483: | Loss: 0.05028 | Train Acc: 98.000 \n",
            "Epoch 484: | Loss: 0.05635 | Train Acc: 99.000 \n",
            "Epoch 485: | Loss: 0.05121 | Train Acc: 97.000 \n",
            "Epoch 486: | Loss: 0.05380 | Train Acc: 97.000 \n",
            "Epoch 487: | Loss: 0.05085 | Train Acc: 98.000 \n",
            "Epoch 488: | Loss: 0.05106 | Train Acc: 98.000 \n",
            "Epoch 489: | Loss: 0.05010 | Train Acc: 98.000 \n",
            "Epoch 490: | Loss: 0.05168 | Train Acc: 98.000 \n",
            "Epoch 491: | Loss: 0.05006 | Train Acc: 97.000 \n",
            "Epoch 492: | Loss: 0.05024 | Train Acc: 97.000 \n",
            "Epoch 493: | Loss: 0.05090 | Train Acc: 97.000 \n",
            "Epoch 494: | Loss: 0.05092 | Train Acc: 99.000 \n",
            "Epoch 495: | Loss: 0.05048 | Train Acc: 99.000 \n",
            "Epoch 496: | Loss: 0.05109 | Train Acc: 98.000 \n",
            "Epoch 497: | Loss: 0.05026 | Train Acc: 98.000 \n",
            "Epoch 498: | Loss: 0.05064 | Train Acc: 97.000 \n",
            "Epoch 499: | Loss: 0.05017 | Train Acc: 97.000 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cd3cIjWQz7u"
      },
      "source": [
        "# 4. Quantitative Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "mC6j7WwTdNzu",
        "outputId": "66fdbfbd-9dfe-4e95-bda9-d9152a16e69e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_loss_history,color='r')\n",
        "plt.plot(test_loss_history,color='g')\n",
        "plt.xlabel('Iteration number')\n",
        "plt.ylabel('Loss value')\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UNWsl33TxZr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be97291e-c4c6-4fc8-8083-9be9c89dde3b"
      },
      "source": [
        "\n",
        "model = model.to(\"cpu\")\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = model(images)\n",
        "        acc = binary_acc(outputs,labels.unsqueeze(1))\n",
        "        #acc = multi_class_acc(outputs,labels.unsqueeze(1))\n",
        "\n",
        "print(\"Test accuracy = {}\".format(acc))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy = 84.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfsrO2wwLUiC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}