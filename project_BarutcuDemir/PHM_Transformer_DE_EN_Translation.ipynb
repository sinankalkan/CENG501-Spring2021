{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PHM Transformer DE EN Translation",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "db2a07c68fc04fad91cbdc687679f389": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0dadac2e91c3428686184168af15f332",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e2c51b8f9a17445fa516247666736ab0",
              "IPY_MODEL_8659b3fadef14af285ccafcbe8faa994"
            ]
          }
        },
        "0dadac2e91c3428686184168af15f332": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e2c51b8f9a17445fa516247666736ab0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_1b65c4ef734c48b09736ab86c693f0ba",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.17MB of 0.17MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8b8a2a430ae84e9aa5e5d8b7c568c678"
          }
        },
        "8659b3fadef14af285ccafcbe8faa994": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_dd4cb660fb044b7c93e999a0bcb38aa3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_adaefc728d6048148359899d971b5be6"
          }
        },
        "1b65c4ef734c48b09736ab86c693f0ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8b8a2a430ae84e9aa5e5d8b7c568c678": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dd4cb660fb044b7c93e999a0bcb38aa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "adaefc728d6048148359899d971b5be6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWbwaoVKx4hM"
      },
      "source": [
        "This is the Colab to reproduce our reproduction of the text style transfer task in [Beyond Fully Connected Layers with Quaternions: Parametrization of Hypercomplex Multiplications with 1/n Parameters](https://openreview.net/pdf?id=rcQdycl0zyk). We will translate German to English, using a PHM Transformer. You can find the detailed explanation in our [Github Repo](https://github.com/sinankalkan/CENG501-Spring2021/tree/main/project_BarutcuDemir). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX9VVpJisotd"
      },
      "source": [
        "This notebook is not annotated in detail and not pretty compared to the style transfer notebook. Most of the code is same except for the data download, German tokenization and the training loop. For other parts please refer to that notebook that you can access in our Github repo linked above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4k_OivV4RoV"
      },
      "source": [
        "Parts of the code are adapted from the Torch source code and [this tutorial](https://pytorch.org/tutorials/beginner/translation_transformer.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0o4wTS8zDL4"
      },
      "source": [
        "# Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9tL0RgKwiAF",
        "outputId": "25f5cec9-15f5-44fa-943a-dae48f80632b"
      },
      "source": [
        "!pip install torchinfo wandb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.5.2-py3-none-any.whl (18 kB)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.11.0-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 27.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.20-py3-none-any.whl (178 kB)\n",
            "\u001b[K     |████████████████████████████████| 178 kB 65.6 MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "  Downloading sentry_sdk-1.3.1-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 63.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting urllib3>=1.26.5\n",
            "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 61.8 MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Collecting requests<3,>=2.0.0\n",
            "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.2)\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=dc460ef082bd7de2be1b7d596543fdb65855877d4070b37fbd521ac9cc910591\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=3876e7bc0cb555d9b0f834dbd040d59f43182792c98b8f8c797d593bbf9483cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: smmap, urllib3, gitdb, subprocess32, shortuuid, sentry-sdk, requests, pathtools, GitPython, docker-pycreds, configparser, wandb, torchinfo\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.20 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 requests-2.26.0 sentry-sdk-1.3.1 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 torchinfo-1.5.2 urllib3-1.26.6 wandb-0.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhhT37pBSgeQ",
        "outputId": "825a3b9e-f62f-4035-81c4-cfb7c349849b"
      },
      "source": [
        "!pip install torch==1.8.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.8.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.8.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (1982.2 MB)\n",
            "\u001b[K     |█████████████▌                  | 834.1 MB 814 kB/s eta 0:23:31tcmalloc: large alloc 1147494400 bytes == 0x55c002b7a000 @  0x7f5fda18a615 0x55bfc9de002c 0x55bfc9ec017a 0x55bfc9de2e4d 0x55bfc9ed4c0d 0x55bfc9e570d8 0x55bfc9e51c35 0x55bfc9de473a 0x55bfc9e56f40 0x55bfc9e51c35 0x55bfc9de473a 0x55bfc9e5393b 0x55bfc9ed5a56 0x55bfc9e52fb3 0x55bfc9ed5a56 0x55bfc9e52fb3 0x55bfc9ed5a56 0x55bfc9e52fb3 0x55bfc9de4b99 0x55bfc9e27e79 0x55bfc9de37b2 0x55bfc9e56e65 0x55bfc9e51c35 0x55bfc9de473a 0x55bfc9e5393b 0x55bfc9e51c35 0x55bfc9de473a 0x55bfc9e52b0e 0x55bfc9de465a 0x55bfc9e52d67 0x55bfc9e51c35\n",
            "\u001b[K     |█████████████████               | 1055.7 MB 1.3 MB/s eta 0:12:12tcmalloc: large alloc 1434370048 bytes == 0x55c0471d0000 @  0x7f5fda18a615 0x55bfc9de002c 0x55bfc9ec017a 0x55bfc9de2e4d 0x55bfc9ed4c0d 0x55bfc9e570d8 0x55bfc9e51c35 0x55bfc9de473a 0x55bfc9e56f40 0x55bfc9e51c35 0x55bfc9de473a 0x55bfc9e5393b 0x55bfc9ed5a56 0x55bfc9e52fb3 0x55bfc9ed5a56 0x55bfc9e52fb3 0x55bfc9ed5a56 0x55bfc9e52fb3 0x55bfc9de4b99 0x55bfc9e27e79 0x55bfc9de37b2 0x55bfc9e56e65 0x55bfc9e51c35 0x55bfc9de473a 0x55bfc9e5393b 0x55bfc9e51c35 0x55bfc9de473a 0x55bfc9e52b0e 0x55bfc9de465a 0x55bfc9e52d67 0x55bfc9e51c35\n",
            "\u001b[K     |█████████████████████▋          | 1336.2 MB 1.4 MB/s eta 0:07:51tcmalloc: large alloc 1792966656 bytes == 0x55bfcc002000 @  0x7f5fda18a615 0x55bfc9de002c 0x55bfc9ec017a 0x55bfc9de2e4d 0x55bfc9ed4c0d 0x55bfc9e570d8 0x55bfc9e51c35 0x55bfc9de473a 0x55bfc9e56f40 0x55bfc9e51c35 0x55bfc9de473a 0x55bfc9e5393b 0x55bfc9ed5a56 0x55bfc9e52fb3 0x55bfc9ed5a56 0x55bfc9e52fb3 0x55bfc9ed5a56 0x55bfc9e52fb3 0x55bfc9de4b99 0x55bfc9e27e79 0x55bfc9de37b2 0x55bfc9e56e65 0x55bfc9e51c35 0x55bfc9de473a 0x55bfc9e5393b 0x55bfc9e51c35 0x55bfc9de473a 0x55bfc9e52b0e 0x55bfc9de465a 0x55bfc9e52d67 0x55bfc9e51c35\n",
            "\u001b[K     |███████████████████████████▎    | 1691.1 MB 1.3 MB/s eta 0:03:49tcmalloc: large alloc 2241208320 bytes == 0x55c036dea000 @  0x7f5fda18a615 0x55bfc9de002c 0x55bfc9ec017a 0x55bfc9de2e4d 0x55bfc9ed4c0d 0x55bfc9e570d8 0x55bfc9e51c35 0x55bfc9de473a 0x55bfc9e56f40 0x55bfc9e51c35 0x55bfc9de473a 0x55bfc9e5393b 0x55bfc9ed5a56 0x55bfc9e52fb3 0x55bfc9ed5a56 0x55bfc9e52fb3 0x55bfc9ed5a56 0x55bfc9e52fb3 0x55bfc9de4b99 0x55bfc9e27e79 0x55bfc9de37b2 0x55bfc9e56e65 0x55bfc9e51c35 0x55bfc9de473a 0x55bfc9e5393b 0x55bfc9e51c35 0x55bfc9de473a 0x55bfc9e52b0e 0x55bfc9de465a 0x55bfc9e52d67 0x55bfc9e51c35\n",
            "\u001b[K     |████████████████████████████████| 1982.2 MB 1.2 MB/s eta 0:00:01tcmalloc: large alloc 1982251008 bytes == 0x55c0bc74c000 @  0x7f5fda1891e7 0x55bfc9e15ae7 0x55bfc9de002c 0x55bfc9ec017a 0x55bfc9de2e4d 0x55bfc9ed4c0d 0x55bfc9e570d8 0x55bfc9e51c35 0x55bfc9de473a 0x55bfc9e52d67 0x55bfc9e51c35 0x55bfc9de473a 0x55bfc9e52d67 0x55bfc9e51c35 0x55bfc9de473a 0x55bfc9e52d67 0x55bfc9e51c35 0x55bfc9de473a 0x55bfc9e52d67 0x55bfc9e51c35 0x55bfc9de473a 0x55bfc9e52d67 0x55bfc9de465a 0x55bfc9e52d67 0x55bfc9e51c35 0x55bfc9de473a 0x55bfc9e5393b 0x55bfc9e51c35 0x55bfc9de473a 0x55bfc9e5393b 0x55bfc9e51c35\n",
            "tcmalloc: large alloc 2477817856 bytes == 0x55c1329b8000 @  0x7f5fda18a615 0x55bfc9de002c 0x55bfc9ec017a 0x55bfc9de2e4d 0x55bfc9ed4c0d 0x55bfc9e570d8 0x55bfc9e51c35 0x55bfc9de473a 0x55bfc9e52d67 0x55bfc9e51c35 0x55bfc9de473a 0x55bfc9e52d67 0x55bfc9e51c35 0x55bfc9de473a 0x55bfc9e52d67 0x55bfc9e51c35 0x55bfc9de473a 0x55bfc9e52d67 0x55bfc9e51c35 0x55bfc9de473a 0x55bfc9e52d67 0x55bfc9de465a 0x55bfc9e52d67 0x55bfc9e51c35 0x55bfc9de473a 0x55bfc9e5393b 0x55bfc9e51c35 0x55bfc9de473a 0x55bfc9e5393b 0x55bfc9e51c35 0x55bfc9de4dd1\n",
            "\u001b[K     |████████████████████████████████| 1982.2 MB 2.2 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu111) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu111) (1.19.5)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.8.0+cu111 which is incompatible.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.8.0+cu111 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.0+cu111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynMDU2DBEdk2",
        "outputId": "6ade74f9-9928-42ab-9b21-3d29b998ba38"
      },
      "source": [
        "!pip install torchtext==0.9.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.9.0\n",
            "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 18.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (2.26.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (4.41.1)\n",
            "Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (1.8.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->torchtext==0.9.0) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2021.5.30)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2.0.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (1.26.6)\n",
            "Installing collected packages: torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.10.0\n",
            "    Uninstalling torchtext-0.10.0:\n",
            "      Successfully uninstalled torchtext-0.10.0\n",
            "Successfully installed torchtext-0.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKz_Fz6CuBzh",
        "outputId": "ab2568b6-5b5f-4641-c766-7da5519426ac"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jul 28 07:36:55 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P8    12W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq8Hh_VK6Rt9"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ll51iOVthS2",
        "outputId": "86b35bda-c824-4d96-88eb-a1161ff8eba2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ-8y9sZ6uq3",
        "outputId": "5f7c2228-3d6c-4ff8-e4a9-36e694a159d3"
      },
      "source": [
        "!wget http://www.statmt.org/wmt13/training-parallel-europarl-v7.tgz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-24 07:31:45--  http://www.statmt.org/wmt13/training-parallel-europarl-v7.tgz\n",
            "Resolving www.statmt.org (www.statmt.org)... 129.215.197.184\n",
            "Connecting to www.statmt.org (www.statmt.org)|129.215.197.184|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 657632379 (627M) [application/x-gzip]\n",
            "Saving to: ‘training-parallel-europarl-v7.tgz’\n",
            "\n",
            "       training-par   0%[                    ] 898.38K  1.07MB/s               ^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiDJz_6qBXpu",
        "outputId": "05917b1f-b9a9-4a4a-851d-067aefb0ae0f"
      },
      "source": [
        "!tar -xzvf \"/content/training-parallel-europarl-v7.tgz\" -C \"/content/\" "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training/europarl-v7.cs-en.cs\n",
            "training/europarl-v7.cs-en.en\n",
            "training/europarl-v7.de-en.de\n",
            "training/europarl-v7.de-en.en\n",
            "training/europarl-v7.es-en.en\n",
            "training/europarl-v7.es-en.es\n",
            "training/europarl-v7.fr-en.en\n",
            "training/europarl-v7.fr-en.fr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrJeZMb_6hZU"
      },
      "source": [
        "German tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq_R04EeHh8L",
        "outputId": "b7a10e96-f92a-4f15-ea26-be94f154a814"
      },
      "source": [
        "!wget https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-28 07:37:55--  https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-releases.githubusercontent.com/84940268/920ce580-06fe-11ea-9159-a0235b43e9b7?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210728%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210728T073755Z&X-Amz-Expires=300&X-Amz-Signature=0086013d32d81526c15ca41e208725a0b4fbef48386877b2c573eed15d27d209&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=84940268&response-content-disposition=attachment%3B%20filename%3Dde_core_news_sm-2.2.5.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2021-07-28 07:37:55--  https://github-releases.githubusercontent.com/84940268/920ce580-06fe-11ea-9159-a0235b43e9b7?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210728%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210728T073755Z&X-Amz-Expires=300&X-Amz-Signature=0086013d32d81526c15ca41e208725a0b4fbef48386877b2c573eed15d27d209&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=84940268&response-content-disposition=attachment%3B%20filename%3Dde_core_news_sm-2.2.5.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.108.154, 185.199.111.154, 185.199.109.154, ...\n",
            "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.108.154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14902417 (14M) [application/octet-stream]\n",
            "Saving to: ‘de_core_news_sm-2.2.5.tar.gz’\n",
            "\n",
            "de_core_news_sm-2.2 100%[===================>]  14.21M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-07-28 07:37:55 (140 MB/s) - ‘de_core_news_sm-2.2.5.tar.gz’ saved [14902417/14902417]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCnpRltkH5C1",
        "outputId": "f1b90378-2cae-4d47-ae50-b0ebcfd03c0b"
      },
      "source": [
        "!tar -xzvf '/content/de_core_news_sm-2.2.5.tar.gz'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "de_core_news_sm-2.2.5/\n",
            "de_core_news_sm-2.2.5/de_core_news_sm.egg-info/\n",
            "de_core_news_sm-2.2.5/de_core_news_sm.egg-info/PKG-INFO\n",
            "de_core_news_sm-2.2.5/de_core_news_sm.egg-info/dependency_links.txt\n",
            "de_core_news_sm-2.2.5/de_core_news_sm.egg-info/top_level.txt\n",
            "de_core_news_sm-2.2.5/de_core_news_sm.egg-info/SOURCES.txt\n",
            "de_core_news_sm-2.2.5/de_core_news_sm.egg-info/requires.txt\n",
            "de_core_news_sm-2.2.5/de_core_news_sm.egg-info/not-zip-safe\n",
            "de_core_news_sm-2.2.5/setup.cfg\n",
            "de_core_news_sm-2.2.5/PKG-INFO\n",
            "de_core_news_sm-2.2.5/MANIFEST.in\n",
            "de_core_news_sm-2.2.5/meta.json\n",
            "de_core_news_sm-2.2.5/setup.py\n",
            "de_core_news_sm-2.2.5/de_core_news_sm/\n",
            "de_core_news_sm-2.2.5/de_core_news_sm/__init__.py\n",
            "de_core_news_sm-2.2.5/de_core_news_sm/meta.json\n",
            "de_core_news_sm-2.2.5/de_core_news_sm/de_core_news_sm-2.2.5/\n",
            "de_core_news_sm-2.2.5/de_core_news_sm/de_core_news_sm-2.2.5/ner/\n",
            "de_core_news_sm-2.2.5/de_core_news_sm/de_core_news_sm-2.2.5/ner/model\n",
            "de_core_news_sm-2.2.5/de_core_news_sm/de_core_news_sm-2.2.5/ner/moves\n",
            "de_core_news_sm-2.2.5/de_core_news_sm/de_core_news_sm-2.2.5/ner/cfg\n",
            "de_core_news_sm-2.2.5/de_core_news_sm/de_core_news_sm-2.2.5/meta.json\n",
            "de_core_news_sm-2.2.5/de_core_news_sm/de_core_news_sm-2.2.5/tokenizer\n",
            "de_core_news_sm-2.2.5/de_core_news_sm/de_core_news_sm-2.2.5/vocab/\n",
            "de_core_news_sm-2.2.5/de_core_news_sm/de_core_news_sm-2.2.5/vocab/lookups.bin\n",
            "de_core_news_sm-2.2.5/de_core_news_sm/de_core_news_sm-2.2.5/vocab/vectors\n",
            "de_core_news_sm-2.2.5/de_core_news_sm/de_core_news_sm-2.2.5/vocab/key2row\n",
            "de_core_news_sm-2.2.5/de_core_news_sm/de_core_news_sm-2.2.5/vocab/strings.json\n",
            "de_core_news_sm-2.2.5/de_core_news_sm/de_core_news_sm-2.2.5/vocab/lexemes.bin\n",
            "de_core_news_sm-2.2.5/de_core_news_sm/de_core_news_sm-2.2.5/accuracy.json\n",
            "de_core_news_sm-2.2.5/de_core_news_sm/de_core_news_sm-2.2.5/parser/\n",
            "de_core_news_sm-2.2.5/de_core_news_sm/de_core_news_sm-2.2.5/parser/model\n",
            "de_core_news_sm-2.2.5/de_core_news_sm/de_core_news_sm-2.2.5/parser/moves\n",
            "de_core_news_sm-2.2.5/de_core_news_sm/de_core_news_sm-2.2.5/parser/cfg\n",
            "de_core_news_sm-2.2.5/de_core_news_sm/de_core_news_sm-2.2.5/tagger/\n",
            "de_core_news_sm-2.2.5/de_core_news_sm/de_core_news_sm-2.2.5/tagger/tag_map\n",
            "de_core_news_sm-2.2.5/de_core_news_sm/de_core_news_sm-2.2.5/tagger/model\n",
            "de_core_news_sm-2.2.5/de_core_news_sm/de_core_news_sm-2.2.5/tagger/cfg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEVSCuLk6n6a"
      },
      "source": [
        "You should download the [dev data](http://www.statmt.org/wmt14/dev.tgz)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woTNlttvDMPP",
        "outputId": "59e53dd4-b3e0-48d3-8920-feefba6eb708"
      },
      "source": [
        "!tar -xzvf \"/path to dev/\" -C \"/content/\" "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dev/\n",
            "dev/newstest2009-src.es.sgm\n",
            "dev/newstest2009.cz\n",
            "dev/newstest2013-ref.en.sgm\n",
            "dev/news-test2008.es\n",
            "dev/newstest2010-ref.de.sgm\n",
            "dev/newstest2010-ref.cz.sgm\n",
            "dev/newssyscomb2009.cs\n",
            "dev/newstest2013-src.en.sgm\n",
            "dev/newstest2011-src.de.sgm\n",
            "dev/news-test2008-ref.de.sgm\n",
            "dev/newsdev2014-ref.hi.sgm\n",
            "dev/newstest2011.de\n",
            "dev/newstest2010-ref.en.sgm\n",
            "dev/newstest2009-src.cz.sgm\n",
            "dev/newstest2011-ref.en.sgm\n",
            "dev/newssyscomb2009-ref.es.sgm\n",
            "dev/newstest2009.en\n",
            "dev/newssyscomb2009-ref.cs.sgm\n",
            "dev/newssyscomb2009.en\n",
            "dev/newstest2011-src.en.sgm\n",
            "dev/newstest2010-src.es.sgm\n",
            "dev/news-test2008-src.es.sgm\n",
            "dev/newstest2009-ref.it.sgm\n",
            "dev/newstest2009-ref.fr.sgm\n",
            "dev/newstest2012.es\n",
            "dev/newstest2012-ref.es.sgm\n",
            "dev/newstest2010.de\n",
            "dev/newstest2012-src.es.sgm\n",
            "dev/newssyscomb2009-src.en.sgm\n",
            "dev/newstest2010.en\n",
            "dev/news-test2008-ref.cs.sgm\n",
            "dev/newstest2011.fr\n",
            "dev/newstest2009-src.fr.sgm\n",
            "dev/newstest2011-ref.es.sgm\n",
            "dev/newstest2009-src.hu.sgm\n",
            "dev/newssyscomb2009-src.es.sgm\n",
            "dev/newstest2011-src.fr.sgm\n",
            "dev/newssyscomb2009-ref.en.sgm\n",
            "dev/newstest2011.es\n",
            "dev/newstest2013-ref.es.sgm\n",
            "dev/newstest2009-ref.de.sgm\n",
            "dev/newsdev2014-src.hi.sgm\n",
            "dev/newssyscomb2009-ref.cz.sgm\n",
            "dev/newssyscomb2009-src.hu.sgm\n",
            "dev/newssyscomb2009.es\n",
            "dev/newstest2009-src.it.sgm\n",
            "dev/newstest2010.cz\n",
            "dev/newssyscomb2009-src.cs.sgm\n",
            "dev/news-test2008-ref.es.sgm\n",
            "dev/news-test2008.fr\n",
            "dev/newstest2009-ref.en.sgm\n",
            "dev/newstest2012-ref.cs.sgm\n",
            "dev/newstest2012.ru\n",
            "dev/newstest2009-src.de.sgm\n",
            "dev/newstest2012-src.fr.sgm\n",
            "dev/newstest2010-src.de.sgm\n",
            "dev/news-test2008-src.de.sgm\n",
            "dev/newstest2010.fr\n",
            "dev/newstest2013.es\n",
            "dev/newstest2013.ru\n",
            "dev/newssyscomb2009.de\n",
            "dev/newstest2012.fr\n",
            "dev/newstest2009.fr\n",
            "dev/newsdev2014.hi\n",
            "dev/newstest2012.cs\n",
            "dev/newstest2012-src.ru.sgm\n",
            "dev/newssyscomb2009-src.de.sgm\n",
            "dev/newssyscomb2009-ref.fr.sgm\n",
            "dev/newssyscomb2009-src.it.sgm\n",
            "dev/newstest2009-ref.es.sgm\n",
            "dev/.newstest2013-ref.en.sgm.swp\n",
            "dev/newstest2010-ref.cs.sgm\n",
            "dev/newsdev2014.en\n",
            "dev/newstest2010-src.cs.sgm\n",
            "dev/newstest2010-ref.es.sgm\n",
            "dev/newstest2012-ref.en.sgm\n",
            "dev/newstest2010-src.cz.sgm\n",
            "dev/news-test2008-src.en.sgm\n",
            "dev/news-test2008-ref.fr.sgm\n",
            "dev/news-test2008.en\n",
            "dev/newstest2013.en\n",
            "dev/news-test2008-src.cs.sgm\n",
            "dev/news-test2008-ref.hu.sgm\n",
            "dev/newstest2010-ref.fr.sgm\n",
            "dev/news-test2008.cz\n",
            "dev/news-test2008.de\n",
            "dev/newstest2009-src.xx.sgm\n",
            "dev/newsdev2014-ref.en.sgm\n",
            "dev/newstest2011-src.cs.sgm\n",
            "dev/newstest2009-ref.cs.sgm\n",
            "dev/newstest2013-src.fr.sgm\n",
            "dev/newstest2009.cs\n",
            "dev/news-test2008.cs\n",
            "dev/newstest2011-ref.cs.sgm\n",
            "dev/newstest2013.de\n",
            "dev/newstest2012-src.cs.sgm\n",
            "dev/newssyscomb2009.fr\n",
            "dev/newstest2010-src.en.sgm\n",
            "dev/newsdev2014-src.en.sgm\n",
            "dev/news-test2008-ref.cz.sgm\n",
            "dev/newstest2013.fr\n",
            "dev/newstest2011.en\n",
            "dev/newstest2011-src.es.sgm\n",
            "dev/newstest2013-src.es.sgm\n",
            "dev/news-test2008-src.cz.sgm\n",
            "dev/news-test2008-src.hu.sgm\n",
            "dev/newssyscomb2009-ref.de.sgm\n",
            "dev/newstest2009-src.cs.sgm\n",
            "dev/newstest2011.cs\n",
            "dev/newssyscomb2009-src.cz.sgm\n",
            "dev/newstest2013-ref.cs.sgm\n",
            "dev/newssyscomb2009-ref.hu.sgm\n",
            "dev/newstest2013-ref.fr.sgm\n",
            "dev/newstest2009.de\n",
            "dev/newstest2009-src.en.sgm\n",
            "dev/newstest2011-ref.de.sgm\n",
            "dev/newstest2012-src.en.sgm\n",
            "dev/newstest2012.de\n",
            "dev/newstest2009.es\n",
            "dev/enhi_v1/\n",
            "dev/enhi_v1/newsdev2014-ref.hi.sgm\n",
            "dev/enhi_v1/newsdev2014-src.hi.sgm\n",
            "dev/enhi_v1/newsdev2014.hi\n",
            "dev/enhi_v1/newsdev2014.en\n",
            "dev/enhi_v1/newsdev2014-ref.en.sgm\n",
            "dev/enhi_v1/newsdev2014-src.en.sgm\n",
            "dev/newstest2009-ref.cz.sgm\n",
            "dev/newstest2013.cs\n",
            "dev/.newsdev2014-ref.en.sgm.swp\n",
            "dev/newstest2012-ref.ru.sgm\n",
            "dev/newstest2013-src.de.sgm\n",
            "dev/news-test2008-ref.en.sgm\n",
            "dev/newssyscomb2009-ref.it.sgm\n",
            "dev/newssyscomb2009-src.fr.sgm\n",
            "dev/newstest2010-src.fr.sgm\n",
            "dev/newstest2013-ref.de.sgm\n",
            "dev/newstest2013-src.ru.sgm\n",
            "dev/newstest2012-src.de.sgm\n",
            "dev/newstest2013-src.cs.sgm\n",
            "dev/newstest2013-ref.ru.sgm\n",
            "dev/newstest2010.es\n",
            "dev/newstest2012-ref.fr.sgm\n",
            "dev/newstest2010.cs\n",
            "dev/newstest2009-ref.hu.sgm\n",
            "dev/newstest2011-ref.fr.sgm\n",
            "dev/news-test2008-src.fr.sgm\n",
            "dev/newstest2012.en\n",
            "dev/newstest2012-ref.de.sgm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nQUQIFE1G2b",
        "outputId": "1782ff28-9d75-4b71-be95-d30f4c09187b"
      },
      "source": [
        "import math\n",
        "import torchtext\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "from torchtext.vocab import Vocab\n",
        "from torchtext.utils import download_from_url, extract_archive\n",
        "from torch import Tensor\n",
        "import io\n",
        "import time\n",
        "\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fea24f8cc90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxON-DPz60fs"
      },
      "source": [
        "Load German tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4JO4MOXE-cQ",
        "outputId": "e8ef4b88-70a8-41e7-8aa1-9f46e1d27a34"
      },
      "source": [
        "import spacy\n",
        "spacy.load('/content/de_core_news_sm-2.2.5/de_core_news_sm/de_core_news_sm-2.2.5')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.de.German at 0x7fea239b2a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eMnHCcD628a"
      },
      "source": [
        "Make sure the paths are correct for this one. This process takes a long time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gzo6oMyzt653"
      },
      "source": [
        "train_filepaths = ['/path to /europarl-v7.de-en.de', '/path to /europarl-v7.de-en.en']\n",
        "val_filepaths = ['/path to /newstest2010.de', '/path to /newstest2010.en']\n",
        "\n",
        "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "de_tokenizer = get_tokenizer('spacy', language='/content/de_core_news_sm-2.2.5/de_core_news_sm/de_core_news_sm-2.2.5')\n",
        "\n",
        "def build_vocab(filepath, tokenizer):\n",
        "  counter = Counter()\n",
        "  with io.open(filepath, encoding=\"utf8\") as f:\n",
        "    for string_ in f:\n",
        "      counter.update(tokenizer(string_))\n",
        "  return Vocab(counter, max_size=32764, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
        "\n",
        "german_vocab = build_vocab('/path to /europarl-v7.de-en.de', de_tokenizer) \n",
        "english_vocab = build_vocab('/path to /europarl-v7.de-en.en', en_tokenizer)\n",
        "#10 dk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRn9QSQP68PQ"
      },
      "source": [
        "This also takes a long time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv7ZwefVtP8U"
      },
      "source": [
        "def data_process(filepaths):\n",
        "  raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
        "  raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
        "  data = []\n",
        "  for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
        "    mo_tensor_ = torch.tensor([german_vocab[token] for token in de_tokenizer(raw_de.rstrip(\"\\n\"))],\n",
        "                            dtype=torch.long)\n",
        "    og_tensor_ = torch.tensor([english_vocab[token] for token in en_tokenizer(raw_en.rstrip(\"\\n\"))],\n",
        "                            dtype=torch.long)\n",
        "    data.append((mo_tensor_, og_tensor_))\n",
        "  return data\n",
        "\n",
        "train_data = data_process(train_filepaths)\n",
        "val_data = data_process(val_filepaths)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "PAD_IDX = german_vocab['<pad>']\n",
        "BOS_IDX = german_vocab['<bos>']\n",
        "EOS_IDX = german_vocab['<eos>']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnLNCst7t655"
      },
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def generate_batch(data_batch):\n",
        "  de_batch, en_batch = [], []\n",
        "  for (de_item, en_item) in data_batch:\n",
        "    de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "    en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "  de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n",
        "  en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
        "  return de_batch, en_batch\n",
        "\n",
        "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6YK9Aoc7Bpd"
      },
      "source": [
        "# The PHM Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htvjhR063fn0"
      },
      "source": [
        "from torch.nn.parameter import Parameter, UninitializedParameter\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from torch.nn import init\n",
        "import math\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class PHMLayer(nn.Module):\n",
        "\n",
        "  def __init__(self, n, in_features, out_features):\n",
        "    super(PHMLayer, self).__init__()\n",
        "    self.n = n\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "\n",
        "    self.bias = Parameter(torch.Tensor(out_features))\n",
        "\n",
        "    self.a = torch.zeros((n, n, n))\n",
        "    self.a = Parameter(torch.nn.init.xavier_uniform_(self.a))\n",
        "\n",
        "    self.s = torch.zeros((n, self.out_features//n, self.in_features//n)) \n",
        "    self.s = Parameter(torch.nn.init.xavier_uniform_(self.s))\n",
        "\n",
        "    self.weight = torch.zeros((self.out_features, self.in_features))\n",
        "\n",
        "    fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
        "    bound = 1 / math.sqrt(fan_in)\n",
        "    init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "  def kronecker_product1(self, a, b):\n",
        "    siz1 = torch.Size(torch.tensor(a.shape[-2:]) * torch.tensor(b.shape[-2:]))\n",
        "    res = a.unsqueeze(-1).unsqueeze(-3) * b.unsqueeze(-2).unsqueeze(-4)\n",
        "    siz0 = res.shape[:-4]\n",
        "    out = res.reshape(siz0 + siz1)\n",
        "    return out\n",
        "\n",
        "  def forward(self, input: Tensor) -> Tensor:\n",
        "    self.weight = torch.sum(self.kronecker_product1(self.a, self.s), dim=0)\n",
        "    input = input.type(dtype=self.weight.type())\n",
        "    return F.linear(input, weight=self.weight, bias=self.bias)\n",
        "\n",
        "  def extra_repr(self) -> str:\n",
        "    return 'in_features={}, out_features={}, bias={}'.format(\n",
        "      self.in_features, self.out_features, self.bias is not None)\n",
        "    \n",
        "  def reset_parameters(self) -> None:\n",
        "    init.kaiming_uniform_(self.a, a=math.sqrt(5))\n",
        "    init.kaiming_uniform_(self.s, a=math.sqrt(5))\n",
        "    fan_in, _ = init._calculate_fan_in_and_fan_out(self.placeholder)\n",
        "    bound = 1 / math.sqrt(fan_in)\n",
        "    init.uniform_(self.bias, -bound, bound)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9tD5ndYxzWB"
      },
      "source": [
        "import warnings\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "#from torch.nn import NonDynamicallyQuantizableLinear\n",
        "from torch.nn.init import xavier_uniform_\n",
        "from torch.nn.init import constant_\n",
        "from torch.nn.init import xavier_normal_\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import Module\n",
        "from torch import functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMGqWZCRqGQI"
      },
      "source": [
        "from torch.nn.functional import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9yKJTwjxNdo"
      },
      "source": [
        "import copy\n",
        "from typing import Optional, Any\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import Module\n",
        "from torch.nn import MultiheadAttention\n",
        "from torch.nn import ModuleList\n",
        "from torch.nn.init import xavier_uniform_\n",
        "from torch.nn import Dropout\n",
        "from torch.nn import Linear\n",
        "from torch.nn import LayerNorm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnOM9z24pECw"
      },
      "source": [
        "def phm_multi_head_attention_forward(\n",
        "    phm: int,\n",
        "    a: Tensor,\n",
        "    s: Tensor,\n",
        "    query: Tensor,\n",
        "    key: Tensor,\n",
        "    value: Tensor,\n",
        "    embed_dim_to_check: int,\n",
        "    num_heads: int,\n",
        "    in_proj_weight: Tensor,\n",
        "    in_proj_bias: Tensor,\n",
        "    bias_k: Optional[Tensor],\n",
        "    bias_v: Optional[Tensor],\n",
        "    add_zero_attn: bool,\n",
        "    dropout_p: float,\n",
        "    out_proj_weight: Tensor,\n",
        "    out_proj_bias: Tensor,\n",
        "    training: bool = True,\n",
        "    key_padding_mask: Optional[Tensor] = None,\n",
        "    need_weights: bool = True,\n",
        "    attn_mask: Optional[Tensor] = None,\n",
        "    use_separate_proj_weight: bool = False,\n",
        "    q_proj_weight: Optional[Tensor] = None,\n",
        "    k_proj_weight: Optional[Tensor] = None,\n",
        "    v_proj_weight: Optional[Tensor] = None,\n",
        "    static_k: Optional[Tensor] = None,\n",
        "    static_v: Optional[Tensor] = None,\n",
        ") -> Tuple[Tensor, Optional[Tensor]]:\n",
        "    r\"\"\"\n",
        "    Args:\n",
        "        query, key, value: map a query and a set of key-value pairs to an output.\n",
        "            See \"Attention Is All You Need\" for more details.\n",
        "        embed_dim_to_check: total dimension of the model.\n",
        "        num_heads: parallel attention heads.\n",
        "        in_proj_weight, in_proj_bias: input projection weight and bias.\n",
        "        bias_k, bias_v: bias of the key and value sequences to be added at dim=0.\n",
        "        add_zero_attn: add a new batch of zeros to the key and\n",
        "                       value sequences at dim=1.\n",
        "        dropout_p: probability of an element to be zeroed.\n",
        "        out_proj_weight, out_proj_bias: the output projection weight and bias.\n",
        "        training: apply dropout if is ``True``.\n",
        "        key_padding_mask: if provided, specified padding elements in the key will\n",
        "            be ignored by the attention. This is an binary mask. When the value is True,\n",
        "            the corresponding value on the attention layer will be filled with -inf.\n",
        "        need_weights: output attn_output_weights.\n",
        "        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n",
        "            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n",
        "        use_separate_proj_weight: the function accept the proj. weights for query, key,\n",
        "            and value in different forms. If false, in_proj_weight will be used, which is\n",
        "            a combination of q_proj_weight, k_proj_weight, v_proj_weight.\n",
        "        q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.\n",
        "        static_k, static_v: static key and value used for attention operators.\n",
        "\n",
        "\n",
        "    Shape:\n",
        "        Inputs:\n",
        "        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
        "          the embedding dimension.\n",
        "        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
        "          the embedding dimension.\n",
        "        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
        "          the embedding dimension.\n",
        "        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n",
        "          If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions\n",
        "          will be unchanged. If a BoolTensor is provided, the positions with the\n",
        "          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
        "        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n",
        "          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,\n",
        "          S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked\n",
        "          positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n",
        "          while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n",
        "          are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
        "          is provided, it will be added to the attention weight.\n",
        "        - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n",
        "          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n",
        "        - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n",
        "          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n",
        "\n",
        "        Outputs:\n",
        "        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
        "          E is the embedding dimension.\n",
        "        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n",
        "          L is the target sequence length, S is the source sequence length.\n",
        "    \"\"\"\n",
        "    tens_ops = (query, key, value, in_proj_weight, in_proj_bias, bias_k, bias_v, out_proj_weight, out_proj_bias)\n",
        "    if has_torch_function(tens_ops):\n",
        "        return handle_torch_function(\n",
        "            multi_head_attention_forward,\n",
        "            tens_ops,\n",
        "            query,\n",
        "            key,\n",
        "            value,\n",
        "            embed_dim_to_check,\n",
        "            num_heads,\n",
        "            in_proj_weight,\n",
        "            in_proj_bias,\n",
        "            bias_k,\n",
        "            bias_v,\n",
        "            add_zero_attn,\n",
        "            dropout_p,\n",
        "            out_proj_weight,\n",
        "            out_proj_bias,\n",
        "            training=training,\n",
        "            key_padding_mask=key_padding_mask,\n",
        "            need_weights=need_weights,\n",
        "            attn_mask=attn_mask,\n",
        "            use_separate_proj_weight=use_separate_proj_weight,\n",
        "            q_proj_weight=q_proj_weight,\n",
        "            k_proj_weight=k_proj_weight,\n",
        "            v_proj_weight=v_proj_weight,\n",
        "            static_k=static_k,\n",
        "            static_v=static_v,\n",
        "        )\n",
        "    tgt_len, bsz, embed_dim = query.size()\n",
        "    assert embed_dim == embed_dim_to_check\n",
        "    # allow MHA to have different sizes for the feature dimension\n",
        "    assert key.size(0) == value.size(0) and key.size(1) == value.size(1)\n",
        "\n",
        "    head_dim = embed_dim // num_heads\n",
        "    assert head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
        "    scaling = float(head_dim) ** -0.5\n",
        "\n",
        "    if not use_separate_proj_weight:\n",
        "        if (query is key or torch.equal(query, key)) and (key is value or torch.equal(key, value)):\n",
        "            # self-attention\n",
        "            q, k, v = linear(query, in_proj_weight, in_proj_bias).chunk(3, dim=-1)\n",
        "\n",
        "        elif key is value or torch.equal(key, value):\n",
        "            # encoder-decoder attention\n",
        "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
        "            _b = in_proj_bias\n",
        "            _start = 0\n",
        "            _end = embed_dim\n",
        "            _w = in_proj_weight[_start:_end, :]\n",
        "            if _b is not None:\n",
        "                _b = _b[_start:_end]\n",
        "            q = linear(query, _w, _b)\n",
        "\n",
        "            if key is None:\n",
        "                assert value is None\n",
        "                k = None\n",
        "                v = None\n",
        "            else:\n",
        "\n",
        "                # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
        "                _b = in_proj_bias\n",
        "                _start = embed_dim\n",
        "                _end = None\n",
        "                _w = in_proj_weight[_start:, :]\n",
        "                if _b is not None:\n",
        "                    _b = _b[_start:]\n",
        "                k, v = linear(key, _w, _b).chunk(2, dim=-1)\n",
        "\n",
        "        else:\n",
        "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
        "            _b = in_proj_bias\n",
        "            _start = 0\n",
        "            _end = embed_dim\n",
        "            _w = in_proj_weight[_start:_end, :]\n",
        "            if _b is not None:\n",
        "                _b = _b[_start:_end]\n",
        "            q = linear(query, _w, _b)\n",
        "\n",
        "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
        "            _b = in_proj_bias\n",
        "            _start = embed_dim\n",
        "            _end = embed_dim * 2\n",
        "            _w = in_proj_weight[_start:_end, :]\n",
        "            if _b is not None:\n",
        "                _b = _b[_start:_end]\n",
        "            k = linear(key, _w, _b)\n",
        "\n",
        "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
        "            _b = in_proj_bias\n",
        "            _start = embed_dim * 2\n",
        "            _end = None\n",
        "            _w = in_proj_weight[_start:, :]\n",
        "            if _b is not None:\n",
        "                _b = _b[_start:]\n",
        "            v = linear(value, _w, _b)\n",
        "    else:\n",
        "        q_proj_weight_non_opt = torch.jit._unwrap_optional(q_proj_weight)\n",
        "        len1, len2 = q_proj_weight_non_opt.size()\n",
        "        assert len1 == embed_dim and len2 == query.size(-1)\n",
        "\n",
        "        k_proj_weight_non_opt = torch.jit._unwrap_optional(k_proj_weight)\n",
        "        len1, len2 = k_proj_weight_non_opt.size()\n",
        "        assert len1 == embed_dim and len2 == key.size(-1)\n",
        "\n",
        "        v_proj_weight_non_opt = torch.jit._unwrap_optional(v_proj_weight)\n",
        "        len1, len2 = v_proj_weight_non_opt.size()\n",
        "        assert len1 == embed_dim and len2 == value.size(-1)\n",
        "\n",
        "        if in_proj_bias is not None:\n",
        "            q = linear(query, q_proj_weight_non_opt, in_proj_bias[0:embed_dim])\n",
        "            k = linear(key, k_proj_weight_non_opt, in_proj_bias[embed_dim : (embed_dim * 2)])\n",
        "            v = linear(value, v_proj_weight_non_opt, in_proj_bias[(embed_dim * 2) :])\n",
        "        else:\n",
        "            q = linear(query, q_proj_weight_non_opt, in_proj_bias)\n",
        "            k = linear(key, k_proj_weight_non_opt, in_proj_bias)\n",
        "            v = linear(value, v_proj_weight_non_opt, in_proj_bias)\n",
        "    q = q * scaling\n",
        "\n",
        "    if attn_mask is not None:\n",
        "        assert (\n",
        "            attn_mask.dtype == torch.float32\n",
        "            or attn_mask.dtype == torch.float64\n",
        "            or attn_mask.dtype == torch.float16\n",
        "            or attn_mask.dtype == torch.uint8\n",
        "            or attn_mask.dtype == torch.bool\n",
        "        ), \"Only float, byte, and bool types are supported for attn_mask, not {}\".format(attn_mask.dtype)\n",
        "        if attn_mask.dtype == torch.uint8:\n",
        "            warnings.warn(\"Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
        "            attn_mask = attn_mask.to(torch.bool)\n",
        "\n",
        "        if attn_mask.dim() == 2:\n",
        "            attn_mask = attn_mask.unsqueeze(0)\n",
        "            if list(attn_mask.size()) != [1, query.size(0), key.size(0)]:\n",
        "                raise RuntimeError(\"The size of the 2D attn_mask is not correct.\")\n",
        "        elif attn_mask.dim() == 3:\n",
        "            if list(attn_mask.size()) != [bsz * num_heads, query.size(0), key.size(0)]:\n",
        "                raise RuntimeError(\"The size of the 3D attn_mask is not correct.\")\n",
        "        else:\n",
        "            raise RuntimeError(\"attn_mask's dimension {} is not supported\".format(attn_mask.dim()))\n",
        "        # attn_mask's dim is 3 now.\n",
        "\n",
        "    # convert ByteTensor key_padding_mask to bool\n",
        "    if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n",
        "        warnings.warn(\n",
        "            \"Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\"\n",
        "        )\n",
        "        key_padding_mask = key_padding_mask.to(torch.bool)\n",
        "\n",
        "    if bias_k is not None and bias_v is not None:\n",
        "        if static_k is None and static_v is None:\n",
        "            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])\n",
        "            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])\n",
        "            if attn_mask is not None:\n",
        "                attn_mask = pad(attn_mask, (0, 1))\n",
        "            if key_padding_mask is not None:\n",
        "                key_padding_mask = pad(key_padding_mask, (0, 1))\n",
        "        else:\n",
        "            assert static_k is None, \"bias cannot be added to static key.\"\n",
        "            assert static_v is None, \"bias cannot be added to static value.\"\n",
        "    else:\n",
        "        assert bias_k is None\n",
        "        assert bias_v is None\n",
        "\n",
        "    q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
        "    if k is not None:\n",
        "        k = k.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
        "    if v is not None:\n",
        "        v = v.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
        "\n",
        "    if static_k is not None:\n",
        "        assert static_k.size(0) == bsz * num_heads\n",
        "        assert static_k.size(2) == head_dim\n",
        "        k = static_k\n",
        "\n",
        "    if static_v is not None:\n",
        "        assert static_v.size(0) == bsz * num_heads\n",
        "        assert static_v.size(2) == head_dim\n",
        "        v = static_v\n",
        "\n",
        "    src_len = k.size(1)\n",
        "\n",
        "    if key_padding_mask is not None:\n",
        "        assert key_padding_mask.size(0) == bsz\n",
        "        assert key_padding_mask.size(1) == src_len\n",
        "\n",
        "    if add_zero_attn:\n",
        "        src_len += 1\n",
        "        k = torch.cat([k, torch.zeros((k.size(0), 1) + k.size()[2:], dtype=k.dtype, device=k.device)], dim=1)\n",
        "        v = torch.cat([v, torch.zeros((v.size(0), 1) + v.size()[2:], dtype=v.dtype, device=v.device)], dim=1)\n",
        "        if attn_mask is not None:\n",
        "            attn_mask = pad(attn_mask, (0, 1))\n",
        "        if key_padding_mask is not None:\n",
        "            key_padding_mask = pad(key_padding_mask, (0, 1))\n",
        "\n",
        "    attn_output_weights = torch.bmm(q, k.transpose(1, 2))\n",
        "    assert list(attn_output_weights.size()) == [bsz * num_heads, tgt_len, src_len]\n",
        "\n",
        "    if attn_mask is not None:\n",
        "        if attn_mask.dtype == torch.bool:\n",
        "            attn_output_weights.masked_fill_(attn_mask, float(\"-inf\"))\n",
        "        else:\n",
        "            attn_output_weights += attn_mask\n",
        "\n",
        "    if key_padding_mask is not None:\n",
        "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
        "        attn_output_weights = attn_output_weights.masked_fill(\n",
        "            key_padding_mask.unsqueeze(1).unsqueeze(2),\n",
        "            float(\"-inf\"),\n",
        "        )\n",
        "        attn_output_weights = attn_output_weights.view(bsz * num_heads, tgt_len, src_len)\n",
        "\n",
        "    attn_output_weights = softmax(attn_output_weights, dim=-1)\n",
        "    attn_output_weights = dropout(attn_output_weights, p=dropout_p, training=training)\n",
        "\n",
        "    attn_output = torch.bmm(attn_output_weights, v)\n",
        "    assert list(attn_output.size()) == [bsz * num_heads, tgt_len, head_dim]\n",
        "    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
        "\n",
        "    #attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
        "\n",
        "    def kronecker_product1(a, b):\n",
        "      siz1 = torch.Size(torch.tensor(a.shape[-2:]) * torch.tensor(b.shape[-2:]))\n",
        "      res = a.unsqueeze(-1).unsqueeze(-3) * b.unsqueeze(-2).unsqueeze(-4)\n",
        "      siz0 = res.shape[:-4]\n",
        "      out = res.reshape(siz0 + siz1)\n",
        "      return out\n",
        "\n",
        "    out_proj_weight2 = torch.sum(kronecker_product1(a, s), dim=0)\n",
        "\n",
        "\n",
        "    attn_output = linear(attn_output, out_proj_weight2, out_proj_bias)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if need_weights:\n",
        "        # average attention weights over heads\n",
        "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
        "        return attn_output, attn_output_weights.sum(dim=1) / num_heads\n",
        "    else:\n",
        "        return attn_output, None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-puq5PXxr6y"
      },
      "source": [
        "class PHMMultiheadAttention(Module):\n",
        "    r\"\"\"Allows the model to jointly attend to information\n",
        "    from different representation subspaces.\n",
        "    See `Attention Is All You Need <https://arxiv.org/abs/1706.03762>`_\n",
        "\n",
        "    .. math::\n",
        "        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n",
        "\n",
        "    where :math:`head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`.\n",
        "\n",
        "    Args:\n",
        "        embed_dim: total dimension of the model.\n",
        "        num_heads: parallel attention heads.\n",
        "        dropout: a Dropout layer on attn_output_weights. Default: 0.0.\n",
        "        bias: add bias as module parameter. Default: True.\n",
        "        add_bias_kv: add bias to the key and value sequences at dim=0.\n",
        "        add_zero_attn: add a new batch of zeros to the key and\n",
        "                       value sequences at dim=1.\n",
        "        kdim: total number of features in key. Default: None.\n",
        "        vdim: total number of features in value. Default: None.\n",
        "        batch_first: If ``True``, then the input and output tensors are provided\n",
        "            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
        "\n",
        "    Note that if :attr:`kdim` and :attr:`vdim` are None, they will be set\n",
        "    to :attr:`embed_dim` such that query, key, and value have the same\n",
        "    number of features.\n",
        "\n",
        "    Examples::\n",
        "\n",
        "        >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
        "    \"\"\"\n",
        "    __constants__ = ['batch_first']\n",
        "    bias_k: Optional[torch.Tensor]\n",
        "    bias_v: Optional[torch.Tensor]\n",
        "\n",
        "    def __init__(self, phm, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False,\n",
        "                 kdim=None, vdim=None, batch_first=False, device=None, dtype=None) -> None:\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        super(PHMMultiheadAttention, self).__init__()\n",
        "        self.phm = phm\n",
        "        self.embed_dim = embed_dim\n",
        "        self.kdim = kdim if kdim is not None else embed_dim\n",
        "        self.vdim = vdim if vdim is not None else embed_dim\n",
        "        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.batch_first = batch_first\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        if self._qkv_same_embed_dim is False:\n",
        "            self.q_proj_weight = Parameter(torch.empty((embed_dim, embed_dim), **factory_kwargs))\n",
        "            self.k_proj_weight = Parameter(torch.empty((embed_dim, self.kdim), **factory_kwargs))\n",
        "            self.v_proj_weight = Parameter(torch.empty((embed_dim, self.vdim), **factory_kwargs))\n",
        "            self.register_parameter('in_proj_weight', None)\n",
        "        else:\n",
        "            #self.in_proj_weight = Parameter(torch.empty((3 * embed_dim, embed_dim), **factory_kwargs))\n",
        "            #self.in_proj_weight = PHMLayer(2, 3 * embed_dim, embed_dim)\n",
        "            \n",
        "            #self.in_proj_weight = torch.zeros((embed_dim, 3 * embed_dim)) #out in\n",
        "\n",
        "            self.a = Parameter(torch.nn.init.xavier_uniform_(torch.zeros((phm, phm, phm))))\n",
        "\n",
        "            self.s = Parameter(torch.nn.init.xavier_uniform_(torch.zeros((phm, (3*embed_dim)//phm, embed_dim//phm))))\n",
        "            \n",
        "            self.register_parameter('q_proj_weight', None)\n",
        "            self.register_parameter('k_proj_weight', None)\n",
        "            self.register_parameter('v_proj_weight', None)\n",
        "\n",
        "        if bias:\n",
        "            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim, **factory_kwargs))\n",
        "        else:\n",
        "            self.register_parameter('in_proj_bias', None)\n",
        "        \n",
        "        #self.out_proj = NonDynamicallyQuantizableLinear(embed_dim, embed_dim, bias=bias, **factory_kwargs)\n",
        "        self.out_proj = PHMLayer(phm, embed_dim, embed_dim)\n",
        "\n",
        "\n",
        "        if add_bias_kv:\n",
        "            self.bias_k = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n",
        "            self.bias_v = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n",
        "        else:\n",
        "            self.bias_k = self.bias_v = None\n",
        "\n",
        "        self.add_zero_attn = add_zero_attn\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        if self._qkv_same_embed_dim:\n",
        "            #xavier_uniform_(self.in_proj_weight)\n",
        "            pass\n",
        "        else:\n",
        "            xavier_uniform_(self.q_proj_weight)\n",
        "            xavier_uniform_(self.k_proj_weight)\n",
        "            xavier_uniform_(self.v_proj_weight)\n",
        "\n",
        "        if self.in_proj_bias is not None:\n",
        "            constant_(self.in_proj_bias, 0.)\n",
        "            constant_(self.out_proj.bias, 0.)\n",
        "        if self.bias_k is not None:\n",
        "            xavier_normal_(self.bias_k)\n",
        "        if self.bias_v is not None:\n",
        "            xavier_normal_(self.bias_v)\n",
        "\n",
        "    def kronecker_product1(self, a, b):\n",
        "        siz1 = torch.Size(torch.tensor(a.shape[-2:]) * torch.tensor(b.shape[-2:]))\n",
        "        res = a.unsqueeze(-1).unsqueeze(-3) * b.unsqueeze(-2).unsqueeze(-4)\n",
        "        siz0 = res.shape[:-4]\n",
        "        out = res.reshape(siz0 + siz1)\n",
        "        return out\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        # Support loading old MultiheadAttention checkpoints generated by v1.1.0\n",
        "        if '_qkv_same_embed_dim' not in state:\n",
        "            state['_qkv_same_embed_dim'] = True\n",
        "\n",
        "        super(PHMMultiheadAttention, self).__setstate__(state)\n",
        "\n",
        "    def forward(self, query: Tensor, key: Tensor, value: Tensor, key_padding_mask: Optional[Tensor] = None,\n",
        "                need_weights: bool = True, attn_mask: Optional[Tensor] = None):\n",
        "        r\"\"\"\n",
        "    Args:\n",
        "        query, key, value: map a query and a set of key-value pairs to an output.\n",
        "            See \"Attention Is All You Need\" for more details.\n",
        "        key_padding_mask: if provided, specified padding elements in the key will\n",
        "            be ignored by the attention. When given a binary mask and a value is True,\n",
        "            the corresponding value on the attention layer will be ignored. When given\n",
        "            a byte mask and a value is non-zero, the corresponding value on the attention\n",
        "            layer will be ignored\n",
        "        need_weights: output attn_output_weights.\n",
        "        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n",
        "            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n",
        "\n",
        "    Shapes for inputs:\n",
        "        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
        "          the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n",
        "        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
        "          the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n",
        "        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
        "          the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n",
        "        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n",
        "          If a ByteTensor is provided, the non-zero positions will be ignored while the position\n",
        "          with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the\n",
        "          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
        "        - attn_mask: if a 2D mask: :math:`(L, S)` where L is the target sequence length, S is the\n",
        "          source sequence length.\n",
        "\n",
        "          If a 3D mask: :math:`(N\\cdot\\text{num\\_heads}, L, S)` where N is the batch size, L is the target sequence\n",
        "          length, S is the source sequence length. ``attn_mask`` ensure that position i is allowed to attend\n",
        "          the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n",
        "          while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n",
        "          is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
        "          is provided, it will be added to the attention weight.\n",
        "\n",
        "    Shapes for outputs:\n",
        "        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
        "          E is the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n",
        "        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n",
        "          L is the target sequence length, S is the source sequence length.\n",
        "        \"\"\"\n",
        "        self.in_proj_weight = torch.sum(self.kronecker_product1(self.a, self.s), dim=0)\n",
        "\n",
        "        if self.batch_first:\n",
        "            query, key, value = [x.transpose(1, 0) for x in (query, key, value)]\n",
        "\n",
        "        if not self._qkv_same_embed_dim:\n",
        "            attn_output, attn_output_weights = phm_multi_head_attention_forward(\n",
        "                self.phm, self.out_proj.a, self.out_proj.s, query, key, value, self.embed_dim, self.num_heads,\n",
        "                self.in_proj_weight, self.in_proj_bias,\n",
        "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
        "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
        "                training=self.training,\n",
        "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
        "                attn_mask=attn_mask, use_separate_proj_weight=True,\n",
        "                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\n",
        "                v_proj_weight=self.v_proj_weight,\n",
        "                )\n",
        "        else:\n",
        "            attn_output, attn_output_weights = phm_multi_head_attention_forward(\n",
        "                self.phm, self.out_proj.a, self.out_proj.s, query, key, value, self.embed_dim, self.num_heads,\n",
        "                self.in_proj_weight, self.in_proj_bias,\n",
        "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
        "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
        "                training=self.training,\n",
        "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
        "                attn_mask=attn_mask)\n",
        "        if self.batch_first:\n",
        "            return attn_output.transpose(1, 0), attn_output_weights\n",
        "        else:\n",
        "            return attn_output, attn_output_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X50ThC5O109i"
      },
      "source": [
        "def _get_activation_fn(activation):\n",
        "    if activation == \"relu\":\n",
        "        return F.relu\n",
        "    elif activation == \"gelu\":\n",
        "        return F.gelu\n",
        "\n",
        "    raise RuntimeError(\"activation should be relu/gelu, not {}\".format(activation))\n",
        "\n",
        "class PHMTransformerEncoderLayer(Module):\n",
        "\n",
        "    __constants__ = ['batch_first']\n",
        "\n",
        "    def __init__(self, phm, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\",\n",
        "                 layer_norm_eps=1e-5, batch_first=False,\n",
        "                 device=None, dtype=None) -> None:\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        super(PHMTransformerEncoderLayer, self).__init__()\n",
        "        self.self_attn = PHMMultiheadAttention(phm, d_model, nhead, dropout=dropout)\n",
        "        self.linear1 = PHMLayer(phm, d_model, dim_feedforward)\n",
        "        self.dropout = Dropout(dropout)\n",
        "        self.linear2 = PHMLayer(phm, dim_feedforward, d_model)\n",
        "        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        self.dropout1 = Dropout(dropout)\n",
        "        self.dropout2 = Dropout(dropout)\n",
        "\n",
        "        self.activation = _get_activation_fn(activation)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        if 'activation' not in state:\n",
        "            state['activation'] = F.relu\n",
        "        super(PHMTransformerEncoderLayer, self).__setstate__(state)\n",
        "\n",
        "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
        " \n",
        "        src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US8QRCVYtVVw"
      },
      "source": [
        "class PHMTransformerDecoderLayer(Module):\n",
        "\n",
        "    __constants__ = ['batch_first']\n",
        "\n",
        "    def __init__(self, phm, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\",\n",
        "                 layer_norm_eps=1e-5, batch_first=False, device=None, dtype=None) -> None:\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        super(PHMTransformerDecoderLayer, self).__init__()\n",
        "        self.self_attn = PHMMultiheadAttention(phm, d_model, nhead, dropout=dropout)\n",
        "        self.multihead_attn = PHMMultiheadAttention(phm, d_model, nhead, dropout=dropout)\n",
        "        self.linear1 = PHMLayer(phm, d_model, dim_feedforward)\n",
        "        self.dropout = Dropout(dropout)\n",
        "        self.linear2 = PHMLayer(phm, dim_feedforward, d_model)\n",
        "        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        self.norm3 = LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        self.dropout1 = Dropout(dropout)\n",
        "        self.dropout2 = Dropout(dropout)\n",
        "        self.dropout3 = Dropout(dropout)\n",
        "\n",
        "        self.activation = _get_activation_fn(activation)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        if 'activation' not in state:\n",
        "            state['activation'] = F.relu\n",
        "        super(PHMTransformerDecoderLayer, self).__setstate__(state)\n",
        "\n",
        "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None,\n",
        "                tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
        "\n",
        "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt = self.norm1(tgt)\n",
        "        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt = self.norm2(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        tgt = self.norm3(tgt)\n",
        "        return tgt\n",
        "\n",
        "    def _get_clones(module, N):\n",
        "        return ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "\n",
        "    def _get_activation_fn(activation):\n",
        "        if activation == \"relu\":\n",
        "            return F.relu\n",
        "        elif activation == \"gelu\":\n",
        "            return F.gelu\n",
        "\n",
        "        raise RuntimeError(\"activation should be relu/gelu, not {}\".format(activation))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-2di7MD1o23"
      },
      "source": [
        "from torch.nn import TransformerEncoder, TransformerDecoder\n",
        "\n",
        "class PHMTransformer(Module):\n",
        "    def __init__(self, phm, nhead, num_encoder_layers: int, num_decoder_layers: int,\n",
        "                 emb_size: int, src_vocab_size: int, tgt_vocab_size: int,\n",
        "                 dim_feedforward:int = 512, dropout:float = 0.1):\n",
        "        super(PHMTransformer, self).__init__()\n",
        "        encoder_layer = PHMTransformerEncoderLayer(phm, d_model=emb_size, nhead=nhead,\n",
        "                                                dim_feedforward=dim_feedforward)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "        decoder_layer = PHMTransformerDecoderLayer(phm, d_model=emb_size, nhead=nhead,\n",
        "                                                dim_feedforward=dim_feedforward)\n",
        "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "                \n",
        "        self.generator = PHMLayer(phm, emb_size, tgt_vocab_size)    ###################### burayi da degistir\n",
        "        self.src_tok_emb = PHMTokenEmbedding(src_vocab_size, emb_size, phm)\n",
        "        self.tgt_tok_emb = PHMTokenEmbedding(tgt_vocab_size, emb_size, phm)\n",
        "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self, src: Tensor, trg: Tensor, src_mask: Tensor,\n",
        "                tgt_mask: Tensor, src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor, memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        memory = self.transformer_encoder(src_emb, src_mask, src_padding_mask)\n",
        "        outs = self.transformer_decoder(tgt_emb, memory, tgt_mask, None,\n",
        "                                        tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer_encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer_decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCCahYjmt656"
      },
      "source": [
        "from torch.nn import (TransformerEncoder, TransformerDecoder,\n",
        "                      TransformerEncoderLayer, TransformerDecoderLayer)\n",
        "\n",
        "\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self, nhead, num_encoder_layers: int, num_decoder_layers: int,\n",
        "                 emb_size: int, src_vocab_size: int, tgt_vocab_size: int,\n",
        "                 dim_feedforward:int = 512, dropout:float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        encoder_layer = TransformerEncoderLayer(d_model=emb_size, nhead=nhead,\n",
        "                                                dim_feedforward=dim_feedforward)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "        decoder_layer = TransformerDecoderLayer(d_model=emb_size, nhead=NHEAD,\n",
        "                                                dim_feedforward=dim_feedforward)\n",
        "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "                \n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self, src: Tensor, trg: Tensor, src_mask: Tensor,\n",
        "                tgt_mask: Tensor, src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor, memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        memory = self.transformer_encoder(src_emb, src_mask, src_padding_mask)\n",
        "        outs = self.transformer_decoder(tgt_emb, memory, tgt_mask, None,\n",
        "                                        tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer_encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer_decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGp9atlkgnvV"
      },
      "source": [
        "class PHMEmbedding(Module):\n",
        "   \n",
        "    __constants__ = ['num_embeddings', 'embedding_dim', 'padding_idx', 'max_norm',\n",
        "                     'norm_type', 'scale_grad_by_freq', 'sparse']\n",
        "\n",
        "    def __init__(self, num_embeddings: int, embedding_dim: int, phm: int, padding_idx: Optional[int] = None,\n",
        "                 max_norm: Optional[float] = None, norm_type: float = 2., scale_grad_by_freq: bool = False,\n",
        "                 sparse: bool = False, _weight: Optional[Tensor] = None,\n",
        "                 device=None, dtype=None):\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        super(PHMEmbedding, self).__init__()\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.embedding_dim = embedding_dim\n",
        "        if padding_idx is not None:\n",
        "            if padding_idx > 0:\n",
        "                assert padding_idx < self.num_embeddings, 'Padding_idx must be within num_embeddings'\n",
        "            elif padding_idx < 0:\n",
        "                assert padding_idx >= -self.num_embeddings, 'Padding_idx must be within num_embeddings'\n",
        "                padding_idx = self.num_embeddings + padding_idx\n",
        "        self.padding_idx = padding_idx\n",
        "        self.max_norm = max_norm\n",
        "        self.norm_type = norm_type\n",
        "        self.scale_grad_by_freq = scale_grad_by_freq\n",
        "        if _weight is None:\n",
        "            self.a = Parameter(torch.nn.init.xavier_uniform_(torch.zeros((phm, phm, phm))))\n",
        "\n",
        "            self.s = Parameter(torch.nn.init.xavier_uniform_(torch.zeros((phm, num_embeddings//phm, embedding_dim//phm))))\n",
        "\n",
        "            self.weight = torch.zeros((num_embeddings, embedding_dim))\n",
        "            #self.weight = Parameter(torch.empty((num_embeddings, embedding_dim), **factory_kwargs))\n",
        "            #self.reset_parameters()\n",
        "        else:\n",
        "            assert list(_weight.shape) == [num_embeddings, embedding_dim], \\\n",
        "                'Shape of weight does not match num_embeddings and embedding_dim'\n",
        "            self.weight = Parameter(_weight)\n",
        "\n",
        "        self.sparse = sparse\n",
        "\n",
        "    def reset_parameters(self) -> None:\n",
        "        #init.normal_(self.weight)\n",
        "        #self._fill_padding_idx_with_zero()\n",
        "        pass\n",
        "\n",
        "    def _fill_padding_idx_with_zero(self) -> None:\n",
        "        if self.padding_idx is not None:\n",
        "            with torch.no_grad():\n",
        "                self.weight[self.padding_idx].fill_(0)\n",
        "\n",
        "    def kronecker_product1(self, a, b):\n",
        "        siz1 = torch.Size(torch.tensor(a.shape[-2:]) * torch.tensor(b.shape[-2:]))\n",
        "        res = a.unsqueeze(-1).unsqueeze(-3) * b.unsqueeze(-2).unsqueeze(-4)\n",
        "        siz0 = res.shape[:-4]\n",
        "        out = res.reshape(siz0 + siz1)\n",
        "        return out\n",
        "\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        self.weight = torch.sum(self.kronecker_product1(self.a, self.s), dim=0)\n",
        "        #self._fill_padding_idx_with_zero()\n",
        "        return F.embedding(\n",
        "            input, self.weight, self.padding_idx, self.max_norm,\n",
        "            self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        s = '{num_embeddings}, {embedding_dim}'\n",
        "        if self.padding_idx is not None:\n",
        "            s += ', padding_idx={padding_idx}'\n",
        "        if self.max_norm is not None:\n",
        "            s += ', max_norm={max_norm}'\n",
        "        if self.norm_type != 2:\n",
        "            s += ', norm_type={norm_type}'\n",
        "        if self.scale_grad_by_freq is not False:\n",
        "            s += ', scale_grad_by_freq={scale_grad_by_freq}'\n",
        "        if self.sparse is not False:\n",
        "            s += ', sparse=True'\n",
        "        return s.format(**self.__dict__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5n_U0CjTt658"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, emb_size: int, dropout, maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + \n",
        "                            self.pos_embedding[:token_embedding.size(0),:])\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "class PHMTokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size, phm):\n",
        "        super(PHMTokenEmbedding, self).__init__()\n",
        "        self.embedding = PHMEmbedding(vocab_size, emb_size, phm)\n",
        "        self.emb_size = emb_size\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QhCOolDt66A"
      },
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "  src_seq_len = src.shape[0]\n",
        "  tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "  tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "  src_mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).type(torch.bool)\n",
        "\n",
        "  src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "  tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "  return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPSOgm2G7NcX"
      },
      "source": [
        "# Instantiate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfVA2zobt66B"
      },
      "source": [
        "SRC_VOCAB_SIZE = len(german_vocab)\n",
        "TGT_VOCAB_SIZE = len(english_vocab)\n",
        "EMB_SIZE = 256\n",
        "NHEAD = 4\n",
        "FFN_HID_DIM = 256\n",
        "BATCH_SIZE = 16\n",
        "NUM_ENCODER_LAYERS = 2\n",
        "NUM_DECODER_LAYERS = 2\n",
        "N = 4 ### this is the scaling factor\n",
        "\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\"\"\"\n",
        "transformer = Seq2SeqTransformer(NHEAD, NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, \n",
        "                                 EMB_SIZE, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE,\n",
        "                                 FFN_HID_DIM)\n",
        "\"\"\"\n",
        "transformer = PHMTransformer(N, NHEAD, NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, \n",
        "                                 EMB_SIZE, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE,\n",
        "                                 FFN_HID_DIM)\n",
        "\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1: ##########################\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(device)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.05\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07yoyaJ-yMfl",
        "outputId": "cd92735f-ddf1-4dbf-a319-d8890eecc6a8"
      },
      "source": [
        "print(transformer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PHMTransformer(\n",
            "  (transformer_encoder): TransformerEncoder(\n",
            "    (layers): ModuleList(\n",
            "      (0): PHMTransformerEncoderLayer(\n",
            "        (self_attn): PHMMultiheadAttention(\n",
            "          (out_proj): PHMLayer(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (linear1): PHMLayer(in_features=256, out_features=256, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): PHMLayer(in_features=256, out_features=256, bias=True)\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): PHMTransformerEncoderLayer(\n",
            "        (self_attn): PHMMultiheadAttention(\n",
            "          (out_proj): PHMLayer(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (linear1): PHMLayer(in_features=256, out_features=256, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): PHMLayer(in_features=256, out_features=256, bias=True)\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (transformer_decoder): TransformerDecoder(\n",
            "    (layers): ModuleList(\n",
            "      (0): PHMTransformerDecoderLayer(\n",
            "        (self_attn): PHMMultiheadAttention(\n",
            "          (out_proj): PHMLayer(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (multihead_attn): PHMMultiheadAttention(\n",
            "          (out_proj): PHMLayer(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (linear1): PHMLayer(in_features=256, out_features=256, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): PHMLayer(in_features=256, out_features=256, bias=True)\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        (dropout3): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): PHMTransformerDecoderLayer(\n",
            "        (self_attn): PHMMultiheadAttention(\n",
            "          (out_proj): PHMLayer(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (multihead_attn): PHMMultiheadAttention(\n",
            "          (out_proj): PHMLayer(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (linear1): PHMLayer(in_features=256, out_features=256, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): PHMLayer(in_features=256, out_features=256, bias=True)\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        (dropout3): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): PHMLayer(in_features=256, out_features=32768, bias=True)\n",
            "  (src_tok_emb): PHMTokenEmbedding(\n",
            "    (embedding): PHMEmbedding(32768, 256)\n",
            "  )\n",
            "  (tgt_tok_emb): PHMTokenEmbedding(\n",
            "    (embedding): PHMEmbedding(32768, 256)\n",
            "  )\n",
            "  (positional_encoding): PositionalEncoding(\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVnO79YW6xxm"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WSwqshe6zNp",
        "outputId": "3a70ea10-429b-4792-83f9-f91977254fce"
      },
      "source": [
        "count_parameters(transformer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6863296"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "id": "pCZT3IE6ZGx8",
        "outputId": "bf035595-8956-43ae-85bd-1e7929757e2f"
      },
      "source": [
        "import wandb\n",
        "wandb.init(project='transformer', entity='demegire')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdemegire\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.11.0<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">breezy-tree-102</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/demegire/transformer\" target=\"_blank\">https://wandb.ai/demegire/transformer</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/demegire/transformer/runs/1jwvggs6\" target=\"_blank\">https://wandb.ai/demegire/transformer/runs/1jwvggs6</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210728_092709-1jwvggs6</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7fe91d60ae50>"
            ],
            "text/html": [
              "<h1>Run(1jwvggs6)</h1><iframe src=\"https://wandb.ai/demegire/transformer/runs/1jwvggs6\" style=\"border:none;width:100%;height:400px\"></iframe>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMay1qm3ZXJ_"
      },
      "source": [
        "config = wandb.config\n",
        "config.learning_rate = optimizer.state_dict()['param_groups'][0]['lr']\n",
        "config.SRC_VOCAB_SIZE = SRC_VOCAB_SIZE\n",
        "config.TGT_VOCAB_SIZE = TGT_VOCAB_SIZE\n",
        "config.EMB_SIZE = EMB_SIZE\n",
        "config.NHEAD = NHEAD\n",
        "config.FFN_HID_DIM = FFN_HID_DIM\n",
        "config.BATCH_SIZE = BATCH_SIZE\n",
        "config.NUM_ENCODER_LAYERS = NUM_ENCODER_LAYERS\n",
        "config.NUM_DECODER_LAYERS = NUM_DECODER_LAYERS\n",
        "config.params = count_parameters(transformer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGoTUoUuk0Mc"
      },
      "source": [
        "from torch.nn.functional import softmax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wjaBUZ9t66E"
      },
      "source": [
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(device)\n",
        "    src_mask = src_mask.to(device)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(device)\n",
        "        memory_mask = torch.zeros(ys.shape[0], memory.shape[0]).to(device).type(torch.bool)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                                    .type(torch.bool)).to(device)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim = 1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "          break\n",
        "    return ys\n",
        "\n",
        "def beam_search(model, src, src_mask, max_len, start_symbol, beam_size=2, alpha=0.7):\n",
        "  with torch.no_grad():\n",
        "    src = src.to(device)\n",
        "    src_mask = src_mask.to(device)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n",
        "\n",
        "    memory = memory.to(device)\n",
        "    memory_mask = torch.zeros(ys.shape[0], memory.shape[0]).to(device).type(torch.bool)\n",
        "    tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                                      .type(torch.bool)).to(device)\n",
        "    out = model.decode(ys, memory, tgt_mask)\n",
        "    out = out.transpose(0, 1)\n",
        "    probs = model.generator(out[:, -1])\n",
        "\n",
        "    soft_probs = softmax(probs, dim=1)\n",
        "          \n",
        "    tops = torch.topk(soft_probs, beam_size, dim = 1)\n",
        "\n",
        "    sentences = []\n",
        "\n",
        "    for i in range(beam_size):\n",
        "      sentences.append((tops.values[0][i].item(), \n",
        "                        torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(tops.indices[0][i].item())], dim=0)))\n",
        "    while True:      \n",
        "      candidate_sentences = []\n",
        "      eos_index = []\n",
        "\n",
        "      for i in range(beam_size):\n",
        "        eos_flag = False\n",
        "        if sentences[i][1][-1].item() != EOS_IDX:\n",
        "          memory = memory.to(device)\n",
        "          memory_mask = torch.zeros(sentences[i][1].shape[0], memory.shape[0]).to(device).type(torch.bool)\n",
        "          tgt_mask = (generate_square_subsequent_mask(sentences[i][1].size(0))\n",
        "                                            .type(torch.bool)).to(device)\n",
        "          out = model.decode(sentences[i][1], memory, tgt_mask)\n",
        "          out = out.transpose(0, 1)\n",
        "          probs = model.generator(out[:, -1])\n",
        "          soft_probs = softmax(probs, dim=1)\n",
        "\n",
        "          tops = torch.topk(soft_probs, beam_size, dim = 1)\n",
        "\n",
        "          for j in range(beam_size):\n",
        "            candidate_sentences.append((1/(len(torch.cat([sentences[i][1], torch.ones(1, 1).type_as(src.data).fill_(tops.indices[0][j].item())], dim=0))**alpha) * sentences[i][0] * tops.values[0][j].item(),\n",
        "                                        torch.cat([sentences[i][1], torch.ones(1, 1).type_as(src.data).fill_(tops.indices[0][j].item())], dim=0) ))\n",
        "        else:\n",
        "          eos_index.append(i)\n",
        "          eos_flag = True\n",
        "\n",
        "      if eos_flag:\n",
        "        for idx in eos_index:\n",
        "          candidate_sentences.append(sentences[idx])\n",
        "\n",
        "      candidate_sentences.sort(key=lambda x: x[0], reverse=True)\n",
        "      sentences = candidate_sentences[:beam_size]\n",
        "      candidate_sentences = []\n",
        "\n",
        "      end_flag = True\n",
        "      max_flag = False\n",
        "\n",
        "      for idx in range(beam_size):\n",
        "        if sentences[idx][1][-1].item() != EOS_IDX :\n",
        "          end_flag = False\n",
        "\n",
        "      for idx in range(beam_size):\n",
        "        if len(sentences[idx][1]) == max_len:\n",
        "          end_flag = True\n",
        "\n",
        "      if end_flag or max_flag:\n",
        "        return sorted(sentences,key=lambda x: x[0], reverse=True)[0][1]\n",
        "      \n",
        "\n",
        "\n",
        "def translate(model, src, src_vocab, tgt_vocab, src_tokenizer):\n",
        "  model.eval()\n",
        "  tokens = [BOS_IDX] + [src_vocab.stoi[tok] for tok in src_tokenizer(src)]+ [EOS_IDX]\n",
        "  num_tokens = len(tokens)\n",
        "  src = (torch.LongTensor(tokens).reshape(num_tokens, 1) )\n",
        "  src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "  #tgt_tokens = greedy_decode(model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "  tgt_tokens = beam_search(model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX, beam_size=5, alpha=0.6).flatten()\n",
        "  return \" \".join([tgt_vocab.itos[tok] for tok in tgt_tokens]).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "CpwLDHwbt66E",
        "outputId": "92f54047-6b5d-40f0-edb2-de3359f752a9"
      },
      "source": [
        "translate(transformer, \"I have half a mind to hit you before you speak again\", german_vocab, english_vocab, de_tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' alike issuance cycles cycles usher usher apprentices charity circulating circulating Seppänen Seppänen Seppänen Seppänen Seppänen circulating circulating circulating'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZLle3ri_eh1"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "smoothie = SmoothingFunction().method4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPW4DFfE7YIG"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYI50I9jBHwu"
      },
      "source": [
        "with open('/content/dev/newstest2010.de') as f:\n",
        "    dev_lines_de = f.readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAmIKSY_Czhj"
      },
      "source": [
        "with open('/content/dev/newstest2010.en') as g:\n",
        "    dev_lines_en = g.readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhbcsaobB1-c"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "def bleu(model, src_lines, tgt_lines, src_vcb, tgt_vcb, src_tok):\n",
        "\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  assert len(src_lines) == len(tgt_lines)\n",
        "  n = len(src_lines)\n",
        "  scores = 0\n",
        "  \n",
        "  for mo, og in zip(src_lines, tgt_lines):\n",
        "    reference = [tokenizer.tokenize(og[:-3])]\n",
        "    candidate = tokenizer.tokenize(translate(model, mo[:-3], src_vcb, tgt_vcb, en_tokenizer))\n",
        "\n",
        "    try:\n",
        "      scores += sentence_bleu(reference, candidate, smoothing_function=smoothie)\n",
        "    except:\n",
        "      print('except')\n",
        "      pass\n",
        "  return scores/n\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTHIyMnp7bj-"
      },
      "source": [
        "Unlike the Style Transfer task the validation occurs during an epoch, so the implementation is slightly different"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLq0dLBCt66C"
      },
      "source": [
        "def train_epoch(model, train_iter, val_iter, optimizer):\n",
        "  \n",
        "  def evaluate(model, val_iter):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "    for idx, (src, tgt) in (enumerate(valid_iter)):\n",
        "      src = src.to(device)\n",
        "      tgt = tgt.to(device)\n",
        "\n",
        "      tgt_input = tgt[:-1, :]\n",
        "\n",
        "      src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "      logits = model(src, tgt_input, src_mask, tgt_mask,\n",
        "                                src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "      tgt_out = tgt[1:,:]\n",
        "      loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "      losses += loss.item()\n",
        "      if idx == 50:\n",
        "        return losses / 50\n",
        "    return losses / len(val_iter)\n",
        "\n",
        "\n",
        "\n",
        "  model.train()\n",
        "  losses = 0\n",
        "\n",
        "  for idx, (src, tgt) in enumerate(train_iter):\n",
        "      src = src.to(device)\n",
        "      tgt = tgt.to(device)\n",
        "            \n",
        "      tgt_input = tgt[:-1, :]\n",
        "\n",
        "      src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "      logits = model(src, tgt_input, src_mask, tgt_mask,\n",
        "                                src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      tgt_out = tgt[1:,:]\n",
        "      loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      losses += loss.item()\n",
        "\n",
        "      if idx % 50 == 0:\n",
        "        wandb.log({\"train_loss\": loss.item()})\n",
        "\n",
        "      if idx % 500 == 0:\n",
        "        val_loss = evaluate(model, valid_iter)\n",
        "        print(val_loss)\n",
        "        wandb.log({\"val_loss\": val_loss})\n",
        "        model.train()\n",
        "\n",
        "      if idx % 10000 == 0:\n",
        "        print(idx)\n",
        "\n",
        "      if idx == 50000:\n",
        "        return losses / len(train_iter)\n",
        "\n",
        "  return losses / len(train_iter)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EUBL4D5t66D",
        "outputId": "97b70dd8-6696-43fa-8483-b13baf333bcd"
      },
      "source": [
        "wandb.watch(transformer)\n",
        "start_time = time.time()\n",
        "train_loss = train_epoch(transformer, train_iter, valid_iter, optimizer)\n",
        "end_time = time.time()\n",
        "print(end_time-start_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10.598576469421387\n",
            "0\n",
            "7.173872318267822\n",
            "6.672988996505738\n",
            "6.474006509780883\n",
            "6.372186603546143\n",
            "6.277548971176148\n",
            "6.192550773620606\n",
            "6.169077386856079\n",
            "6.092277603149414\n",
            "6.064428472518921\n",
            "6.007866668701172\n",
            "5.971266860961914\n",
            "5.887045183181763\n",
            "5.8723788070678715\n",
            "5.836114101409912\n",
            "5.751456031799316\n",
            "5.7895409297943115\n",
            "5.706301603317261\n",
            "5.6919488525390625\n",
            "5.636869096755982\n",
            "5.646174039840698\n",
            "10000\n",
            "5.6134465789794925\n",
            "5.5279426097869875\n",
            "5.494049663543701\n",
            "5.494959659576416\n",
            "5.5014401721954345\n",
            "5.499492645263672\n",
            "5.432122173309327\n",
            "5.414782619476318\n",
            "5.406279373168945\n",
            "5.407729368209839\n",
            "5.325137166976929\n",
            "5.3358434963226316\n",
            "5.365396203994751\n",
            "5.2861355113983155\n",
            "5.289815769195557\n",
            "5.232405090332032\n",
            "5.288078517913818\n",
            "5.195194349288941\n",
            "5.1787729167938235\n",
            "5.17140751838684\n",
            "20000\n",
            "5.218420381546021\n",
            "5.187234020233154\n",
            "5.192825765609741\n",
            "5.132443227767944\n",
            "5.147489576339722\n",
            "5.14576735496521\n",
            "5.109126100540161\n",
            "5.150072746276855\n",
            "5.122694683074951\n",
            "5.129611721038819\n",
            "5.0585581398010255\n",
            "5.127977066040039\n",
            "5.105482959747315\n",
            "5.0684811401367185\n",
            "5.056614923477173\n",
            "5.045969820022583\n",
            "5.08762113571167\n",
            "5.027419939041137\n",
            "5.0901854133605955\n",
            "5.044424247741699\n",
            "30000\n",
            "5.049526166915894\n",
            "5.004517412185669\n",
            "4.955136051177979\n",
            "5.002580785751343\n",
            "5.0494898986816406\n",
            "4.981647300720215\n",
            "4.98330062866211\n",
            "4.991925888061523\n",
            "4.958205137252808\n",
            "4.938408889770508\n",
            "4.910023212432861\n",
            "4.929261121749878\n",
            "4.878863925933838\n",
            "4.9316307735443115\n",
            "4.923877639770508\n",
            "4.852610092163086\n",
            "4.916118478775024\n",
            "4.898449478149414\n",
            "4.863045711517334\n",
            "4.857759838104248\n",
            "40000\n",
            "4.829323186874389\n",
            "4.827401490211487\n",
            "4.851513881683349\n",
            "4.802932929992676\n",
            "4.784843196868897\n",
            "4.799424076080323\n",
            "4.791767988204956\n",
            "4.799296402931214\n",
            "4.8331614589691165\n",
            "4.736805610656738\n",
            "4.767608671188355\n",
            "4.703789157867432\n",
            "4.743080024719238\n",
            "4.707115545272827\n",
            "4.773388261795044\n",
            "4.746646747589112\n",
            "4.724537739753723\n",
            "4.72226845741272\n",
            "4.730736331939697\n",
            "4.739582595825195\n",
            "50000\n",
            "2745.4995715618134\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uewUHd107mIp"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50zzenEPjkK3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "255ec444-8b2f-4676-9026-75f28b664d7e"
      },
      "source": [
        "start_time = time.time()\n",
        "score = bleu(transformer, dev_lines_de[:500], dev_lines_en[:500], german_vocab, english_vocab, de_tokenizer)\n",
        "end_time = time.time()\n",
        "print(score)\n",
        "print(end_time-start_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.17876482114663714\n",
            "437.6989495754242\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQDYIHXheSPu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510,
          "referenced_widgets": [
            "db2a07c68fc04fad91cbdc687679f389",
            "0dadac2e91c3428686184168af15f332",
            "e2c51b8f9a17445fa516247666736ab0",
            "8659b3fadef14af285ccafcbe8faa994",
            "1b65c4ef734c48b09736ab86c693f0ba",
            "8b8a2a430ae84e9aa5e5d8b7c568c678",
            "dd4cb660fb044b7c93e999a0bcb38aa3",
            "adaefc728d6048148359899d971b5be6"
          ]
        },
        "outputId": "08306739-9565-4bb4-86bd-82cd41763692"
      },
      "source": [
        "wandb.finish()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 434<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db2a07c68fc04fad91cbdc687679f389",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210728_082530-1jknso45/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210728_082530-1jknso45/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>train_loss</td><td>3.36155</td></tr><tr><td>_runtime</td><td>2653</td></tr><tr><td>_timestamp</td><td>1627463384</td></tr><tr><td>_step</td><td>1101</td></tr><tr><td>val_loss</td><td>4.66185</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>train_loss</td><td>█▇▅▅▅▄▄▄▃▄▃▄▃▄▃▄▃▃▂▃▂▂▃▃▃▃▂▂▁▃▃▂▂▂▂▂▂▂▂▁</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">cool-breeze-101</strong>: <a href=\"https://wandb.ai/demegire/transformer/runs/1jknso45\" target=\"_blank\">https://wandb.ai/demegire/transformer/runs/1jknso45</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}