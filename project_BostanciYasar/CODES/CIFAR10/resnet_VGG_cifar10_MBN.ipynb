{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"resnet_VGG_cifar10_MBN.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"240eGBNsHFH7","executionInfo":{"status":"ok","timestamp":1627421350655,"user_tz":-180,"elapsed":412,"user":{"displayName":"CENG 501","photoUrl":"","userId":"15156543895114676389"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.backends.cudnn as cudnn\n","import numpy as np\n","import torchvision\n","import torchvision.transforms as transforms\n","import sys\n","\n","import matplotlib.pyplot as plt \n","import time   \n","\n","import os\n","import argparse\n","\n","\n"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cg9oDp-IEwZD"},"source":["### 1.1 Enable GPU\n","\n","From \"Edit -> Notebook Settings -> Hardware accelerator\" select GPU. With the following we will specify to PyTorch that we want to use the GPU."]},{"cell_type":"code","metadata":{"id":"0tNnBZeMvvoZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627421351129,"user_tz":-180,"elapsed":16,"user":{"displayName":"CENG 501","photoUrl":"","userId":"15156543895114676389"}},"outputId":"5e83950d-199c-4cad-b166-c970b28514fc"},"source":["if torch.cuda.is_available():\n","  print(\"Cuda (GPU support) is available and enabled!\")\n","  device = torch.device(\"cuda\")\n","else:\n","  print(\"Cuda (GPU support) is not available :(\")\n","  device = torch.device(\"cpu\")"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Cuda (GPU support) is available and enabled!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"V_h9IIGnFb-k"},"source":["### 2 Model Definition\n","\n","\n","In the original paper there are two different frameworks; ResNet and VGG. In this section, different models for different frameworks are created. However, in the original paper different normalization methods are applied in these frameworks and as mentioned the authors of the paper propose a new method which is called momentum Batch Normalization. In ResNet and VGG frameworks a naive batch normalization block is used and modified according to the proposed Momentum Batch Normalization method. On the other hand, Pytorch functions are used directly for other normalization methods such as Group Normalization, Layer Normalization, Instance Normalization and Batch Normalization for ResNet. "]},{"cell_type":"markdown","metadata":{"id":"Ltsl0oRWgpCX"},"source":["### 2.1 Momentum Batch Normalization\n","\n","In the original paper a novel method is proposed called Momentum Batch Normalization. In this section, naive batch normalization implementation is taken from Dive Into Deep Learning book and it is modified according to the original paper. \n","\n"]},{"cell_type":"code","metadata":{"id":"micNIpitgmhe","executionInfo":{"status":"ok","timestamp":1627421351131,"user_tz":-180,"elapsed":12,"user":{"displayName":"CENG 501","photoUrl":"","userId":"15156543895114676389"}}},"source":["def batch_norm(X, gamma, beta, moving_mean, moving_var, moving_mean_inf, moving_var_inf, momentum, momentum_inf, eps):\n","    # Use `is_grad_enabled` to determine whether the current mode is training\n","    # mode or prediction mode\n","    if not torch.is_grad_enabled():\n","        # If it is prediction mode, directly use the mean and variance\n","        # obtained by moving average\n","        X_hat = (X - moving_mean_inf) / torch.sqrt(moving_var_inf + eps)\n","        Y = gamma * X_hat + beta\n","    else:\n","        assert len(X.shape) in (2, 4)\n","        if len(X.shape) == 2:\n","            # When using a fully-connected layer, calculate the mean and\n","            # variance on the feature dimension\n","            mean = X.mean(dim=0)\n","            var = ((X - mean)**2).mean(dim=0)\n","        else:\n","            # When using a two-dimensional convolutional layer, calculate the\n","            # mean and variance on the channel dimension (axis=1). Here we\n","            # need to maintain the shape of `X`, so that the broadcasting\n","            # operation can be carried out later\n","            mean = X.mean(dim=(0, 2, 3), keepdim=True)\n","            var = ((X - mean)**2).mean(dim=(0, 2, 3), keepdim=True)\n","\n","        \n","        # Update the mean and variance using moving average\n","        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n","        moving_var = momentum * moving_var + (1.0 - momentum) * var\n","        # In training mode, the updated moving mean and updated moving variance are used for the\n","        # standardization\n","        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n","        Y = gamma * X_hat + beta  # Scale and shift\n","        moving_mean_inf = momentum_inf * moving_mean_inf + (1.0 - momentum_inf) * mean\n","        moving_var_inf = momentum_inf * moving_var_inf + (1.0 - momentum_inf) * var \n","    return Y, moving_mean.data, moving_var.data, moving_mean_inf.data, moving_var_inf.data\n","\n","class BatchNorm(nn.Module):\n","    # `num_features`: the number of outputs for a fully-connected layer\n","    # or the number of output channels for a convolutional layer. `num_dims`:\n","    # 2 for a fully-connected layer and 4 for a convolutional layer\n","    def __init__(self, num_features, num_dims, batch_size):\n","        super().__init__()\n","        #num_features=64\n","        if num_dims == 2:\n","            shape = (1, num_features)\n","        else:\n","            shape = (1, num_features, 1, 1)\n","        # The scale parameter and the shift parameter (model parameters) are\n","        # initialized to 1 and 0, respectively\n","        self.gamma = nn.Parameter(torch.ones(shape))\n","        self.beta = nn.Parameter(torch.zeros(shape))\n","        # The variables that are not model parameters are initialized to 0 and 1\n","        self.moving_mean = torch.zeros(shape)\n","        self.moving_var = torch.ones(shape)\n","        self.moving_mean_inf = torch.zeros(shape)\n","        self.moving_var_inf = torch.ones(shape)\n","        self.batch_size = batch_size\n","        self.momentum_inf = 0.85**(batch_size/4 / 32)\n","\n","    def forward(self, x):\n","        # If `X` is not on the main memory, copy `moving_mean` and\n","        # `moving_var` to the device where `X` is located\n","        if self.moving_mean.device != x.device:\n","            self.moving_mean = self.moving_mean.to(x.device)\n","            self.moving_var = self.moving_var.to(x.device)\n","            self.moving_mean_inf = self.moving_mean_inf.to(x.device)\n","            self.moving_var_inf = self.moving_var_inf.to(x.device)\n","        # Save the updated `moving_mean` and `moving_var`\n","        ro = min(self.batch_size / 32 , 1)**(1 / epoch_number)\n","        momentum = ro**((epoch_number)/(epoch_number - 1)*max((epoch_number - epoch), 0))- ro**epoch_number\n","        momentum=max(0,momentum)\n","        #print('Momentum:',momentum)\n","        Y, self.moving_mean, self.moving_var, self.moving_mean_inf, self.moving_var_inf = batch_norm(\n","            x, self.gamma, self.beta, self.moving_mean, self.moving_var, \n","            self.moving_mean_inf, self.moving_var_inf, momentum, self.momentum_inf,\n","            eps=1e-5)\n","        return Y"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1DBPPzmN_PEQ"},"source":["### 2.2 ResNet\n","\n","ResNet18, ResNet34 and ResNet50 architectures are defined according to the original paper as folllows."]},{"cell_type":"code","metadata":{"id":"bH1akIYwtvKE","executionInfo":{"status":"ok","timestamp":1627421352378,"user_tz":-180,"elapsed":1257,"user":{"displayName":"CENG 501","photoUrl":"","userId":"15156543895114676389"}}},"source":["class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, batch_size, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(\n","            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = BatchNorm(planes, 4, batch_size)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n","                               stride=1, padding=1, bias=False)\n","        self.bn2 = BatchNorm(planes, 4, batch_size)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes,\n","                          kernel_size=1, stride=stride, bias=False),\n","                BatchNorm(self.expansion*planes, 4, batch_size)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, batch_size, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = BatchNorm(planes, 4, batch_size)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n","                               stride=stride, padding=1, bias=False)\n","        self.bn2 = BatchNorm(planes, 4, batch_size)\n","        self.conv3 = nn.Conv2d(planes, self.expansion *\n","                               planes, kernel_size=1, bias=False)\n","        self.bn3 = BatchNorm(self.expansion*planes, 4, batch_size)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes,\n","                          kernel_size=1, stride=stride, bias=False),\n","                BatchNorm(self.expansion*planes, 4, batch_size)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, batch_size, num_classes=100):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","        self.batch_size = batch_size\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n","                               stride=1, padding=1, bias=False)\n","        self.bn1 = BatchNorm(64, 4, batch_size)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, self.batch_size, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = F.avg_pool2d(out, 4)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18(batch_size):\n","    return ResNet(BasicBlock, [2, 2, 2, 2], batch_size)\n","\n","\n","def ResNet34(batch_size):\n","    return ResNet(BasicBlock, [3, 4, 6, 3], batch_size)\n","\n","\n","def ResNet50(batch_size):\n","    return ResNet(Bottleneck, [3, 4, 6, 3], batch_size)\n"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bjIf8a4L_zwT"},"source":["### 2.3 VGG\n","\n","VGG11 and VGG16 architectures are defined according to the original paper as folllows.\n"]},{"cell_type":"code","metadata":{"id":"DikohAmZGQtx","executionInfo":{"status":"ok","timestamp":1627421352387,"user_tz":-180,"elapsed":103,"user":{"displayName":"CENG 501","photoUrl":"","userId":"15156543895114676389"}}},"source":["configuration = {\n","    'VGG11': [64, 'pool', 128, 'pool', 256, 256, 'pool', 512, 512, 'pool', 512, 512, 'pool'],\n","    'VGG16': [64, 64, 'pool', 128, 128, 'pool', 256, 256, 256, 'pool', 512, 512, 512, 'pool', 512, 512, 512, 'pool']\n","}\n","\n","\n","class VGG(nn.Module):\n","    def __init__(self, vgg_name,batch_size):\n","        super(VGG, self).__init__()\n","        self.features = self._make_layers(cfg[vgg_name])\n","        self.classifier = nn.Linear(512, 100)\n","\n","    def forward(self, x):\n","        out = self.features(x)\n","        out = out.view(out.size(0), -1)\n","        out = self.classifier(out)\n","        return out\n","\n","    def _make_layers(self, cfg):\n","        layers = []\n","        in_channels = 3\n","        for x in cfg:\n","            if x == 'pool':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n","                           BatchNorm(x, 4, batch_size),\n","                           nn.ReLU(inplace=True)]\n","                in_channels = x\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n","        return nn.Sequential(*layers)\n"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Bp01CBRE91M"},"source":["## 3 The Dataset\n","\n","In the original paper authors used CIFAR10 and CIFAR100 datasets. In this section torchvision function is used to load train/test datasets;  \n","\n","*   There are 50000 training images and 10000 test images for  10 different classes for CIFAR10\n","*   There are 50000 training images and 10000 test images for 100 different classes for CIFAR100.\n","\n","In the original paper, authors mentions standard data augmentation and preprocessing techniques are used but any specific parameters were not given in the paper or in the supplementary material of the paper. Therefore, data augmentation and preprocessing techniques are taken from the literature and applied to the datasets. \n","\n","In the original paper batch size is set to 8, 16 and 32. It can be changed in this section for different experiments.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2onI3bMcLy8e","executionInfo":{"status":"ok","timestamp":1627421354085,"user_tz":-180,"elapsed":1790,"user":{"displayName":"CENG 501","photoUrl":"","userId":"15156543895114676389"}},"outputId":"f6c84a44-ac1b-49ef-dbf5-ac8f2c348762"},"source":["# Data\n","print('==> Preparing data..')\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0, 0, 0), (1, 1, 1)),\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0, 0, 0), (1, 1, 1)),\n","])\n","\n","###################\n","batch_size=32 #batch size (2 per gpu)\n","m0=32 #ideal batch size\n","######################\n","\n","\n","trainset = torchvision.datasets.CIFAR10(\n","    root='./data', train=True, download=True, transform=transform_train)\n","trainloader = torch.utils.data.DataLoader(\n","    trainset, batch_size=batch_size, shuffle=True, num_workers=2) # 4 gpu\n","\n","testset = torchvision.datasets.CIFAR10(\n","    root='./data', train=False, download=True, transform=transform_test)\n","testloader = torch.utils.data.DataLoader(\n","    testset, batch_size=batch_size, shuffle=False, num_workers=2)\n"],"execution_count":15,"outputs":[{"output_type":"stream","text":["==> Preparing data..\n","Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BPbSc-a2Aeva"},"source":["### 4 Define and Train Model\n","In this section user can choose the model between the ResNet18, ResNet34, ResNet50, VGG11 and VGG16 frameworks. "]},{"cell_type":"markdown","metadata":{"id":"YfC6tcf1igI_"},"source":["### 4.1 Define Loss Function and the Optimizer\n","\n","Instance is created for the model based on the choice.\n","\n","In the original paper authors used Stochastic Gradient Descent for the optimization with the momentum 0.9 and weight decay 0.0001 paramaeters set. In the experiments these parameters and optimization method from the paper are preserved.\n","\n","Since the criterion for the loss function is not specified exactly in the original paper Cross-Entropy Loss method is chosen in the experiments.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"twExnQMrR3Hb","executionInfo":{"status":"ok","timestamp":1627421354091,"user_tz":-180,"elapsed":137,"user":{"displayName":"CENG 501","photoUrl":"","userId":"15156543895114676389"}},"outputId":"070289f0-3268-441a-adcf-21074e456170"},"source":["sys.argv=['']\n","del sys\n","\n","\n","best_acc = 0  # best test accuracy\n","start_epoch = 1  \n","\n","\n","# Model\n","print('==> Building model..')\n","\n","#net = VGG('VGG11',batch_size)\n","# net = VGG('VGG16',batch_size)\n","net = ResNet18(trainloader.batch_size)\n","# net = ResNet34(trainloader.batch_size)\n","# net = ResNet50(trainloader.batch_size)\n","\n","net = net.to(device)\n","if device == 'cuda':\n","    net = torch.nn.DataParallel(net)\n","    cudnn.benchmark = True\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","optimizer = optim.SGD(net.parameters(), lr=0.1*batch_size/64,\n","                      momentum=0.9, weight_decay=1e-4)\n","scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[60, 120, 180], gamma=0.1)\n","\n"],"execution_count":16,"outputs":[{"output_type":"stream","text":["==> Building model..\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1JlyKsFGiCZl"},"source":["### 4.2 Train the Model\n","\n","In the original paper, authors propose a new method in order to train large datasets with small batch-sizes for insufficient memory resources. Therefore, in the experiments considerably smaller batch sizes are used and it extends the training times. In order to overcome high training times problem GPU usage becomes a necessity and cuda is used in the experiments with Colab Pro.\n","\n"]},{"cell_type":"code","metadata":{"id":"RmmPWjZ3KaH0","executionInfo":{"status":"ok","timestamp":1627421354095,"user_tz":-180,"elapsed":116,"user":{"displayName":"CENG 501","photoUrl":"","userId":"15156543895114676389"}}},"source":["# Training and Accuracy Calculation\n","def train(verbose=True):\n","    \n","    loss_history=[]\n","    net.train()\n","    correct = 0\n","    total = 0\n","    #for epoch in range(start_epoch, start_epoch+epoch_number):\n","    \n","    \n","    start = time.time()\n","    for batch_idx, (inputs, targets) in enumerate(trainloader):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        \n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","\n","        loss_history.append(loss.item()) \n","\n","    end = time.time()\n","    if verbose: print(f'Epoch {epoch} / {epoch_number}: avg. loss of last 5 iterations {np.sum(loss_history[:-6:-1])/5} , Time : {end - start} seconds')            \n","\n","    global best_acc\n","    global test_loss\n","    net.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(testloader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = net(inputs)  \n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()       \n","\n","\n","    acc = 100.*correct/total\n","    historyy.append(acc)\n","\n","    print('acc:',acc,'best_acc:',best_acc)\n","    if acc > best_acc:\n","        print('Saving..' )\n","        \n","        state = {\n","            'net': net.state_dict(),\n","            'acc': acc,\n","            'epoch': epoch,\n","        }\n","        best_acc = acc\n","        \n","    print('\\n\\n') \n","    scheduler.step() \n","    return loss_history      "],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hu5lXdfLt-hQ","executionInfo":{"status":"ok","timestamp":1627435233721,"user_tz":-180,"elapsed":13879729,"user":{"displayName":"CENG 501","photoUrl":"","userId":"15156543895114676389"}},"outputId":"ccfdfd3c-5114-42e6-ad76-1bae13d318d1"},"source":["epoch_number=200\n","historyy=[]\n","losses=[]\n","\n","for epoch in range(start_epoch, start_epoch+epoch_number):\n","  loss_history=train()\n","\n","\n"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Epoch 1 / 200: avg. loss of last 5 iterations 1.5970205307006835 , Time : 66.03467345237732 seconds\n","acc: 39.12 best_acc: 0\n","Saving..\n","\n","\n","\n","Epoch 2 / 200: avg. loss of last 5 iterations 0.9433666586875915 , Time : 64.85452008247375 seconds\n","acc: 58.6 best_acc: 39.12\n","Saving..\n","\n","\n","\n","Epoch 3 / 200: avg. loss of last 5 iterations 0.8340570092201233 , Time : 65.82135224342346 seconds\n","acc: 69.17 best_acc: 58.6\n","Saving..\n","\n","\n","\n","Epoch 4 / 200: avg. loss of last 5 iterations 0.6230902194976806 , Time : 65.51446199417114 seconds\n","acc: 75.78 best_acc: 69.17\n","Saving..\n","\n","\n","\n","Epoch 5 / 200: avg. loss of last 5 iterations 0.6156484365463257 , Time : 65.71468329429626 seconds\n","acc: 79.46 best_acc: 75.78\n","Saving..\n","\n","\n","\n","Epoch 6 / 200: avg. loss of last 5 iterations 0.3764741599559784 , Time : 65.38649678230286 seconds\n","acc: 77.29 best_acc: 79.46\n","\n","\n","\n","Epoch 7 / 200: avg. loss of last 5 iterations 0.5662323117256165 , Time : 66.35640931129456 seconds\n","acc: 78.87 best_acc: 79.46\n","\n","\n","\n","Epoch 8 / 200: avg. loss of last 5 iterations 0.5443852841854095 , Time : 65.56661367416382 seconds\n","acc: 77.25 best_acc: 79.46\n","\n","\n","\n","Epoch 9 / 200: avg. loss of last 5 iterations 0.34721956849098207 , Time : 64.95797848701477 seconds\n","acc: 80.47 best_acc: 79.46\n","Saving..\n","\n","\n","\n","Epoch 10 / 200: avg. loss of last 5 iterations 0.365020227432251 , Time : 65.22622990608215 seconds\n","acc: 85.18 best_acc: 80.47\n","Saving..\n","\n","\n","\n","Epoch 11 / 200: avg. loss of last 5 iterations 0.35520285069942475 , Time : 67.02961134910583 seconds\n","acc: 85.41 best_acc: 85.18\n","Saving..\n","\n","\n","\n","Epoch 12 / 200: avg. loss of last 5 iterations 0.2974256291985512 , Time : 66.12046122550964 seconds\n","acc: 85.38 best_acc: 85.41\n","\n","\n","\n","Epoch 13 / 200: avg. loss of last 5 iterations 0.35461140871047975 , Time : 65.01029777526855 seconds\n","acc: 82.81 best_acc: 85.41\n","\n","\n","\n","Epoch 14 / 200: avg. loss of last 5 iterations 0.32423798441886903 , Time : 65.57871532440186 seconds\n","acc: 83.93 best_acc: 85.41\n","\n","\n","\n","Epoch 15 / 200: avg. loss of last 5 iterations 0.4543773829936981 , Time : 64.92163586616516 seconds\n","acc: 84.28 best_acc: 85.41\n","\n","\n","\n","Epoch 16 / 200: avg. loss of last 5 iterations 0.2561647042632103 , Time : 65.88653469085693 seconds\n","acc: 84.48 best_acc: 85.41\n","\n","\n","\n","Epoch 17 / 200: avg. loss of last 5 iterations 0.293391627073288 , Time : 65.53995656967163 seconds\n","acc: 86.74 best_acc: 85.41\n","Saving..\n","\n","\n","\n","Epoch 18 / 200: avg. loss of last 5 iterations 0.3812149927020073 , Time : 65.70624899864197 seconds\n","acc: 87.96 best_acc: 86.74\n","Saving..\n","\n","\n","\n","Epoch 19 / 200: avg. loss of last 5 iterations 0.4540005445480347 , Time : 65.54691553115845 seconds\n","acc: 82.46 best_acc: 87.96\n","\n","\n","\n","Epoch 20 / 200: avg. loss of last 5 iterations 0.24149881005287172 , Time : 66.54865193367004 seconds\n","acc: 80.3 best_acc: 87.96\n","\n","\n","\n","Epoch 21 / 200: avg. loss of last 5 iterations 0.2705673396587372 , Time : 65.61983394622803 seconds\n","acc: 86.92 best_acc: 87.96\n","\n","\n","\n","Epoch 22 / 200: avg. loss of last 5 iterations 0.3209673225879669 , Time : 64.90380930900574 seconds\n","acc: 85.7 best_acc: 87.96\n","\n","\n","\n","Epoch 23 / 200: avg. loss of last 5 iterations 0.2525430493056774 , Time : 65.78094553947449 seconds\n","acc: 87.03 best_acc: 87.96\n","\n","\n","\n","Epoch 24 / 200: avg. loss of last 5 iterations 0.25415774807333946 , Time : 65.71499633789062 seconds\n","acc: 88.18 best_acc: 87.96\n","Saving..\n","\n","\n","\n","Epoch 25 / 200: avg. loss of last 5 iterations 0.24106786400079727 , Time : 66.19526100158691 seconds\n","acc: 86.28 best_acc: 88.18\n","\n","\n","\n","Epoch 26 / 200: avg. loss of last 5 iterations 0.2097479835152626 , Time : 65.570072889328 seconds\n","acc: 88.33 best_acc: 88.18\n","Saving..\n","\n","\n","\n","Epoch 27 / 200: avg. loss of last 5 iterations 0.2900217205286026 , Time : 66.10521674156189 seconds\n","acc: 88.34 best_acc: 88.33\n","Saving..\n","\n","\n","\n","Epoch 28 / 200: avg. loss of last 5 iterations 0.21061516404151917 , Time : 66.76134467124939 seconds\n","acc: 81.87 best_acc: 88.34\n","\n","\n","\n","Epoch 29 / 200: avg. loss of last 5 iterations 0.1533626064658165 , Time : 65.7829110622406 seconds\n","acc: 87.3 best_acc: 88.34\n","\n","\n","\n","Epoch 30 / 200: avg. loss of last 5 iterations 0.2521220535039902 , Time : 65.99799466133118 seconds\n","acc: 88.14 best_acc: 88.34\n","\n","\n","\n","Epoch 31 / 200: avg. loss of last 5 iterations 0.25060872733592987 , Time : 66.72294092178345 seconds\n","acc: 88.2 best_acc: 88.34\n","\n","\n","\n","Epoch 32 / 200: avg. loss of last 5 iterations 0.24467326402664186 , Time : 66.00447344779968 seconds\n","acc: 88.57 best_acc: 88.34\n","Saving..\n","\n","\n","\n","Epoch 33 / 200: avg. loss of last 5 iterations 0.2852538675069809 , Time : 66.73615789413452 seconds\n","acc: 88.18 best_acc: 88.57\n","\n","\n","\n","Epoch 34 / 200: avg. loss of last 5 iterations 0.20069778859615325 , Time : 65.42095828056335 seconds\n","acc: 88.47 best_acc: 88.57\n","\n","\n","\n","Epoch 35 / 200: avg. loss of last 5 iterations 0.2472984693944454 , Time : 65.070796251297 seconds\n","acc: 84.56 best_acc: 88.57\n","\n","\n","\n","Epoch 36 / 200: avg. loss of last 5 iterations 0.1813078850507736 , Time : 65.96299934387207 seconds\n","acc: 87.96 best_acc: 88.57\n","\n","\n","\n","Epoch 37 / 200: avg. loss of last 5 iterations 0.13802264928817748 , Time : 65.83960008621216 seconds\n","acc: 87.63 best_acc: 88.57\n","\n","\n","\n","Epoch 38 / 200: avg. loss of last 5 iterations 0.15129525512456893 , Time : 65.91694116592407 seconds\n","acc: 89.17 best_acc: 88.57\n","Saving..\n","\n","\n","\n","Epoch 39 / 200: avg. loss of last 5 iterations 0.3048669070005417 , Time : 66.12203812599182 seconds\n","acc: 87.79 best_acc: 89.17\n","\n","\n","\n","Epoch 40 / 200: avg. loss of last 5 iterations 0.3542268455028534 , Time : 66.80244851112366 seconds\n","acc: 83.68 best_acc: 89.17\n","\n","\n","\n","Epoch 41 / 200: avg. loss of last 5 iterations 0.15891928747296333 , Time : 65.36836647987366 seconds\n","acc: 88.82 best_acc: 89.17\n","\n","\n","\n","Epoch 42 / 200: avg. loss of last 5 iterations 0.22760677337646484 , Time : 65.5801522731781 seconds\n","acc: 83.94 best_acc: 89.17\n","\n","\n","\n","Epoch 43 / 200: avg. loss of last 5 iterations 0.16248276084661484 , Time : 66.11690545082092 seconds\n","acc: 89.28 best_acc: 89.17\n","Saving..\n","\n","\n","\n","Epoch 44 / 200: avg. loss of last 5 iterations 0.20532600283622743 , Time : 66.02503609657288 seconds\n","acc: 88.09 best_acc: 89.28\n","\n","\n","\n","Epoch 45 / 200: avg. loss of last 5 iterations 0.25612586736679077 , Time : 66.276522397995 seconds\n","acc: 87.98 best_acc: 89.28\n","\n","\n","\n","Epoch 46 / 200: avg. loss of last 5 iterations 0.21431265100836755 , Time : 66.86045050621033 seconds\n","acc: 88.94 best_acc: 89.28\n","\n","\n","\n","Epoch 47 / 200: avg. loss of last 5 iterations 0.15009433776140213 , Time : 64.79181814193726 seconds\n","acc: 88.79 best_acc: 89.28\n","\n","\n","\n","Epoch 48 / 200: avg. loss of last 5 iterations 0.3076238423585892 , Time : 65.47673082351685 seconds\n","acc: 89.41 best_acc: 89.28\n","Saving..\n","\n","\n","\n","Epoch 49 / 200: avg. loss of last 5 iterations 0.29962681233882904 , Time : 65.71411299705505 seconds\n","acc: 89.22 best_acc: 89.41\n","\n","\n","\n","Epoch 50 / 200: avg. loss of last 5 iterations 0.16601575464010238 , Time : 65.80282425880432 seconds\n","acc: 87.52 best_acc: 89.41\n","\n","\n","\n","Epoch 51 / 200: avg. loss of last 5 iterations 0.11387022659182548 , Time : 65.31714916229248 seconds\n","acc: 89.4 best_acc: 89.41\n","\n","\n","\n","Epoch 52 / 200: avg. loss of last 5 iterations 0.3065933436155319 , Time : 66.33976864814758 seconds\n","acc: 86.65 best_acc: 89.41\n","\n","\n","\n","Epoch 53 / 200: avg. loss of last 5 iterations 0.12388260960578919 , Time : 65.54674530029297 seconds\n","acc: 88.35 best_acc: 89.41\n","\n","\n","\n","Epoch 54 / 200: avg. loss of last 5 iterations 0.41543383300304415 , Time : 65.78238821029663 seconds\n","acc: 82.86 best_acc: 89.41\n","\n","\n","\n","Epoch 55 / 200: avg. loss of last 5 iterations 0.24316414594650268 , Time : 66.20771598815918 seconds\n","acc: 87.83 best_acc: 89.41\n","\n","\n","\n","Epoch 56 / 200: avg. loss of last 5 iterations 0.2183939754962921 , Time : 66.34189653396606 seconds\n","acc: 88.31 best_acc: 89.41\n","\n","\n","\n","Epoch 57 / 200: avg. loss of last 5 iterations 0.13000570088624955 , Time : 66.00705480575562 seconds\n","acc: 89.14 best_acc: 89.41\n","\n","\n","\n","Epoch 58 / 200: avg. loss of last 5 iterations 0.24193461388349533 , Time : 66.6316602230072 seconds\n","acc: 87.34 best_acc: 89.41\n","\n","\n","\n","Epoch 59 / 200: avg. loss of last 5 iterations 0.15113100707530974 , Time : 65.78184509277344 seconds\n","acc: 87.93 best_acc: 89.41\n","\n","\n","\n","Epoch 60 / 200: avg. loss of last 5 iterations 0.13460254818201065 , Time : 66.06776714324951 seconds\n","acc: 89.4 best_acc: 89.41\n","\n","\n","\n","Epoch 61 / 200: avg. loss of last 5 iterations 0.055657974630594256 , Time : 64.9501326084137 seconds\n","acc: 93.86 best_acc: 89.41\n","Saving..\n","\n","\n","\n","Epoch 62 / 200: avg. loss of last 5 iterations 0.09874898865818978 , Time : 65.32063937187195 seconds\n","acc: 94.07 best_acc: 93.86\n","Saving..\n","\n","\n","\n","Epoch 63 / 200: avg. loss of last 5 iterations 0.055065797711722556 , Time : 66.66845560073853 seconds\n","acc: 94.22 best_acc: 94.07\n","Saving..\n","\n","\n","\n","Epoch 64 / 200: avg. loss of last 5 iterations 0.029461401142179967 , Time : 65.00345611572266 seconds\n","acc: 94.17 best_acc: 94.22\n","\n","\n","\n","Epoch 65 / 200: avg. loss of last 5 iterations 0.015404179319739341 , Time : 64.98565673828125 seconds\n","acc: 94.39 best_acc: 94.22\n","Saving..\n","\n","\n","\n","Epoch 66 / 200: avg. loss of last 5 iterations 0.07590079754590988 , Time : 65.7919282913208 seconds\n","acc: 94.31 best_acc: 94.39\n","\n","\n","\n","Epoch 67 / 200: avg. loss of last 5 iterations 0.031117785116657616 , Time : 65.65901255607605 seconds\n","acc: 94.27 best_acc: 94.39\n","\n","\n","\n","Epoch 68 / 200: avg. loss of last 5 iterations 0.006838318170048296 , Time : 65.90712547302246 seconds\n","acc: 94.21 best_acc: 94.39\n","\n","\n","\n","Epoch 69 / 200: avg. loss of last 5 iterations 0.015380257461220025 , Time : 65.15681409835815 seconds\n","acc: 94.4 best_acc: 94.39\n","Saving..\n","\n","\n","\n","Epoch 70 / 200: avg. loss of last 5 iterations 0.02727212030440569 , Time : 64.91348576545715 seconds\n","acc: 94.32 best_acc: 94.4\n","\n","\n","\n","Epoch 71 / 200: avg. loss of last 5 iterations 0.026432619418483228 , Time : 66.014488697052 seconds\n","acc: 94.29 best_acc: 94.4\n","\n","\n","\n","Epoch 72 / 200: avg. loss of last 5 iterations 0.010295288055203855 , Time : 65.91655802726746 seconds\n","acc: 94.34 best_acc: 94.4\n","\n","\n","\n","Epoch 73 / 200: avg. loss of last 5 iterations 0.0117040840908885 , Time : 65.77235865592957 seconds\n","acc: 94.31 best_acc: 94.4\n","\n","\n","\n","Epoch 74 / 200: avg. loss of last 5 iterations 0.0018093131424393506 , Time : 66.77264499664307 seconds\n","acc: 94.23 best_acc: 94.4\n","\n","\n","\n","Epoch 75 / 200: avg. loss of last 5 iterations 0.01376742220018059 , Time : 66.66454195976257 seconds\n","acc: 94.35 best_acc: 94.4\n","\n","\n","\n","Epoch 76 / 200: avg. loss of last 5 iterations 0.02533875178778544 , Time : 66.16099214553833 seconds\n","acc: 94.44 best_acc: 94.4\n","Saving..\n","\n","\n","\n","Epoch 77 / 200: avg. loss of last 5 iterations 0.004861563222948462 , Time : 66.42451024055481 seconds\n","acc: 94.19 best_acc: 94.44\n","\n","\n","\n","Epoch 78 / 200: avg. loss of last 5 iterations 0.0032954719848930837 , Time : 65.71949362754822 seconds\n","acc: 94.47 best_acc: 94.44\n","Saving..\n","\n","\n","\n","Epoch 79 / 200: avg. loss of last 5 iterations 0.013641822268255055 , Time : 65.3296582698822 seconds\n","acc: 94.47 best_acc: 94.47\n","\n","\n","\n","Epoch 80 / 200: avg. loss of last 5 iterations 0.024666427960619332 , Time : 66.34729838371277 seconds\n","acc: 94.43 best_acc: 94.47\n","\n","\n","\n","Epoch 81 / 200: avg. loss of last 5 iterations 0.018563847336918116 , Time : 65.7269926071167 seconds\n","acc: 94.29 best_acc: 94.47\n","\n","\n","\n","Epoch 82 / 200: avg. loss of last 5 iterations 0.017349698534235358 , Time : 65.56864953041077 seconds\n","acc: 94.53 best_acc: 94.47\n","Saving..\n","\n","\n","\n","Epoch 83 / 200: avg. loss of last 5 iterations 0.025711645318369846 , Time : 64.80703926086426 seconds\n","acc: 94.48 best_acc: 94.53\n","\n","\n","\n","Epoch 84 / 200: avg. loss of last 5 iterations 0.004562692368926946 , Time : 66.48607134819031 seconds\n","acc: 94.59 best_acc: 94.53\n","Saving..\n","\n","\n","\n","Epoch 85 / 200: avg. loss of last 5 iterations 0.008547730231657624 , Time : 66.40759205818176 seconds\n","acc: 94.29 best_acc: 94.59\n","\n","\n","\n","Epoch 86 / 200: avg. loss of last 5 iterations 0.007627955259522423 , Time : 65.97271180152893 seconds\n","acc: 94.48 best_acc: 94.59\n","\n","\n","\n","Epoch 87 / 200: avg. loss of last 5 iterations 0.004577431501820684 , Time : 65.64569282531738 seconds\n","acc: 94.51 best_acc: 94.59\n","\n","\n","\n","Epoch 88 / 200: avg. loss of last 5 iterations 0.01316778256878024 , Time : 66.00462627410889 seconds\n","acc: 94.7 best_acc: 94.59\n","Saving..\n","\n","\n","\n","Epoch 89 / 200: avg. loss of last 5 iterations 0.0026818689453648402 , Time : 64.79346108436584 seconds\n","acc: 94.82 best_acc: 94.7\n","Saving..\n","\n","\n","\n","Epoch 90 / 200: avg. loss of last 5 iterations 0.0039822273363824936 , Time : 65.13435578346252 seconds\n","acc: 94.61 best_acc: 94.82\n","\n","\n","\n","Epoch 91 / 200: avg. loss of last 5 iterations 0.012410391961748247 , Time : 65.70609211921692 seconds\n","acc: 94.79 best_acc: 94.82\n","\n","\n","\n","Epoch 92 / 200: avg. loss of last 5 iterations 0.0006742251018295065 , Time : 65.65061283111572 seconds\n","acc: 94.74 best_acc: 94.82\n","\n","\n","\n","Epoch 93 / 200: avg. loss of last 5 iterations 0.0010489008214790373 , Time : 66.2917902469635 seconds\n","acc: 94.52 best_acc: 94.82\n","\n","\n","\n","Epoch 94 / 200: avg. loss of last 5 iterations 0.004507140052737668 , Time : 65.28757834434509 seconds\n","acc: 94.78 best_acc: 94.82\n","\n","\n","\n","Epoch 95 / 200: avg. loss of last 5 iterations 0.01302170631242916 , Time : 65.39415884017944 seconds\n","acc: 94.7 best_acc: 94.82\n","\n","\n","\n","Epoch 96 / 200: avg. loss of last 5 iterations 0.0013229042757302522 , Time : 66.96478867530823 seconds\n","acc: 94.67 best_acc: 94.82\n","\n","\n","\n","Epoch 97 / 200: avg. loss of last 5 iterations 0.02146228412457276 , Time : 65.43991708755493 seconds\n","acc: 94.84 best_acc: 94.82\n","Saving..\n","\n","\n","\n","Epoch 98 / 200: avg. loss of last 5 iterations 0.006639440823346376 , Time : 66.18797063827515 seconds\n","acc: 94.5 best_acc: 94.84\n","\n","\n","\n","Epoch 99 / 200: avg. loss of last 5 iterations 0.001705384673550725 , Time : 65.39672040939331 seconds\n","acc: 94.62 best_acc: 94.84\n","\n","\n","\n","Epoch 100 / 200: avg. loss of last 5 iterations 0.028100537066347898 , Time : 65.8323061466217 seconds\n","acc: 94.81 best_acc: 94.84\n","\n","\n","\n","Epoch 101 / 200: avg. loss of last 5 iterations 0.0011377476213965565 , Time : 67.08931159973145 seconds\n","acc: 94.64 best_acc: 94.84\n","\n","\n","\n","Epoch 102 / 200: avg. loss of last 5 iterations 0.00916221252118703 , Time : 66.50365614891052 seconds\n","acc: 94.47 best_acc: 94.84\n","\n","\n","\n","Epoch 103 / 200: avg. loss of last 5 iterations 0.01011431603692472 , Time : 65.97977948188782 seconds\n","acc: 94.54 best_acc: 94.84\n","\n","\n","\n","Epoch 104 / 200: avg. loss of last 5 iterations 0.004029299080139026 , Time : 65.82638931274414 seconds\n","acc: 94.78 best_acc: 94.84\n","\n","\n","\n","Epoch 105 / 200: avg. loss of last 5 iterations 0.002597237698500976 , Time : 66.83000612258911 seconds\n","acc: 94.79 best_acc: 94.84\n","\n","\n","\n","Epoch 106 / 200: avg. loss of last 5 iterations 0.012950254557654261 , Time : 65.56611657142639 seconds\n","acc: 94.52 best_acc: 94.84\n","\n","\n","\n","Epoch 107 / 200: avg. loss of last 5 iterations 0.005701918306294828 , Time : 65.685467004776 seconds\n","acc: 94.81 best_acc: 94.84\n","\n","\n","\n","Epoch 108 / 200: avg. loss of last 5 iterations 0.0007817824953235686 , Time : 65.31524991989136 seconds\n","acc: 94.19 best_acc: 94.84\n","\n","\n","\n","Epoch 109 / 200: avg. loss of last 5 iterations 0.0072786538919899614 , Time : 65.28922939300537 seconds\n","acc: 94.64 best_acc: 94.84\n","\n","\n","\n","Epoch 110 / 200: avg. loss of last 5 iterations 0.03386450345715275 , Time : 65.67950105667114 seconds\n","acc: 94.5 best_acc: 94.84\n","\n","\n","\n","Epoch 111 / 200: avg. loss of last 5 iterations 0.0033150014467537405 , Time : 65.00179481506348 seconds\n","acc: 94.55 best_acc: 94.84\n","\n","\n","\n","Epoch 112 / 200: avg. loss of last 5 iterations 0.006460215453989804 , Time : 65.61491870880127 seconds\n","acc: 94.42 best_acc: 94.84\n","\n","\n","\n","Epoch 113 / 200: avg. loss of last 5 iterations 0.011531208039377816 , Time : 66.10803389549255 seconds\n","acc: 94.41 best_acc: 94.84\n","\n","\n","\n","Epoch 114 / 200: avg. loss of last 5 iterations 0.0015914236486423762 , Time : 65.44179010391235 seconds\n","acc: 94.32 best_acc: 94.84\n","\n","\n","\n","Epoch 115 / 200: avg. loss of last 5 iterations 0.023158381541725247 , Time : 66.07660436630249 seconds\n","acc: 94.35 best_acc: 94.84\n","\n","\n","\n","Epoch 116 / 200: avg. loss of last 5 iterations 0.0012837753049097956 , Time : 65.78381896018982 seconds\n","acc: 94.4 best_acc: 94.84\n","\n","\n","\n","Epoch 117 / 200: avg. loss of last 5 iterations 0.0015815488121006637 , Time : 65.13665294647217 seconds\n","acc: 94.61 best_acc: 94.84\n","\n","\n","\n","Epoch 118 / 200: avg. loss of last 5 iterations 0.003070372808724642 , Time : 64.85762023925781 seconds\n","acc: 94.66 best_acc: 94.84\n","\n","\n","\n","Epoch 119 / 200: avg. loss of last 5 iterations 0.033770955703221264 , Time : 65.73593926429749 seconds\n","acc: 94.41 best_acc: 94.84\n","\n","\n","\n","Epoch 120 / 200: avg. loss of last 5 iterations 0.006141146554728039 , Time : 65.51850938796997 seconds\n","acc: 94.57 best_acc: 94.84\n","\n","\n","\n","Epoch 121 / 200: avg. loss of last 5 iterations 0.0014567447535227985 , Time : 65.76232886314392 seconds\n","acc: 94.8 best_acc: 94.84\n","\n","\n","\n","Epoch 122 / 200: avg. loss of last 5 iterations 0.00102907440304989 , Time : 65.35377240180969 seconds\n","acc: 94.84 best_acc: 94.84\n","\n","\n","\n","Epoch 123 / 200: avg. loss of last 5 iterations 0.004066619521472603 , Time : 66.2811074256897 seconds\n","acc: 94.91 best_acc: 94.84\n","Saving..\n","\n","\n","\n","Epoch 124 / 200: avg. loss of last 5 iterations 0.0025322319997940212 , Time : 65.7011649608612 seconds\n","acc: 94.96 best_acc: 94.91\n","Saving..\n","\n","\n","\n","Epoch 125 / 200: avg. loss of last 5 iterations 0.004416984954150394 , Time : 65.69461584091187 seconds\n","acc: 95.01 best_acc: 94.96\n","Saving..\n","\n","\n","\n","Epoch 126 / 200: avg. loss of last 5 iterations 0.00228975204663584 , Time : 66.18179893493652 seconds\n","acc: 94.97 best_acc: 95.01\n","\n","\n","\n","Epoch 127 / 200: avg. loss of last 5 iterations 0.0017749566395650618 , Time : 65.77495789527893 seconds\n","acc: 94.99 best_acc: 95.01\n","\n","\n","\n","Epoch 128 / 200: avg. loss of last 5 iterations 0.0014589954676921479 , Time : 65.47020697593689 seconds\n","acc: 95.14 best_acc: 95.01\n","Saving..\n","\n","\n","\n","Epoch 129 / 200: avg. loss of last 5 iterations 0.005610033533594105 , Time : 66.20038390159607 seconds\n","acc: 95.08 best_acc: 95.14\n","\n","\n","\n","Epoch 130 / 200: avg. loss of last 5 iterations 0.0023574867431307212 , Time : 66.0158486366272 seconds\n","acc: 95.06 best_acc: 95.14\n","\n","\n","\n","Epoch 131 / 200: avg. loss of last 5 iterations 0.0008770276413997635 , Time : 65.29461693763733 seconds\n","acc: 95.11 best_acc: 95.14\n","\n","\n","\n","Epoch 132 / 200: avg. loss of last 5 iterations 0.008459076582221314 , Time : 64.56826639175415 seconds\n","acc: 95.1 best_acc: 95.14\n","\n","\n","\n","Epoch 133 / 200: avg. loss of last 5 iterations 0.0010646225739037618 , Time : 65.25144815444946 seconds\n","acc: 95.02 best_acc: 95.14\n","\n","\n","\n","Epoch 134 / 200: avg. loss of last 5 iterations 0.0003098555898759514 , Time : 66.12418985366821 seconds\n","acc: 95.05 best_acc: 95.14\n","\n","\n","\n","Epoch 135 / 200: avg. loss of last 5 iterations 0.0003663904673885554 , Time : 66.66682744026184 seconds\n","acc: 94.98 best_acc: 95.14\n","\n","\n","\n","Epoch 136 / 200: avg. loss of last 5 iterations 0.0006852241858723573 , Time : 66.12606143951416 seconds\n","acc: 94.96 best_acc: 95.14\n","\n","\n","\n","Epoch 137 / 200: avg. loss of last 5 iterations 0.0003541348211001605 , Time : 66.07489132881165 seconds\n","acc: 94.97 best_acc: 95.14\n","\n","\n","\n","Epoch 138 / 200: avg. loss of last 5 iterations 0.001989979762583971 , Time : 67.58321523666382 seconds\n","acc: 95.03 best_acc: 95.14\n","\n","\n","\n","Epoch 139 / 200: avg. loss of last 5 iterations 0.0008943258901126683 , Time : 65.75618720054626 seconds\n","acc: 95.06 best_acc: 95.14\n","\n","\n","\n","Epoch 140 / 200: avg. loss of last 5 iterations 0.007894299580948427 , Time : 66.36429357528687 seconds\n","acc: 95.06 best_acc: 95.14\n","\n","\n","\n","Epoch 141 / 200: avg. loss of last 5 iterations 0.0006551874481374398 , Time : 65.59344005584717 seconds\n","acc: 95.05 best_acc: 95.14\n","\n","\n","\n","Epoch 142 / 200: avg. loss of last 5 iterations 0.0012465301726479084 , Time : 66.30775570869446 seconds\n","acc: 95.01 best_acc: 95.14\n","\n","\n","\n","Epoch 143 / 200: avg. loss of last 5 iterations 0.002935596540919505 , Time : 65.42157411575317 seconds\n","acc: 95.02 best_acc: 95.14\n","\n","\n","\n","Epoch 144 / 200: avg. loss of last 5 iterations 0.006245419371407479 , Time : 65.81320309638977 seconds\n","acc: 95.02 best_acc: 95.14\n","\n","\n","\n","Epoch 145 / 200: avg. loss of last 5 iterations 0.0055491196282673625 , Time : 66.40740489959717 seconds\n","acc: 95.05 best_acc: 95.14\n","\n","\n","\n","Epoch 146 / 200: avg. loss of last 5 iterations 0.006298409902956337 , Time : 65.60656833648682 seconds\n","acc: 95.08 best_acc: 95.14\n","\n","\n","\n","Epoch 147 / 200: avg. loss of last 5 iterations 0.0013231705466751008 , Time : 65.81524682044983 seconds\n","acc: 95.11 best_acc: 95.14\n","\n","\n","\n","Epoch 148 / 200: avg. loss of last 5 iterations 0.0003861157500068657 , Time : 66.02159571647644 seconds\n","acc: 95.14 best_acc: 95.14\n","\n","\n","\n","Epoch 149 / 200: avg. loss of last 5 iterations 0.014124787371838466 , Time : 65.94756865501404 seconds\n","acc: 95.06 best_acc: 95.14\n","\n","\n","\n","Epoch 150 / 200: avg. loss of last 5 iterations 0.005978430708637461 , Time : 65.25847625732422 seconds\n","acc: 95.03 best_acc: 95.14\n","\n","\n","\n","Epoch 151 / 200: avg. loss of last 5 iterations 0.0016894013577257283 , Time : 65.96737408638 seconds\n","acc: 95.1 best_acc: 95.14\n","\n","\n","\n","Epoch 152 / 200: avg. loss of last 5 iterations 0.000556687725475058 , Time : 65.9879879951477 seconds\n","acc: 95.06 best_acc: 95.14\n","\n","\n","\n","Epoch 153 / 200: avg. loss of last 5 iterations 0.0009971032355679198 , Time : 66.58339023590088 seconds\n","acc: 95.16 best_acc: 95.14\n","Saving..\n","\n","\n","\n","Epoch 154 / 200: avg. loss of last 5 iterations 0.0007677990477532148 , Time : 64.76530742645264 seconds\n","acc: 95.16 best_acc: 95.16\n","\n","\n","\n","Epoch 155 / 200: avg. loss of last 5 iterations 0.0008714120922377333 , Time : 65.3561840057373 seconds\n","acc: 95.11 best_acc: 95.16\n","\n","\n","\n","Epoch 156 / 200: avg. loss of last 5 iterations 0.0004354049371613655 , Time : 65.4790575504303 seconds\n","acc: 95.12 best_acc: 95.16\n","\n","\n","\n","Epoch 157 / 200: avg. loss of last 5 iterations 0.0014548686158377677 , Time : 65.89424967765808 seconds\n","acc: 95.13 best_acc: 95.16\n","\n","\n","\n","Epoch 158 / 200: avg. loss of last 5 iterations 0.00043685173150151966 , Time : 65.66129684448242 seconds\n","acc: 95.07 best_acc: 95.16\n","\n","\n","\n","Epoch 159 / 200: avg. loss of last 5 iterations 0.0001933051593368873 , Time : 65.33580422401428 seconds\n","acc: 95.12 best_acc: 95.16\n","\n","\n","\n","Epoch 160 / 200: avg. loss of last 5 iterations 0.0008944712812080979 , Time : 65.4465434551239 seconds\n","acc: 95.13 best_acc: 95.16\n","\n","\n","\n","Epoch 161 / 200: avg. loss of last 5 iterations 0.006249687819217797 , Time : 65.68615794181824 seconds\n","acc: 95.08 best_acc: 95.16\n","\n","\n","\n","Epoch 162 / 200: avg. loss of last 5 iterations 0.0003151897981297225 , Time : 65.73633217811584 seconds\n","acc: 95.09 best_acc: 95.16\n","\n","\n","\n","Epoch 163 / 200: avg. loss of last 5 iterations 0.0010498958014068193 , Time : 66.24527192115784 seconds\n","acc: 95.04 best_acc: 95.16\n","\n","\n","\n","Epoch 164 / 200: avg. loss of last 5 iterations 0.024671974743978354 , Time : 65.56948447227478 seconds\n","acc: 95.07 best_acc: 95.16\n","\n","\n","\n","Epoch 165 / 200: avg. loss of last 5 iterations 0.0008205922960769386 , Time : 66.17075109481812 seconds\n","acc: 95.09 best_acc: 95.16\n","\n","\n","\n","Epoch 166 / 200: avg. loss of last 5 iterations 0.001319554104702547 , Time : 66.53011631965637 seconds\n","acc: 95.07 best_acc: 95.16\n","\n","\n","\n","Epoch 167 / 200: avg. loss of last 5 iterations 0.0005046244652476162 , Time : 65.47727704048157 seconds\n","acc: 95.07 best_acc: 95.16\n","\n","\n","\n","Epoch 168 / 200: avg. loss of last 5 iterations 0.0005771464144345373 , Time : 65.59678149223328 seconds\n","acc: 95.06 best_acc: 95.16\n","\n","\n","\n","Epoch 169 / 200: avg. loss of last 5 iterations 0.0030654121190309526 , Time : 66.50702929496765 seconds\n","acc: 94.97 best_acc: 95.16\n","\n","\n","\n","Epoch 170 / 200: avg. loss of last 5 iterations 0.0013232036697445437 , Time : 65.45773935317993 seconds\n","acc: 95.08 best_acc: 95.16\n","\n","\n","\n","Epoch 171 / 200: avg. loss of last 5 iterations 0.0016532965732039885 , Time : 66.70864629745483 seconds\n","acc: 95.08 best_acc: 95.16\n","\n","\n","\n","Epoch 172 / 200: avg. loss of last 5 iterations 0.0040142666723113505 , Time : 66.18692207336426 seconds\n","acc: 95.14 best_acc: 95.16\n","\n","\n","\n","Epoch 173 / 200: avg. loss of last 5 iterations 0.001147202297579497 , Time : 66.36959338188171 seconds\n","acc: 95.05 best_acc: 95.16\n","\n","\n","\n","Epoch 174 / 200: avg. loss of last 5 iterations 0.0008190372172975913 , Time : 65.70301580429077 seconds\n","acc: 95.1 best_acc: 95.16\n","\n","\n","\n","Epoch 175 / 200: avg. loss of last 5 iterations 0.001227966177975759 , Time : 65.88223314285278 seconds\n","acc: 95.13 best_acc: 95.16\n","\n","\n","\n","Epoch 176 / 200: avg. loss of last 5 iterations 0.0008112616633297876 , Time : 66.03492379188538 seconds\n","acc: 95.08 best_acc: 95.16\n","\n","\n","\n","Epoch 177 / 200: avg. loss of last 5 iterations 0.000710436434019357 , Time : 65.33314561843872 seconds\n","acc: 95.08 best_acc: 95.16\n","\n","\n","\n","Epoch 178 / 200: avg. loss of last 5 iterations 0.0009748325916007161 , Time : 66.44085550308228 seconds\n","acc: 95.12 best_acc: 95.16\n","\n","\n","\n","Epoch 179 / 200: avg. loss of last 5 iterations 0.0008575875981478021 , Time : 66.01383972167969 seconds\n","acc: 95.08 best_acc: 95.16\n","\n","\n","\n","Epoch 180 / 200: avg. loss of last 5 iterations 0.0020063987154571804 , Time : 65.39506602287292 seconds\n","acc: 95.13 best_acc: 95.16\n","\n","\n","\n","Epoch 181 / 200: avg. loss of last 5 iterations 0.00030212199490051714 , Time : 65.70136213302612 seconds\n","acc: 95.07 best_acc: 95.16\n","\n","\n","\n","Epoch 182 / 200: avg. loss of last 5 iterations 0.0019250830402597785 , Time : 65.77319765090942 seconds\n","acc: 95.03 best_acc: 95.16\n","\n","\n","\n","Epoch 183 / 200: avg. loss of last 5 iterations 0.0047519285028101875 , Time : 65.70161819458008 seconds\n","acc: 95.01 best_acc: 95.16\n","\n","\n","\n","Epoch 184 / 200: avg. loss of last 5 iterations 0.0006141123405541294 , Time : 66.13403010368347 seconds\n","acc: 95.01 best_acc: 95.16\n","\n","\n","\n","Epoch 185 / 200: avg. loss of last 5 iterations 0.0006135586823802442 , Time : 66.2435040473938 seconds\n","acc: 94.99 best_acc: 95.16\n","\n","\n","\n","Epoch 186 / 200: avg. loss of last 5 iterations 0.0005721517110941932 , Time : 66.4099612236023 seconds\n","acc: 95.08 best_acc: 95.16\n","\n","\n","\n","Epoch 187 / 200: avg. loss of last 5 iterations 0.005189506118767895 , Time : 66.41289496421814 seconds\n","acc: 94.98 best_acc: 95.16\n","\n","\n","\n","Epoch 188 / 200: avg. loss of last 5 iterations 0.0005371125356759876 , Time : 65.69378185272217 seconds\n","acc: 94.99 best_acc: 95.16\n","\n","\n","\n","Epoch 189 / 200: avg. loss of last 5 iterations 0.00888094977417495 , Time : 65.35526823997498 seconds\n","acc: 95.02 best_acc: 95.16\n","\n","\n","\n","Epoch 190 / 200: avg. loss of last 5 iterations 0.0004687414548243396 , Time : 65.39679312705994 seconds\n","acc: 95.12 best_acc: 95.16\n","\n","\n","\n","Epoch 191 / 200: avg. loss of last 5 iterations 0.0013355754083022476 , Time : 65.3087682723999 seconds\n","acc: 94.98 best_acc: 95.16\n","\n","\n","\n","Epoch 192 / 200: avg. loss of last 5 iterations 0.0002877884166082367 , Time : 65.56378316879272 seconds\n","acc: 95.0 best_acc: 95.16\n","\n","\n","\n","Epoch 193 / 200: avg. loss of last 5 iterations 0.0006609660311369225 , Time : 66.00044178962708 seconds\n","acc: 95.05 best_acc: 95.16\n","\n","\n","\n","Epoch 194 / 200: avg. loss of last 5 iterations 0.0075779800390591845 , Time : 65.68424081802368 seconds\n","acc: 95.07 best_acc: 95.16\n","\n","\n","\n","Epoch 195 / 200: avg. loss of last 5 iterations 0.007946815941249952 , Time : 65.70781183242798 seconds\n","acc: 95.0 best_acc: 95.16\n","\n","\n","\n","Epoch 196 / 200: avg. loss of last 5 iterations 0.0006458695192122832 , Time : 65.53284025192261 seconds\n","acc: 95.1 best_acc: 95.16\n","\n","\n","\n","Epoch 197 / 200: avg. loss of last 5 iterations 0.0006414833471353632 , Time : 65.35298299789429 seconds\n","acc: 95.01 best_acc: 95.16\n","\n","\n","\n","Epoch 198 / 200: avg. loss of last 5 iterations 0.0008785758003796218 , Time : 66.44208931922913 seconds\n","acc: 95.01 best_acc: 95.16\n","\n","\n","\n","Epoch 199 / 200: avg. loss of last 5 iterations 0.0009254437241906999 , Time : 65.00227284431458 seconds\n","acc: 95.09 best_acc: 95.16\n","\n","\n","\n","Epoch 200 / 200: avg. loss of last 5 iterations 0.0016737006575567649 , Time : 65.2425742149353 seconds\n","acc: 95.01 best_acc: 95.16\n","\n","\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gnaT7_kfGxie"},"source":["### 4.3 The Loss Curve\n","Loss curve is visualized."]},{"cell_type":"code","metadata":{"id":"3fbhdotYius9"},"source":["plt.plot(loss_history)\n","plt.xlabel('Iteration number')\n","plt.ylabel('Loss value')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uSvgOaIUzWCk"},"source":["### 4.4 Quantitative Analysis\n","\n","We can analyze the accuracy of the predictions as follows. "]},{"cell_type":"code","metadata":{"id":"kghpVDAly9Kf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627435237007,"user_tz":-180,"elapsed":3459,"user":{"displayName":"CENG 501","photoUrl":"","userId":"15156543895114676389"}},"outputId":"0f3983ea-2928-4a88-c0e8-4c302b656ab3"},"source":["correct = 0\n","total = 0\n","with torch.no_grad():\n","    for batch_idx, (inputs, targets) in enumerate(testloader):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        outputs = net(inputs)\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","print('Accuracy of the network on the 10000 test images: %d %%' % (\n","    100 * correct / total))  "],"execution_count":20,"outputs":[{"output_type":"stream","text":["Accuracy of the network on the 10000 test images: 95 %\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KwEmxNBOgQsD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627435237011,"user_tz":-180,"elapsed":58,"user":{"displayName":"CENG 501","photoUrl":"","userId":"15156543895114676389"}},"outputId":"74353c93-847b-4853-dd18-8bc547c3fda3"},"source":["print('saving' ,'acc:',best_acc)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["saving acc: 95.16\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"f1fPq-45BI_8"},"source":["### 5 Results\n","\n","Accuracy vs epoch is plotted according to the test results. "]},{"cell_type":"code","metadata":{"id":"j9j3BViCkt5r","colab":{"base_uri":"https://localhost:8080/","height":316},"executionInfo":{"status":"ok","timestamp":1627435237796,"user_tz":-180,"elapsed":813,"user":{"displayName":"CENG 501","photoUrl":"","userId":"15156543895114676389"}},"outputId":"629f3eae-b086-41eb-ceba-41575deab3c8"},"source":["plt.plot(historyy)\n","plt.xlabel('Epoch number')\n","plt.ylabel('Accuracy')\n","plt.show()\n","print(historyy)"],"execution_count":22,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["[39.12, 58.6, 69.17, 75.78, 79.46, 77.29, 78.87, 77.25, 80.47, 85.18, 85.41, 85.38, 82.81, 83.93, 84.28, 84.48, 86.74, 87.96, 82.46, 80.3, 86.92, 85.7, 87.03, 88.18, 86.28, 88.33, 88.34, 81.87, 87.3, 88.14, 88.2, 88.57, 88.18, 88.47, 84.56, 87.96, 87.63, 89.17, 87.79, 83.68, 88.82, 83.94, 89.28, 88.09, 87.98, 88.94, 88.79, 89.41, 89.22, 87.52, 89.4, 86.65, 88.35, 82.86, 87.83, 88.31, 89.14, 87.34, 87.93, 89.4, 93.86, 94.07, 94.22, 94.17, 94.39, 94.31, 94.27, 94.21, 94.4, 94.32, 94.29, 94.34, 94.31, 94.23, 94.35, 94.44, 94.19, 94.47, 94.47, 94.43, 94.29, 94.53, 94.48, 94.59, 94.29, 94.48, 94.51, 94.7, 94.82, 94.61, 94.79, 94.74, 94.52, 94.78, 94.7, 94.67, 94.84, 94.5, 94.62, 94.81, 94.64, 94.47, 94.54, 94.78, 94.79, 94.52, 94.81, 94.19, 94.64, 94.5, 94.55, 94.42, 94.41, 94.32, 94.35, 94.4, 94.61, 94.66, 94.41, 94.57, 94.8, 94.84, 94.91, 94.96, 95.01, 94.97, 94.99, 95.14, 95.08, 95.06, 95.11, 95.1, 95.02, 95.05, 94.98, 94.96, 94.97, 95.03, 95.06, 95.06, 95.05, 95.01, 95.02, 95.02, 95.05, 95.08, 95.11, 95.14, 95.06, 95.03, 95.1, 95.06, 95.16, 95.16, 95.11, 95.12, 95.13, 95.07, 95.12, 95.13, 95.08, 95.09, 95.04, 95.07, 95.09, 95.07, 95.07, 95.06, 94.97, 95.08, 95.08, 95.14, 95.05, 95.1, 95.13, 95.08, 95.08, 95.12, 95.08, 95.13, 95.07, 95.03, 95.01, 95.01, 94.99, 95.08, 94.98, 94.99, 95.02, 95.12, 94.98, 95.0, 95.05, 95.07, 95.0, 95.1, 95.01, 95.01, 95.09, 95.01]\n"],"name":"stdout"}]}]}