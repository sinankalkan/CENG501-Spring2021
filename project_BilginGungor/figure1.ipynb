{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 775,
     "status": "ok",
     "timestamp": 1626643190062,
     "user": {
      "displayName": "Ertuğrul Güngör",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicvIzQgyHfsqD4h979_wDnn11aI-XtLnxmnC_8=s64",
      "userId": "06165302180630202587"
     },
     "user_tz": -180
    },
    "id": "YBHMtNLN3GnS"
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class Identity(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Identity, self).__init__()\n",
    "      \n",
    "  def forward(self, x):\n",
    "    return x\n",
    "\n",
    "def get_vgg_model(num_outputs, init_scale):\n",
    "  vgg16 = models.vgg16()\n",
    "\n",
    "  # Remove avgpool since it's removed in the paper\n",
    "  vgg16.avgpool = Identity()\n",
    "\n",
    "  # In the paper, VGG16 model's 2 last FC layers are removed\n",
    "  # Output of last max pool layer is [batch_size,512,7,7] for 224x224 image\n",
    "  #vgg16.classifier = nn.Linear(512*7*7, num_outputs)\n",
    "\n",
    "  # For 32x32 image\n",
    "  vgg16.classifier = nn.Linear(512, num_outputs)\n",
    "\n",
    "  #for module in vgg16.modules():\n",
    "  #  if type(module) == nn.modules.pooling.MaxPool2d:\n",
    "  #    module = nn.MaxPool2d(2, stride=2, padding='same')\n",
    "    #if \"MaxPool2d\" is in str(type(module)):\n",
    "    #  print(\"module is : {}\".format(type(module)))\n",
    "  # Change weight initialization with a nested function\n",
    "  def init_weights(param):\n",
    "    if (type(param) == nn.Conv2d or type(param) == nn.Linear):\n",
    "      # Parameters are initialized with He initialization in the paper.\n",
    "\n",
    "      torch.nn.init.kaiming_normal_(param.weight, mode='fan_out', nonlinearity='relu')\n",
    "      # # ??\n",
    "      # param.weight = torch.nn.Parameter(param.weight * init_scale)\n",
    "\n",
    "      # print(type(param.weight))\n",
    "      # param.weight.data.fill_(param.weight * init_scale)\n",
    "      # torch.full(param.weight.size(), 3.141592)\n",
    "\n",
    "      # Default value of biases are already 0, just to be more verbose\n",
    "      # param.bias.data.fill_(0)\n",
    "\n",
    "      # print(param)\n",
    "      # print(type(param))\n",
    "\n",
    "  vgg16.apply(init_weights)\n",
    "  return vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7907,
     "status": "ok",
     "timestamp": 1626643197966,
     "user": {
      "displayName": "Ertuğrul Güngör",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicvIzQgyHfsqD4h979_wDnn11aI-XtLnxmnC_8=s64",
      "userId": "06165302180630202587"
     },
     "user_tz": -180
    },
    "id": "JZcqlcdLDtfM",
    "outputId": "7453edbd-e3ab-4b76-a8da-ed9b4634bdc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /opt/conda/include/python3.8/UNKNOWN\n",
      "sysconfig: /opt/conda/include/python3.8\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "Requirement already satisfied: torchinfo in /opt/conda/lib/python3.8/site-packages (1.5.2)\n",
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /opt/conda/include/python3.8/UNKNOWN\n",
      "sysconfig: /opt/conda/include/python3.8\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "VGG                                      --                        --\n",
      "├─Sequential: 1-1                        [1, 512, 1, 1]            --\n",
      "│    └─Conv2d: 2-1                       [1, 64, 32, 32]           1,792\n",
      "│    └─ReLU: 2-2                         [1, 64, 32, 32]           --\n",
      "│    └─Conv2d: 2-3                       [1, 64, 32, 32]           36,928\n",
      "│    └─ReLU: 2-4                         [1, 64, 32, 32]           --\n",
      "│    └─MaxPool2d: 2-5                    [1, 64, 16, 16]           --\n",
      "│    └─Conv2d: 2-6                       [1, 128, 16, 16]          73,856\n",
      "│    └─ReLU: 2-7                         [1, 128, 16, 16]          --\n",
      "│    └─Conv2d: 2-8                       [1, 128, 16, 16]          147,584\n",
      "│    └─ReLU: 2-9                         [1, 128, 16, 16]          --\n",
      "│    └─MaxPool2d: 2-10                   [1, 128, 8, 8]            --\n",
      "│    └─Conv2d: 2-11                      [1, 256, 8, 8]            295,168\n",
      "│    └─ReLU: 2-12                        [1, 256, 8, 8]            --\n",
      "│    └─Conv2d: 2-13                      [1, 256, 8, 8]            590,080\n",
      "│    └─ReLU: 2-14                        [1, 256, 8, 8]            --\n",
      "│    └─Conv2d: 2-15                      [1, 256, 8, 8]            590,080\n",
      "│    └─ReLU: 2-16                        [1, 256, 8, 8]            --\n",
      "│    └─MaxPool2d: 2-17                   [1, 256, 4, 4]            --\n",
      "│    └─Conv2d: 2-18                      [1, 512, 4, 4]            1,180,160\n",
      "│    └─ReLU: 2-19                        [1, 512, 4, 4]            --\n",
      "│    └─Conv2d: 2-20                      [1, 512, 4, 4]            2,359,808\n",
      "│    └─ReLU: 2-21                        [1, 512, 4, 4]            --\n",
      "│    └─Conv2d: 2-22                      [1, 512, 4, 4]            2,359,808\n",
      "│    └─ReLU: 2-23                        [1, 512, 4, 4]            --\n",
      "│    └─MaxPool2d: 2-24                   [1, 512, 2, 2]            --\n",
      "│    └─Conv2d: 2-25                      [1, 512, 2, 2]            2,359,808\n",
      "│    └─ReLU: 2-26                        [1, 512, 2, 2]            --\n",
      "│    └─Conv2d: 2-27                      [1, 512, 2, 2]            2,359,808\n",
      "│    └─ReLU: 2-28                        [1, 512, 2, 2]            --\n",
      "│    └─Conv2d: 2-29                      [1, 512, 2, 2]            2,359,808\n",
      "│    └─ReLU: 2-30                        [1, 512, 2, 2]            --\n",
      "│    └─MaxPool2d: 2-31                   [1, 512, 1, 1]            --\n",
      "├─Identity: 1-2                          [1, 512, 1, 1]            --\n",
      "├─Linear: 1-3                            [1, 13]                   6,669\n",
      "==========================================================================================\n",
      "Total params: 14,721,357\n",
      "Trainable params: 14,721,357\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 313.48\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 2.21\n",
      "Params size (MB): 58.89\n",
      "Estimated Total Size (MB): 61.11\n",
      "==========================================================================================\n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): Identity()\n",
      "  (classifier): Linear(in_features=512, out_features=13, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# install and import the torchinfo library\n",
    "!pip install torchinfo\n",
    "from torchinfo import summary\n",
    "\n",
    "vgg16 = get_vgg_model(13, 1)\n",
    "# batch_size = 256\n",
    "print(summary(vgg16, input_size=(1, 3, 32, 32)))\n",
    "#print(summary(vgg16, input_size=(1, 3, 224, 224)))\n",
    "\n",
    "# summary(vgg16, input_size=(batch_size, 3, 224, 224))\n",
    "# summary(models.vgg16(), input_size=(1, 3, 32, 32))\n",
    "print(vgg16)\n",
    "#vgg16 = models.vgg16()\n",
    "#print(vgg16)\n",
    "\n",
    "\n",
    "# print(vgg16)\n",
    "\n",
    "# vgg16.apply(init_weights)\n",
    "# print(\"dummy\")\n",
    "# for child in vgg16.named_children():\n",
    "#   # print(\"x\")\n",
    "#   print(child)\n",
    "\n",
    "# for name, param in vgg16.named_parameters():\n",
    "#   print(name)\n",
    "#   print(param.size())\n",
    "#   print(param[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1626643197966,
     "user": {
      "displayName": "Ertuğrul Güngör",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicvIzQgyHfsqD4h979_wDnn11aI-XtLnxmnC_8=s64",
      "userId": "06165302180630202587"
     },
     "user_tz": -180
    },
    "id": "6N3MbPVav4wt"
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "def get_cifar10_sets(num_examples_upstream, num_examples_downstream, num_classes_upstream, \\\n",
    "                     num_classes_downstream, is_downstream_random, batch_size):\n",
    "  # @TODO: Check if it's the same as paper\n",
    "  TF = transforms.Compose([\n",
    "      #transforms.Resize(256),\n",
    "      #transforms.CenterCrop(224),\n",
    "      transforms.ToTensor(),\n",
    "      # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "  ])\n",
    "\n",
    "  # num_examples_upstream = 20000\n",
    "  # num_examples_downstream = 10000\n",
    "  # num_classes_upstream = 30\n",
    "  # num_classes_downstream = 15\n",
    "  # is_downstream_random = False\n",
    "\n",
    "  trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=TF)\n",
    "  # test and transform??\n",
    "  testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=TF)\n",
    "\n",
    "  # Create random indices for splitting upstream and downstream tasks\n",
    "  indices = np.arange(0,len(trainset))\n",
    "  np.random.seed(12345)\n",
    "  np.random.shuffle(indices)\n",
    "  indices = indices.tolist()\n",
    "\n",
    "  # Make upstream labels random\n",
    "  temp = np.array(trainset.targets)\n",
    "  temp[indices[:num_examples_upstream]] = np.random.randint(num_classes_upstream, size=num_examples_upstream)\n",
    "\n",
    "  # If the downstream task uses random labels\n",
    "  if is_downstream_random:\n",
    "    temp[indices[-num_examples_downstream:]] = np.random.randint(num_classes_downstream, size=num_examples_downstream).tolist()\n",
    "\n",
    "  trainset.targets = [int(label) for label in temp]\n",
    "\n",
    "  # Build dataloaders for upstream and downstream tasks\n",
    "  train_upstream_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=False,\n",
    "                                                sampler=torch.utils.data.SubsetRandomSampler(indices[:num_examples_upstream]))\n",
    "\n",
    "  train_downstream_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=False,\n",
    "                                                sampler=torch.utils.data.SubsetRandomSampler(indices[-num_examples_downstream:]))\n",
    "\n",
    "  test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "  # To check the random labels\n",
    "  '''\n",
    "  u_batch = next(iter(train_upstream_loader))\n",
    "  d_batch = next(iter(train_downstream_loader))\n",
    "\n",
    "  u_labels = u_batch[1]\n",
    "  print(u_labels)\n",
    "  d_labels = d_batch[1]\n",
    "  print(d_labels)\n",
    "  '''\n",
    "  return (train_upstream_loader, train_downstream_loader, test_loader)\n",
    "\n",
    "\n",
    "def get_cifar10_orig():\n",
    "  transform = transforms.Compose(\n",
    "      [transforms.ToTensor(),\n",
    "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "  batch_size = 256\n",
    "\n",
    "\n",
    "  trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                          download=False, transform=transform)\n",
    "  indices = np.arange(0,len(trainset))\n",
    "  trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=2, \n",
    "                                            sampler=torch.utils.data.SubsetRandomSampler(indices[:5000]))\n",
    "\n",
    "  testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=False, transform=transform)\n",
    "  testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "  return trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1626643197967,
     "user": {
      "displayName": "Ertuğrul Güngör",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicvIzQgyHfsqD4h979_wDnn11aI-XtLnxmnC_8=s64",
      "userId": "06165302180630202587"
     },
     "user_tz": -180
    },
    "id": "oxh4UOW0g5rq"
   },
   "outputs": [],
   "source": [
    "def accuracy(model, data=\"train\"):\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  _, train_downstream_loader, test_loader = get_cifar10_sets(\n",
    "    num_examples_upstream = 5000,\n",
    "    num_examples_downstream = 20000,\n",
    "    num_classes_upstream = 5,\n",
    "    num_classes_downstream = 10,  \n",
    "    is_downstream_random = False,\n",
    "    batch_size = 256*20\n",
    "  )\n",
    "  if data== \"train\":\n",
    "    dataloader = train_downstream_loader\n",
    "  elif data == \"test\":\n",
    "    dataloader = train_downstream_loader\n",
    "  else:\n",
    "    print(\"choose a proper data mode\")\n",
    "    return\n",
    "  with torch.no_grad():\n",
    "    for data in dataloader:\n",
    "      inputs, labels = data\n",
    "      outputs = model(inputs)\n",
    "      _, predicted = torch.max(outputs.data, 1)\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted == labels).sum().item()\n",
    "\n",
    "  return (correct / total)\n",
    "\n",
    "def train(model, criterion, optimizer, epochs, dataloader, device, scheduler=None, acc_mode=\"none\", verbose=True):\n",
    "  loss_history = []\n",
    "  train_accuracies = []\n",
    "  test_accuracies = []\n",
    "  for epoch in range(epochs):\n",
    "    for i, data in enumerate(dataloader, 0):     \n",
    "      # print(\"{}.th batch\".format(i))\n",
    "      # Our batch:\n",
    "      inputs, labels = data\n",
    "      inputs = inputs.to(device)\n",
    "      labels = labels.to(device)\n",
    "      # print(\"batching!\")\n",
    "\n",
    "      # zero the gradients as PyTorch accumulates them\n",
    "      optimizer.zero_grad()\n",
    "      # print(\"zero grad!\")\n",
    "\n",
    "      # Obtain the scores\n",
    "      outputs = model(inputs)\n",
    "      # print(\"feedforward!\")\n",
    "\n",
    "      # Calculate loss\n",
    "      loss = criterion(outputs.to(device), labels)\n",
    "      # print(\"loss!\")\n",
    "\n",
    "      # Backpropagate\n",
    "      loss.backward()\n",
    "      # print(\"backward!\")\n",
    "\n",
    "      # Update the weights\n",
    "      optimizer.step()\n",
    "      # print(\"update!\")\n",
    "\n",
    "      loss_history.append(loss.item())\n",
    "      if acc_mode==\"both\":\n",
    "        train_acc = accuracy(model, data=\"train\")\n",
    "        train_accuracies.append(train_acc)\n",
    "        test_acc = accuracy(model, data=\"test\")\n",
    "        test_accuracies.append(test_acc)\n",
    "      elif acc_mode==\"train\":\n",
    "        train_acc = accuracy(model, data=\"train\")\n",
    "        train_accuracies.append(train_acc)\n",
    "      elif acc_mode==\"test\":\n",
    "        test_acc = accuracy(model, data=\"test\")\n",
    "        test_accuracies.append(test_acc)\n",
    "      else:\n",
    "        print(\"invalid accuracy option\")\n",
    "      #print(f'Epoch {epoch} / {epochs}: accuracy of {acc}')\n",
    "    if verbose: \n",
    "        print(f'Epoch {epoch} / {epochs}: avg. loss of last 5 iterations {np.sum(loss_history[:-6:-1])/5}')\n",
    "        if acc_mode==\"train\" or acc_mode ==\"both\":\n",
    "          print(f'Epoch {epoch} / {epochs}: Last train acc:{train_acc}')\n",
    "        if acc_mode==\"test\" or acc_mode ==\"both\":\n",
    "          print(f'Epoch {epoch} / {epochs}: Last test acc:{test_acc}')\n",
    "\n",
    "    if scheduler is not None:\n",
    "      scheduler.step()\n",
    "      # Print Learning Rate\n",
    "      print('Epoch:', epoch,'LR:', scheduler.get_last_lr())\n",
    "\n",
    "  if accuracy==\"both\":\n",
    "    return loss_history, train_accuracies, test_accuracies\n",
    "  elif accuracy==\"train\":\n",
    "    return loss_history, train_accuracies\n",
    "  elif accuracy==\"test\":\n",
    "    return loss_history, test_accuracies\n",
    "  return loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1626643197968,
     "user": {
      "displayName": "Ertuğrul Güngör",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicvIzQgyHfsqD4h979_wDnn11aI-XtLnxmnC_8=s64",
      "userId": "06165302180630202587"
     },
     "user_tz": -180
    },
    "id": "3hhJodbVnyOi"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def get_training_components(learnable_params, init_lr, step_size):\n",
    "  '''\n",
    "  Function to prepare components of the training. These are all the same\n",
    "  throughout the experiments made.\n",
    "  '''\n",
    "  # The problem always be a image classification\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  # Which parameters to be updated depends on if the training is done from scratch\n",
    "  # or with pre-trained model.\n",
    "  optimizer = optim.SGD(learnable_params, lr=init_lr, momentum=0.9)\n",
    "  # Step size depends on the # of epoch, gamma is contant\n",
    "  scheduler = StepLR(optimizer, step_size=step_size, gamma=1/3)\n",
    "  return (criterion, optimizer, scheduler)\n",
    "\n",
    "def get_device():\n",
    "  if torch.cuda.is_available():\n",
    "    print(\"Cuda (GPU support) is available and enabled!\")\n",
    "    device = torch.device(\"cuda\")\n",
    "  else:\n",
    "    print(\"Cuda (GPU support) is not available :(\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "def get_learnable_parameters(model):\n",
    "  params_to_update = []\n",
    "  for name, param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "      #print(name)\n",
    "      params_to_update.append(param)\n",
    "  return params_to_update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1626643197968,
     "user": {
      "displayName": "Ertuğrul Güngör",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicvIzQgyHfsqD4h979_wDnn11aI-XtLnxmnC_8=s64",
      "userId": "06165302180630202587"
     },
     "user_tz": -180
    },
    "id": "Udp6bdgUi8A4"
   },
   "outputs": [],
   "source": [
    "# def train_upstream():\n",
    "#   train_upstream_loader, train_downstream_loader, test_loader = get_cifar10_sets(\n",
    "#     num_examples_upstream = 5000,\n",
    "#     num_examples_downstream = 20000,\n",
    "#     num_classes_upstream = 5,\n",
    "#     num_classes_downstream = 5,\n",
    "#     is_downstream_random = False,\n",
    "#     batch_size = 256\n",
    "#   )\n",
    "\n",
    "#   model = get_vgg_model(5, 0.526)\n",
    "#   device = get_device()\n",
    "#   model = model.to(device)\n",
    "#   model.train()\n",
    "#   epochs = 30\n",
    "\n",
    "#   learnable_parameters = get_learnable_parameters(model)\n",
    "#   criterion, optimizer, scheduler = get_training_components(learnable_parameters, 0.01, (int)(epochs/3))\n",
    "#   print(\"before training\")\n",
    "#   loss_history = train(model, criterion, optimizer, epochs, train_upstream_loader, device, scheduler)\n",
    "\n",
    "#   torch.save(model.state_dict(), \"/content/pretrained_model_dict.pt\")\n",
    "#   torch.save(model, \"/content/pretrained_model.pt\")\n",
    "\n",
    "#   return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kEy0gUI0n6_p",
    "outputId": "bfb71388-9147-4cde-cda6-e32f82f6753f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "before training\n",
      "Cuda (GPU support) is available and enabled!\n",
      "Epoch 0 / 120: avg. loss of last 5 iterations 1.6116274118423461\n",
      "Epoch: 0 LR: [0.001]\n",
      "Epoch 1 / 120: avg. loss of last 5 iterations 1.6148926734924316\n",
      "Epoch: 1 LR: [0.001]\n",
      "Epoch 2 / 120: avg. loss of last 5 iterations 1.610210108757019\n",
      "Epoch: 2 LR: [0.001]\n",
      "Epoch 3 / 120: avg. loss of last 5 iterations 1.602347183227539\n",
      "Epoch: 3 LR: [0.001]\n",
      "Epoch 4 / 120: avg. loss of last 5 iterations 1.6011620998382567\n",
      "Epoch: 4 LR: [0.001]\n",
      "Epoch 5 / 120: avg. loss of last 5 iterations 1.6027331590652465\n",
      "Epoch: 5 LR: [0.001]\n",
      "Epoch 6 / 120: avg. loss of last 5 iterations 1.5997007369995118\n",
      "Epoch: 6 LR: [0.001]\n",
      "Epoch 7 / 120: avg. loss of last 5 iterations 1.599612045288086\n",
      "Epoch: 7 LR: [0.001]\n",
      "Epoch 8 / 120: avg. loss of last 5 iterations 1.5869549036026\n",
      "Epoch: 8 LR: [0.001]\n",
      "Epoch 9 / 120: avg. loss of last 5 iterations 1.586445116996765\n",
      "Epoch: 9 LR: [0.001]\n",
      "Epoch 10 / 120: avg. loss of last 5 iterations 1.585192584991455\n",
      "Epoch: 10 LR: [0.001]\n",
      "Epoch 11 / 120: avg. loss of last 5 iterations 1.5792682409286498\n",
      "Epoch: 11 LR: [0.001]\n",
      "Epoch 12 / 120: avg. loss of last 5 iterations 1.5698668241500855\n",
      "Epoch: 12 LR: [0.001]\n",
      "Epoch 13 / 120: avg. loss of last 5 iterations 1.57263503074646\n",
      "Epoch: 13 LR: [0.001]\n",
      "Epoch 14 / 120: avg. loss of last 5 iterations 1.5579508066177368\n",
      "Epoch: 14 LR: [0.001]\n",
      "Epoch 15 / 120: avg. loss of last 5 iterations 1.5289263010025025\n",
      "Epoch: 15 LR: [0.001]\n",
      "Epoch 16 / 120: avg. loss of last 5 iterations 1.5241767644882203\n",
      "Epoch: 16 LR: [0.001]\n",
      "Epoch 17 / 120: avg. loss of last 5 iterations 1.510363483428955\n",
      "Epoch: 17 LR: [0.001]\n",
      "Epoch 18 / 120: avg. loss of last 5 iterations 1.5270854234695435\n",
      "Epoch: 18 LR: [0.001]\n",
      "Epoch 19 / 120: avg. loss of last 5 iterations 1.4784794569015502\n",
      "Epoch: 19 LR: [0.001]\n",
      "Epoch 20 / 120: avg. loss of last 5 iterations 1.4677688837051392\n",
      "Epoch: 20 LR: [0.001]\n",
      "Epoch 21 / 120: avg. loss of last 5 iterations 1.415785002708435\n",
      "Epoch: 21 LR: [0.001]\n",
      "Epoch 22 / 120: avg. loss of last 5 iterations 1.426246452331543\n",
      "Epoch: 22 LR: [0.001]\n",
      "Epoch 23 / 120: avg. loss of last 5 iterations 1.3841572046279906\n",
      "Epoch: 23 LR: [0.001]\n",
      "Epoch 24 / 120: avg. loss of last 5 iterations 1.3252039909362794\n",
      "Epoch: 24 LR: [0.001]\n",
      "Epoch 25 / 120: avg. loss of last 5 iterations 1.3312014579772948\n",
      "Epoch: 25 LR: [0.001]\n",
      "Epoch 26 / 120: avg. loss of last 5 iterations 1.2393404245376587\n",
      "Epoch: 26 LR: [0.001]\n",
      "Epoch 27 / 120: avg. loss of last 5 iterations 1.1749399662017823\n",
      "Epoch: 27 LR: [0.001]\n",
      "Epoch 28 / 120: avg. loss of last 5 iterations 1.0966962814331054\n",
      "Epoch: 28 LR: [0.001]\n",
      "Epoch 29 / 120: avg. loss of last 5 iterations 1.1857522487640382\n",
      "Epoch: 29 LR: [0.001]\n",
      "Epoch 30 / 120: avg. loss of last 5 iterations 0.9080344915390015\n",
      "Epoch: 30 LR: [0.001]\n",
      "Epoch 31 / 120: avg. loss of last 5 iterations 0.8228265523910523\n",
      "Epoch: 31 LR: [0.001]\n",
      "Epoch 32 / 120: avg. loss of last 5 iterations 0.7534050822257996\n",
      "Epoch: 32 LR: [0.001]\n",
      "Epoch 33 / 120: avg. loss of last 5 iterations 0.7273909389972687\n",
      "Epoch: 33 LR: [0.001]\n",
      "Epoch 34 / 120: avg. loss of last 5 iterations 0.7258173942565918\n",
      "Epoch: 34 LR: [0.001]\n",
      "Epoch 35 / 120: avg. loss of last 5 iterations 0.6513443768024445\n",
      "Epoch: 35 LR: [0.001]\n",
      "Epoch 36 / 120: avg. loss of last 5 iterations 0.637853741645813\n",
      "Epoch: 36 LR: [0.001]\n",
      "Epoch 37 / 120: avg. loss of last 5 iterations 0.5612288057804108\n",
      "Epoch: 37 LR: [0.001]\n",
      "Epoch 38 / 120: avg. loss of last 5 iterations 0.374536794424057\n",
      "Epoch: 38 LR: [0.001]\n",
      "Epoch 39 / 120: avg. loss of last 5 iterations 0.43993674516677855\n",
      "Epoch: 39 LR: [0.0003333333333333333]\n",
      "Epoch 40 / 120: avg. loss of last 5 iterations 0.05440500974655151\n",
      "Epoch: 40 LR: [0.0003333333333333333]\n",
      "Epoch 41 / 120: avg. loss of last 5 iterations 0.03398570232093334\n",
      "Epoch: 41 LR: [0.0003333333333333333]\n",
      "Epoch 42 / 120: avg. loss of last 5 iterations 0.01737873274832964\n",
      "Epoch: 42 LR: [0.0003333333333333333]\n",
      "Epoch 43 / 120: avg. loss of last 5 iterations 0.013122071139514446\n",
      "Epoch: 43 LR: [0.0003333333333333333]\n",
      "Epoch 44 / 120: avg. loss of last 5 iterations 0.008542199619114399\n",
      "Epoch: 44 LR: [0.0003333333333333333]\n",
      "Epoch 45 / 120: avg. loss of last 5 iterations 0.007195606920868158\n",
      "Epoch: 45 LR: [0.0003333333333333333]\n",
      "Epoch 46 / 120: avg. loss of last 5 iterations 0.006708457786589861\n",
      "Epoch: 46 LR: [0.0003333333333333333]\n",
      "Epoch 47 / 120: avg. loss of last 5 iterations 0.004212184110656381\n",
      "Epoch: 47 LR: [0.0003333333333333333]\n",
      "Epoch 48 / 120: avg. loss of last 5 iterations 0.004137201327830553\n",
      "Epoch: 48 LR: [0.0003333333333333333]\n",
      "Epoch 49 / 120: avg. loss of last 5 iterations 0.003934452310204506\n",
      "Epoch: 49 LR: [0.0003333333333333333]\n",
      "Epoch 50 / 120: avg. loss of last 5 iterations 0.0035444654989987613\n",
      "Epoch: 50 LR: [0.0003333333333333333]\n",
      "Epoch 51 / 120: avg. loss of last 5 iterations 0.003665304183959961\n",
      "Epoch: 51 LR: [0.0003333333333333333]\n",
      "Epoch 52 / 120: avg. loss of last 5 iterations 0.0032391814980655908\n",
      "Epoch: 52 LR: [0.0003333333333333333]\n",
      "Epoch 53 / 120: avg. loss of last 5 iterations 0.0026383074931800366\n",
      "Epoch: 53 LR: [0.0003333333333333333]\n",
      "Epoch 54 / 120: avg. loss of last 5 iterations 0.0025293110404163597\n",
      "Epoch: 54 LR: [0.0003333333333333333]\n",
      "Epoch 55 / 120: avg. loss of last 5 iterations 0.0019899022299796343\n",
      "Epoch: 55 LR: [0.0003333333333333333]\n",
      "Epoch 56 / 120: avg. loss of last 5 iterations 0.0024294915376231073\n",
      "Epoch: 56 LR: [0.0003333333333333333]\n",
      "Epoch 57 / 120: avg. loss of last 5 iterations 0.0018315328285098075\n",
      "Epoch: 57 LR: [0.0003333333333333333]\n",
      "Epoch 58 / 120: avg. loss of last 5 iterations 0.001792528317309916\n",
      "Epoch: 58 LR: [0.0003333333333333333]\n",
      "Epoch 59 / 120: avg. loss of last 5 iterations 0.0017744489246979355\n",
      "Epoch: 59 LR: [0.0003333333333333333]\n",
      "Epoch 60 / 120: avg. loss of last 5 iterations 0.001681758789345622\n",
      "Epoch: 60 LR: [0.0003333333333333333]\n",
      "Epoch 61 / 120: avg. loss of last 5 iterations 0.001342577813193202\n",
      "Epoch: 61 LR: [0.0003333333333333333]\n",
      "Epoch 62 / 120: avg. loss of last 5 iterations 0.0017178986920043827\n",
      "Epoch: 62 LR: [0.0003333333333333333]\n",
      "Epoch 63 / 120: avg. loss of last 5 iterations 0.0012248648214153945\n",
      "Epoch: 63 LR: [0.0003333333333333333]\n",
      "Epoch 64 / 120: avg. loss of last 5 iterations 0.0013008484849706293\n",
      "Epoch: 64 LR: [0.0003333333333333333]\n",
      "Epoch 65 / 120: avg. loss of last 5 iterations 0.0014715122524648905\n",
      "Epoch: 65 LR: [0.0003333333333333333]\n",
      "Epoch 66 / 120: avg. loss of last 5 iterations 0.001283877552486956\n",
      "Epoch: 66 LR: [0.0003333333333333333]\n",
      "Epoch 67 / 120: avg. loss of last 5 iterations 0.0013659506337717175\n",
      "Epoch: 67 LR: [0.0003333333333333333]\n",
      "Epoch 68 / 120: avg. loss of last 5 iterations 0.0011790233431383968\n",
      "Epoch: 68 LR: [0.0003333333333333333]\n",
      "Epoch 69 / 120: avg. loss of last 5 iterations 0.001353851007297635\n",
      "Epoch: 69 LR: [0.0003333333333333333]\n",
      "Epoch 70 / 120: avg. loss of last 5 iterations 0.0013176648644730449\n",
      "Epoch: 70 LR: [0.0003333333333333333]\n",
      "Epoch 71 / 120: avg. loss of last 5 iterations 0.0010891947778873146\n",
      "Epoch: 71 LR: [0.0003333333333333333]\n",
      "Epoch 72 / 120: avg. loss of last 5 iterations 0.0009235843317583203\n",
      "Epoch: 72 LR: [0.0003333333333333333]\n",
      "Epoch 73 / 120: avg. loss of last 5 iterations 0.0009976232540793717\n",
      "Epoch: 73 LR: [0.0003333333333333333]\n",
      "Epoch 74 / 120: avg. loss of last 5 iterations 0.0009302536491304636\n",
      "Epoch: 74 LR: [0.0003333333333333333]\n",
      "Epoch 75 / 120: avg. loss of last 5 iterations 0.0009837640332989394\n",
      "Epoch: 75 LR: [0.0003333333333333333]\n",
      "Epoch 76 / 120: avg. loss of last 5 iterations 0.0008317946107126772\n",
      "Epoch: 76 LR: [0.0003333333333333333]\n",
      "Epoch 77 / 120: avg. loss of last 5 iterations 0.0008286085911095142\n",
      "Epoch: 77 LR: [0.0003333333333333333]\n",
      "Epoch 78 / 120: avg. loss of last 5 iterations 0.0007316739414818585\n",
      "Epoch: 78 LR: [0.0003333333333333333]\n",
      "Epoch 79 / 120: avg. loss of last 5 iterations 0.0008300547953695059\n",
      "Epoch: 79 LR: [0.0001111111111111111]\n",
      "Epoch 80 / 120: avg. loss of last 5 iterations 0.0008184714708477259\n",
      "Epoch: 80 LR: [0.0001111111111111111]\n",
      "Epoch 81 / 120: avg. loss of last 5 iterations 0.0008866640971973538\n",
      "Epoch: 81 LR: [0.0001111111111111111]\n",
      "Epoch 82 / 120: avg. loss of last 5 iterations 0.0007636680151335895\n",
      "Epoch: 82 LR: [0.0001111111111111111]\n",
      "Epoch 83 / 120: avg. loss of last 5 iterations 0.0008256633998826146\n",
      "Epoch: 83 LR: [0.0001111111111111111]\n",
      "Epoch 84 / 120: avg. loss of last 5 iterations 0.0007867012405768037\n",
      "Epoch: 84 LR: [0.0001111111111111111]\n",
      "Epoch 85 / 120: avg. loss of last 5 iterations 0.0008692415896803141\n",
      "Epoch: 85 LR: [0.0001111111111111111]\n",
      "Epoch 86 / 120: avg. loss of last 5 iterations 0.0007354660890996456\n",
      "Epoch: 86 LR: [0.0001111111111111111]\n",
      "Epoch 87 / 120: avg. loss of last 5 iterations 0.0007206289097666741\n",
      "Epoch: 87 LR: [0.0001111111111111111]\n",
      "Epoch 88 / 120: avg. loss of last 5 iterations 0.0008155810763128101\n",
      "Epoch: 88 LR: [0.0001111111111111111]\n",
      "Epoch 89 / 120: avg. loss of last 5 iterations 0.0006980741338338703\n",
      "Epoch: 89 LR: [0.0001111111111111111]\n",
      "Epoch 90 / 120: avg. loss of last 5 iterations 0.0007005433551967144\n",
      "Epoch: 90 LR: [0.0001111111111111111]\n",
      "Epoch 91 / 120: avg. loss of last 5 iterations 0.0007385100587271153\n",
      "Epoch: 91 LR: [0.0001111111111111111]\n",
      "Epoch 92 / 120: avg. loss of last 5 iterations 0.0008191359229385853\n",
      "Epoch: 92 LR: [0.0001111111111111111]\n",
      "Epoch 93 / 120: avg. loss of last 5 iterations 0.0007214409881271422\n",
      "Epoch: 93 LR: [0.0001111111111111111]\n",
      "Epoch 94 / 120: avg. loss of last 5 iterations 0.0006240191520191729\n",
      "Epoch: 94 LR: [0.0001111111111111111]\n",
      "Epoch 95 / 120: avg. loss of last 5 iterations 0.0006870375713333487\n",
      "Epoch: 95 LR: [0.0001111111111111111]\n",
      "Epoch 96 / 120: avg. loss of last 5 iterations 0.0007036964292638004\n",
      "Epoch: 96 LR: [0.0001111111111111111]\n",
      "Epoch 97 / 120: avg. loss of last 5 iterations 0.0007927530095912516\n",
      "Epoch: 97 LR: [0.0001111111111111111]\n",
      "Epoch 98 / 120: avg. loss of last 5 iterations 0.000729653099551797\n",
      "Epoch: 98 LR: [0.0001111111111111111]\n",
      "Epoch 99 / 120: avg. loss of last 5 iterations 0.000749145052395761\n",
      "Epoch: 99 LR: [0.0001111111111111111]\n",
      "Epoch 100 / 120: avg. loss of last 5 iterations 0.0005884407146368176\n",
      "Epoch: 100 LR: [0.0001111111111111111]\n",
      "Epoch 101 / 120: avg. loss of last 5 iterations 0.0005849236680660397\n",
      "Epoch: 101 LR: [0.0001111111111111111]\n",
      "Epoch 102 / 120: avg. loss of last 5 iterations 0.0007185349939391017\n",
      "Epoch: 102 LR: [0.0001111111111111111]\n",
      "Epoch 103 / 120: avg. loss of last 5 iterations 0.0005699059984181076\n",
      "Epoch: 103 LR: [0.0001111111111111111]\n",
      "Epoch 104 / 120: avg. loss of last 5 iterations 0.0006768248858861625\n",
      "Epoch: 104 LR: [0.0001111111111111111]\n",
      "Epoch 105 / 120: avg. loss of last 5 iterations 0.0006298926775343717\n",
      "Epoch: 105 LR: [0.0001111111111111111]\n",
      "Epoch 106 / 120: avg. loss of last 5 iterations 0.0006625704350881279\n",
      "Epoch: 106 LR: [0.0001111111111111111]\n",
      "Epoch 107 / 120: avg. loss of last 5 iterations 0.0006537071079947054\n",
      "Epoch: 107 LR: [0.0001111111111111111]\n",
      "Epoch 108 / 120: avg. loss of last 5 iterations 0.0006276093190535903\n",
      "Epoch: 108 LR: [0.0001111111111111111]\n",
      "Epoch 109 / 120: avg. loss of last 5 iterations 0.000850564579013735\n",
      "Epoch: 109 LR: [0.0001111111111111111]\n",
      "Epoch 110 / 120: avg. loss of last 5 iterations 0.0005241116799879819\n",
      "Epoch: 110 LR: [0.0001111111111111111]\n",
      "Epoch 111 / 120: avg. loss of last 5 iterations 0.0008084095432423055\n",
      "Epoch: 111 LR: [0.0001111111111111111]\n",
      "Epoch 112 / 120: avg. loss of last 5 iterations 0.0005206139816436916\n",
      "Epoch: 112 LR: [0.0001111111111111111]\n",
      "Epoch 113 / 120: avg. loss of last 5 iterations 0.0005627731210552156\n",
      "Epoch: 113 LR: [0.0001111111111111111]\n",
      "Epoch 114 / 120: avg. loss of last 5 iterations 0.0006107896100729704\n",
      "Epoch: 114 LR: [0.0001111111111111111]\n",
      "Epoch 115 / 120: avg. loss of last 5 iterations 0.0005574273876845836\n",
      "Epoch: 115 LR: [0.0001111111111111111]\n",
      "Epoch 116 / 120: avg. loss of last 5 iterations 0.0006820730573963374\n",
      "Epoch: 116 LR: [0.0001111111111111111]\n",
      "Epoch 117 / 120: avg. loss of last 5 iterations 0.0005528347159270197\n",
      "Epoch: 117 LR: [0.0001111111111111111]\n",
      "Epoch 118 / 120: avg. loss of last 5 iterations 0.0005517038225661963\n",
      "Epoch: 118 LR: [0.0001111111111111111]\n",
      "Epoch 119 / 120: avg. loss of last 5 iterations 0.0005590659566223621\n",
      "Epoch: 119 LR: [3.703703703703703e-05]\n"
     ]
    }
   ],
   "source": [
    "train_upstream_loader, train_downstream_loader, test_loader = get_cifar10_sets(\n",
    "    num_examples_upstream = 20000,\n",
    "    num_examples_downstream = 20000,\n",
    "    num_classes_upstream = 5,\n",
    "    num_classes_downstream = 5,\n",
    "    is_downstream_random = False,\n",
    "    batch_size = 256\n",
    "  )\n",
    "\n",
    "model = get_vgg_model(5, 1)\n",
    "\n",
    "#train_upstream_loader = get_cifar10_orig()\n",
    "#model = get_vgg_model(10, 1)\n",
    "\n",
    "epochs = 120\n",
    "learnable_parameters = get_learnable_parameters(model)\n",
    "criterion, optimizer, scheduler = get_training_components(learnable_parameters, 0.001, (int)(epochs/3))\n",
    "print(\"before training\")\n",
    "device = get_device()\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "loss_history = train(model, criterion, optimizer, epochs, train_upstream_loader, device, scheduler=scheduler)\n",
    "#loss_history = train(model, criterion, optimizer, epochs, train_upstream_loader, device)\n",
    "\n",
    "torch.save(model.state_dict(), \"./pretrained_model_dict_upstream_1_2.pt\")\n",
    "torch.save(model, \"./pretrained_model_upstream_1_2.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def setup_and_train_downstream(epochs, lr, step_size, num_examples_upstream, num_examples_downstream,\n",
    "                   num_classes_upstream, num_classes_downstream, is_downstream_random,\n",
    "                   batch_size, num_outputs, upstream_model_path=None):\n",
    "    \n",
    "    train_upstream_loader, train_downstream_loader, test_loader = get_cifar10_sets(\n",
    "        num_examples_upstream, num_examples_downstream, num_classes_upstream,\n",
    "        num_classes_downstream, is_downstream_random, batch_size)\n",
    "\n",
    "    model = get_vgg_model(num_outputs, 1)\n",
    "    if upstream_model_path is not None:\n",
    "        model.load_state_dict(torch.load(upstream_model_path), strict=False)\n",
    "        model.classifier = nn.Linear(512, num_outputs)\n",
    "        torch.nn.init.kaiming_normal_(model.classifier.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    learnable_parameters = get_learnable_parameters(model)\n",
    "    criterion, optimizer, scheduler = get_training_components(learnable_parameters, lr, step_size)\n",
    "    device = get_device()\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    if not is_downstream_random:\n",
    "      loss_history, train_accuracies, test_accuracies = train(model, criterion, optimizer, epochs, train_downstream_loader, \n",
    "                                                              device, scheduler=scheduler, acc_mode=\"both\")\n",
    "      return loss_history, train_accuracies, test_accuracies\n",
    "    \n",
    "    loss_history, train_accuracies = train(model, criterion, optimizer, epochs, train_downstream_loader, \n",
    "                                           device, scheduler=scheduler, acc_mode=\"train\")\n",
    "    return loss_history, train_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda (GPU support) is available and enabled!\n"
     ]
    }
   ],
   "source": [
    "# From scratch with real labels\n",
    "loss_scratch_real, accs_scratch_real_train, accs_scratch_real_test = setup_and_train_downstream(\n",
    "    epochs=1, lr=0.001, step_size=(int)(40/3), num_examples_upstream=20000, \n",
    "    num_examples_downstream=20000, num_classes_upstream = 5, num_classes_downstream = 5,\n",
    "    is_downstream_random = False, batch_size = 256, num_outputs=10\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda (GPU support) is available and enabled!\n",
      "Epoch 0 / 9: avg. loss of last 5 iterations 1.7449617385864258\n",
      "Epoch: 0 LR: [0.001]\n",
      "Epoch 1 / 9: avg. loss of last 5 iterations 1.5306174755096436\n",
      "Epoch: 1 LR: [0.001]\n",
      "Epoch 2 / 9: avg. loss of last 5 iterations 1.3967819213867188\n",
      "Epoch: 2 LR: [0.0003333333333333333]\n",
      "Epoch 3 / 9: avg. loss of last 5 iterations 1.1582336664199828\n",
      "Epoch: 3 LR: [0.0003333333333333333]\n",
      "Epoch 4 / 9: avg. loss of last 5 iterations 1.1891555547714234\n",
      "Epoch: 4 LR: [0.0003333333333333333]\n",
      "Epoch 5 / 9: avg. loss of last 5 iterations 1.0669708251953125\n",
      "Epoch: 5 LR: [0.0001111111111111111]\n",
      "Epoch 6 / 9: avg. loss of last 5 iterations 1.0692302227020263\n",
      "Epoch: 6 LR: [0.0001111111111111111]\n",
      "Epoch 7 / 9: avg. loss of last 5 iterations 0.9981380224227905\n",
      "Epoch: 7 LR: [0.0001111111111111111]\n",
      "Epoch 8 / 9: avg. loss of last 5 iterations 0.9906956315040588\n",
      "Epoch: 8 LR: [3.703703703703703e-05]\n"
     ]
    }
   ],
   "source": [
    "# Using upstream training's weights with real labels\n",
    "loss_upstream_real, accs_upstream_real_train, accs_upstream_real_test = setup_and_train_downstream(\n",
    "    epochs=9, lr=0.001, step_size=(int)(9/3), num_examples_upstream=20000, \n",
    "    num_examples_downstream=20000, num_classes_upstream = 5, num_classes_downstream = 5,\n",
    "    is_downstream_random = False, batch_size = 256, num_outputs=10,\n",
    "    upstream_model_path = \"pretrained_model_dict_1_2.pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(accuracies)\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of figure1.ipynb",
   "provenance": [
    {
     "file_id": "1BNwYcPcE3j7kX8PsVSfBpXp1s6Dwmfne",
     "timestamp": 1626623850743
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
