{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "figure5",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cc__MQqYn3K"
      },
      "source": [
        "from torchvision import models\n",
        "from torch import nn\n",
        "import torch\n",
        "\n",
        "class Identity(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Identity, self).__init__()\n",
        "      \n",
        "  def forward(self, x):\n",
        "    return x\n",
        "\n",
        "\n",
        "from torch import nn\n",
        "import math\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SimpleCNN, self).__init__()\n",
        "    \n",
        "    self.conv = nn.Conv2d(in_channels=3, out_channels=256, \n",
        "                        kernel_size=3, stride=1, padding=0)\n",
        "    last_size = 30*30*256\n",
        "    self.fc = nn.Linear(last_size, 64)\n",
        "    self.classifier = nn.Linear(64, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    forward = nn.Sequential(self.conv, nn.ReLU(),\n",
        "                            nn.Flatten(),\n",
        "                            self.fc, nn.ReLU(),\n",
        "                            self.classifier)\n",
        "    x = forward(x)\n",
        "    return x\n",
        "\n",
        "# install and import the torchinfo library\n",
        "from torchinfo import summary\n",
        "\n",
        "# get some info for the models to check if everything is OK\n",
        "simple_cnn = SimpleCNN()\n",
        "print(summary(simple_cnn))\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "import copy\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction import image\n",
        "\n",
        "def get_cifar10_sets(num_examples_upstream, num_examples_downstream, num_classes_upstream, \\\n",
        "                     num_classes_downstream, is_downstream_random, batch_size):\n",
        "  \"\"\"\n",
        "  Function to create dataloaders to be used throughout the experiments.\n",
        "\n",
        "  num_examples_upstream: # of samples to be used in upstream training\n",
        "  num_examples_downstream: # of samples to be used in downstream training\n",
        "  num_classes_upstream: # of random classes to be used in upstream training\n",
        "  num_classes_downstream: # of random classes to be used in downstream training. If `is_downstream_random`\n",
        "                          is False then this parameter will be ignored.\n",
        "  is_downstream_random: Whether downstream task uses random labels. \n",
        "  batch_size: Batch size.\n",
        "  \"\"\"\n",
        "\n",
        "  # as a result it normalizes the input between [-1,1]\n",
        "  TF = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "  ])\n",
        "\n",
        "  trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=TF)\n",
        "  testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=TF)\n",
        "\n",
        "  # Create random indices for splitting upstream and downstream tasks\n",
        "  indices = np.arange(0,len(trainset))\n",
        "  #np.random.seed(12345)\n",
        "  np.random.shuffle(indices)\n",
        "  indices = indices.tolist()\n",
        "\n",
        "  # Make upstream labels random\n",
        "  temp = np.array(trainset.targets)\n",
        "  temp[indices[:num_examples_upstream]] = np.random.randint(num_classes_upstream, size=num_examples_upstream)\n",
        "  \n",
        "  # If the downstream task uses random labels\n",
        "  if is_downstream_random:\n",
        "    #np.random.seed(12345)\n",
        "    temp[indices[-num_examples_downstream:]] = np.random.randint(num_classes_downstream, size=num_examples_downstream).tolist()\n",
        "  trainset.targets = [int(label) for label in temp]\n",
        "\n",
        "  # Build dataloaders for upstream and downstream tasks\n",
        "  train_upstream_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=False,\n",
        "                                                sampler=torch.utils.data.SubsetRandomSampler(indices[:num_examples_upstream]))\n",
        "\n",
        "  train_downstream_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=False,\n",
        "                                                sampler=torch.utils.data.SubsetRandomSampler(indices[-num_examples_downstream:]))\n",
        "\n",
        "  test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  # To check the random labels\n",
        "  '''\n",
        "  u_batch = next(iter(train_upstream_loader))\n",
        "  d_batch = next(iter(train_downstream_loader))\n",
        "\n",
        "  u_labels = u_batch[1]\n",
        "  print(u_labels)\n",
        "  d_labels = d_batch[1]\n",
        "  print(d_labels)\n",
        "  '''\n",
        "  return (train_upstream_loader, train_downstream_loader, test_loader)\n",
        "\n",
        "\n",
        "def accuracy(model, dataloader):\n",
        "  \"\"\"\n",
        "  Calculates accuracy of model in given dataloader\n",
        "\n",
        "  model: Model that the accuracy is calculated for\n",
        "  dataloader: Dataloader to get samples that accuracy is calculated on\n",
        "  \"\"\"\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "      inputs, labels = data\n",
        "      outputs = model(inputs)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  return (correct / total)\n",
        "\n",
        "def train(model, criterion, optimizer, epochs, dataloader, device, scheduler=None,\n",
        "          acc_mode=\"none\", acc_dataloaders=None, verbose=True):\n",
        "  \"\"\"\n",
        "  Trains given model with given configuration.\n",
        "\n",
        "  model: Model to be used\n",
        "  criterion: Loss function to be used\n",
        "  optimizer: Optimizer to be used.\n",
        "  epochs: # of epochs\n",
        "  dataloader: Dataloader for training set\n",
        "  device: which hardware the model is trained on. CPU or GPU(cuda)\n",
        "  scheduler: learning rate scheduler\n",
        "  acc_mode: Accuracy mode indicating that which datasets accuracy metrics are calculated.\n",
        "            \"none\"   = no accuracy calculation\n",
        "            \"traing\" = accuracy calculation on training set\n",
        "            \"both\"   = accuracy calculation on both training and test set\n",
        "  acc_dataloaders: Dataloaders to get samples that are used for accuracy calculation. Value of this\n",
        "                   parameter should match with `acc_mode`. For example given `acc_mode` with \"both\"\n",
        "                   option, two dataloaders should be given inside this parameter.\n",
        "  verbose: Verbosity option\n",
        "  \"\"\"\n",
        "  loss_history = []\n",
        "  train_accuracies = []\n",
        "  test_accuracies = []\n",
        "  for epoch in range(1,epochs+1):\n",
        "    count = 0\n",
        "    for i, data in enumerate(dataloader, 0):     \n",
        "      inputs, labels = data\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # canonical forward-backward-update chain\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs.to(device), labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      loss_history.append(loss.item())\n",
        "      # count is used to control resolution of accuracy data. Experiments in the paper calculates\n",
        "      # accuracy on the whole training-test set each iteration which is very time consuming. With\n",
        "      # the variable it's just made less frequent.\n",
        "\n",
        "      count += 1\n",
        "      if (count % 15 == 0):\n",
        "          if acc_mode==\"both\":\n",
        "            train_acc = accuracy(model, acc_dataloaders[0])\n",
        "            train_accuracies.append(train_acc)\n",
        "            test_acc = accuracy(model, acc_dataloaders[1])\n",
        "            test_accuracies.append(test_acc)\n",
        "          elif acc_mode==\"train\":\n",
        "            train_acc = accuracy(model, acc_dataloaders[0])\n",
        "            train_accuracies.append(train_acc)\n",
        "          elif acc_mode==\"test\":\n",
        "            test_acc = accuracy(model, acc_dataloaders[1])\n",
        "            test_accuracies.append(test_acc)\n",
        "    \n",
        "    if verbose: \n",
        "        print(f'Epoch {epoch} / {epochs}: avg. loss of last 5 iterations {np.sum(loss_history[:-6:-1])/5}')\n",
        "        if acc_mode==\"train\" or acc_mode ==\"both\":\n",
        "          print(f'Epoch {epoch} / {epochs}: Last train acc:{train_acc}')\n",
        "        if acc_mode==\"test\" or acc_mode ==\"both\":\n",
        "          print(f'Epoch {epoch} / {epochs}: Last test acc:{test_acc}')\n",
        "\n",
        "    if scheduler is not None:\n",
        "      scheduler.step()\n",
        "      # Print Learning Rate\n",
        "      print('Epoch:', epoch,'LR:', scheduler.get_last_lr())\n",
        "\n",
        "  if acc_mode==\"both\":\n",
        "    return loss_history, train_accuracies, test_accuracies\n",
        "  elif acc_mode==\"train\":\n",
        "    return loss_history, train_accuracies\n",
        "  elif acc_mode==\"test\":\n",
        "    return loss_history, test_accuracies\n",
        "  return loss_history\n",
        "\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "def get_training_components(learnable_params, init_lr, step_size):\n",
        "  '''\n",
        "  Function to prepare components of the training. These are all the same\n",
        "  throughout the experiments made.\n",
        "  \n",
        "  learnable_params: Parameters that optimizer runs to update.\n",
        "  init_lr: Initial learning rate that scheduler is given. \n",
        "  step_size: step size of lowering learning rate. Every `step_size` epochs\n",
        "             learning rate will be scaled by constant 1/3.\n",
        "  '''\n",
        "  # The problem always be a image classification\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  # Which parameters to be updated depends on if the training is done from scratch\n",
        "  # or with pre-trained model.\n",
        "  optimizer = optim.SGD(learnable_params, lr=init_lr, momentum=0.9)\n",
        "  # Step size depends on the # of epoch, gamma is contant in the paper\n",
        "  scheduler = StepLR(optimizer, step_size=step_size, gamma=1/3)\n",
        "  return (criterion, optimizer, scheduler)\n",
        "\n",
        "def get_device():\n",
        "  if torch.cuda.is_available():\n",
        "    print(\"Cuda (GPU support) is available and enabled!\")\n",
        "    device = torch.device(\"cuda\")\n",
        "  else:\n",
        "    print(\"Cuda (GPU support) is not available :(\")\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "def get_learnable_parameters(model):\n",
        "  params_to_update = []\n",
        "  for name, param in model.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "      params_to_update.append(param)\n",
        "  return params_to_update\n",
        "\n",
        "def create_patches_data(trainloader, patch_size):\n",
        "  # 32x32 images in CIFAR10, stride is 1\n",
        "  num_patches_per_image = ((32 - patch_size + 1)**2)\n",
        "  # 3 channels  \n",
        "  flattened_patches_length = (patch_size**2) * 3\n",
        "  data_matrix = None\n",
        "  batch = 1\n",
        "  patches_list = []\n",
        "  for data in trainloader:\n",
        "    inputs, labels = data\n",
        "    for i in range(len(inputs)):\n",
        "      img = inputs[i]\n",
        "      img = img.permute(1,2,0)\n",
        "      patches = image.extract_patches_2d(img, (patch_size, patch_size))\n",
        "      patches_list.append(patches)\n",
        "    batch+=1\n",
        "  patches_list = np.stack(patches_list, axis=0)\n",
        "  print(patches_list.shape)\n",
        "  data_matrix = np.reshape(patches_list, (-1, flattened_patches_length))\n",
        "  print(data_matrix.shape)\n",
        "  return data_matrix\n",
        "\n",
        "def create_eigs(data):\n",
        "  cov_matrix = np.cov(data , rowvar=False)\n",
        "  w, v = LA.eig(cov_matrix)\n",
        "  return w,v\n",
        "\n",
        "# Upstream training for 1st and 2nd graphs of figure1\n",
        "from torch import linalg as LA\n",
        "\n",
        "train_upstream_loader, train_downstream_loader, test_loader = get_cifar10_sets(\n",
        "    num_examples_upstream = 10000,\n",
        "    num_examples_downstream = 10000,\n",
        "    num_classes_upstream = 10,\n",
        "    num_classes_downstream = 10,\n",
        "    is_downstream_random = False,\n",
        "    batch_size = 256\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAICtzjWYszC"
      },
      "source": [
        "\n",
        "\n",
        "model = SimpleCNN()\n",
        "norm_dict = {}\n",
        "for name, param in model.named_parameters():\n",
        "  norm_dict[name] = LA.norm(param)\n",
        "\n",
        "epochs = 40\n",
        "learnable_parameters = model.parameters()\n",
        "criterion, optimizer, scheduler = get_training_components(learnable_parameters, 0.001, (int)(epochs/3))\n",
        "\n",
        "device = get_device()\n",
        "model = model.to(device)\n",
        "model.train()\n",
        "loss_history = train(model, criterion, optimizer, epochs, train_upstream_loader, device, scheduler=scheduler)\n",
        "\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), \"./pretrainednoscale_model_dict_upstream_1_2.pt\")\n",
        "torch.save(model, \"./pretrainednoscale_model_upstream_1_2.pt\")\n",
        "\n",
        "# scale norms back to initial values after upstream training\n",
        "for name, param in model.named_parameters():\n",
        "  scale = norm_dict[name]\n",
        "  param.data.copy_(param*(scale.item()/LA.norm(param)))\n",
        "\n",
        "CNN_weights = np.reshape(model.conv.weight.detach().numpy(), (model.conv.weight.detach().numpy().shape[0], -1))\n",
        "Cov_y = np.cov(CNN_weights , rowvar=False)\n",
        "print(Cov_y.shape)\n",
        "# save upstream training\n",
        "torch.save(model.state_dict(), \"./pretrained_model_dict_upstream_1_2.pt\")\n",
        "torch.save(model, \"./pretrained_model_upstream_1_2.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccQpoBFWJWoz"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "def setup_and_train_downstream(train_upstream_loader, train_downstream_loader, test_loader,\n",
        "                   acc_train_loader, acc_test_loader, epochs, lr, step_size, num_examples_upstream, \n",
        "                   num_examples_downstream, num_classes_upstream, num_classes_downstream, \n",
        "                   is_downstream_random, batch_size, num_outputs,  upstream_model_path=None, \n",
        "                   weights_are_covariance=False, cov_mat=None):\n",
        "  \"\"\"\n",
        "  Creates required parameters for downstream task and starts training. \n",
        "\n",
        "  epochs: # of epochs\n",
        "  lr: learning rate\n",
        "  num_examples_upstream: # of samples to be used in upstream training\n",
        "  num_examples_downstream: # of samples to be used in downstream training\n",
        "  num_classes_upstream: # of random classes to be used in upstream training\n",
        "  num_classes_downstream: # of random classes to be used in downstream training. If `is_downstream_random`\n",
        "                          is False then this parameter will be ignored.\n",
        "  is_downstream_random: Whether downstream task uses random labels. \n",
        "  batch_size: Batch size.\n",
        "  num_outputs: # of neurons in classifier layer that is equal to # of classes in the problem\n",
        "  upstream_model_path: Path to load weights of upstream training.\n",
        "  \n",
        "  train_upstream_loader, train_downstream_loader, test_loader = get_cifar10_sets(\n",
        "      num_examples_upstream, num_examples_downstream, num_classes_upstream,\n",
        "      num_classes_downstream, is_downstream_random, batch_size)\n",
        "  \n",
        "  # separate dataloader only used for accuracy\n",
        "  _, acc_train_loader, acc_test_loader = get_cifar10_sets(\n",
        "      num_examples_upstream = 10000,\n",
        "      num_examples_downstream = num_examples_downstream,\n",
        "      num_classes_upstream = 10,\n",
        "      num_classes_downstream = num_classes_downstream,  \n",
        "      is_downstream_random = is_downstream_random,\n",
        "      batch_size = num_examples_downstream\n",
        "  )\n",
        "  \"\"\"\n",
        "  # to not get an error while loading state of upstream training, classification layer is\n",
        "  # initialized with `num_classes_downstream` neurons. Afterwards, layer is corrected with\n",
        "  # `num_outputs` parameter\n",
        "  model2 = SimpleCNN()\n",
        "  if upstream_model_path is not None:\n",
        "      model2.load_state_dict(torch.load(upstream_model_path), strict=False)\n",
        "      #model2.load(upstream_model_path)\n",
        "      model2.classifier = nn.Linear(64, num_outputs)#nn.Linear(256, 64)#\n",
        "      torch.nn.init.kaiming_normal_(model2.classifier.weight, mode='fan_out', nonlinearity='relu')\n",
        "  if weights_are_covariance is not False:\n",
        "    #patch_size = 3\n",
        "    #patches_data = create_patches_data(train_upstream_loader, patch_size)a view of a leaf Variable that requires grad is being used in an in-place operation.\n",
        "    #cov_mat = np.cov(patches_data , rowvar=False)\n",
        "    #print(model.conv.weight.detach().numpy().shape )\n",
        "    #print(cov_mat.shape)\n",
        "    #model.conv.weight.requires_grad = False\n",
        "    new_weights = np.empty((256,3,3,3))\n",
        "    for i in range(256): #Out Channels\n",
        "        new_weights[i] = np.random.multivariate_normal(np.zeros(cov_mat.shape[0]), cov_mat, 1).reshape((3,3,3))\n",
        "        #print(new_weights.shape)\n",
        "        #print(model.conv.weight[i].shape)\n",
        "    \n",
        "    model2.conv.weight = torch.nn.Parameter( torch.tensor(new_weights).float())\n",
        "    model2.conv.weight.requires_grad = True\n",
        "  #print(model.conv.weight.detach().numpy().shape )\n",
        "  learnable_parameters = model2.parameters()\n",
        "  criterion, optimizer, scheduler = get_training_components(learnable_parameters, lr, step_size)\n",
        "  device = get_device()\n",
        "  model2 = model2.to(device)\n",
        "  model2.train()\n",
        "  if not is_downstream_random:\n",
        "    loss_history, train_accuracies, test_accuracies = train(model2, criterion, optimizer, epochs, train_downstream_loader, \n",
        "                                                            device, scheduler, \"both\",\n",
        "                                                            [acc_train_loader, acc_test_loader])\n",
        "    return loss_history, train_accuracies, test_accuracies\n",
        "  loss_history, train_accuracies = train(model2, criterion, optimizer, epochs, train_downstream_loader, \n",
        "                                          device, scheduler, \"train\",\n",
        "                                          [acc_train_loader])\n",
        "  return loss_history, train_accuracies\n",
        "\n",
        "\n",
        "num_examples_upstream=10000 \n",
        "num_examples_downstream=10000\n",
        "num_classes_upstream = 10 \n",
        "num_classes_downstream = 10\n",
        "is_downstream_random = False \n",
        "batch_size = 256\n",
        "\n",
        "#train_upstream_loader, train_downstream_loader, test_loader = get_cifar10_sets(\n",
        "#      num_examples_upstream, num_examples_downstream, num_classes_upstream,\n",
        "#      num_classes_downstream, is_downstream_random, batch_size)\n",
        "  \n",
        "  # separate dataloader only used for accuracy\n",
        "_, acc_train_loader, acc_test_loader = get_cifar10_sets(\n",
        "num_examples_upstream = 10000,\n",
        "num_examples_downstream = num_examples_downstream,\n",
        "num_classes_upstream = 10,\n",
        "num_classes_downstream = num_classes_downstream,  \n",
        "is_downstream_random = is_downstream_random,\n",
        "batch_size = num_examples_downstream)\n",
        "\n",
        "\n",
        "loss_noscale_real, accs_noscale_real_train, accs_noscale_real_test = setup_and_train_downstream(\n",
        "    train_upstream_loader, train_downstream_loader,\n",
        "    test_loader, acc_train_loader, acc_test_loader,\n",
        "    epochs=40, lr=0.001, step_size=(int)(40/3), num_examples_upstream=10000, \n",
        "    num_examples_downstream=10000, num_classes_upstream = 10, num_classes_downstream = 10,\n",
        "    is_downstream_random = False, batch_size = 256, num_outputs=10, upstream_model_path = \"pretrainednoscale_model_dict_upstream_1_2.pt\")\n",
        "\n",
        "\n",
        " #Using upstream training's weights with real labels\n",
        "loss_upstream_real, accs_upstream_real_train, accs_upstream_real_test = setup_and_train_downstream(\n",
        "    train_upstream_loader, train_downstream_loader,\n",
        "    test_loader, acc_train_loader, acc_test_loader,\n",
        "    epochs=40, lr=0.001, step_size=(int)(40/3), num_examples_upstream=10000, \n",
        "    num_examples_downstream=10000, num_classes_upstream = 10, num_classes_downstream = 10,\n",
        "    is_downstream_random = False, batch_size = 256, num_outputs=10, upstream_model_path = \"pretrained_model_dict_upstream_1_2.pt\")\n",
        "\n",
        "\n",
        "loss_covariance_real, accs_covariance_real_train, accs_covariance_real_test = setup_and_train_downstream(\n",
        "    train_upstream_loader, train_downstream_loader,\n",
        "    test_loader, acc_train_loader, acc_test_loader,\n",
        "    epochs=40, lr=0.001, step_size=(int)(40/3), num_examples_upstream=10000, \n",
        "    num_examples_downstream=10000, num_classes_upstream = 10, num_classes_downstream = 10,\n",
        "    is_downstream_random = False, batch_size = 256, num_outputs=10, weights_are_covariance = True, cov_mat=Cov_y)\n",
        "\n",
        " #From scratch with real labels\n",
        "loss_scratch_real, accs_scratch_real_train, accs_scratch_real_test = setup_and_train_downstream(\n",
        "    train_upstream_loader, train_downstream_loader,\n",
        "    test_loader, acc_train_loader, acc_test_loader,\n",
        "    epochs=40, lr=0.001, step_size=(int)(40/3), num_examples_upstream=10000, \n",
        "    num_examples_downstream=10000, num_classes_upstream = 10, num_classes_downstream = 10,\n",
        "    is_downstream_random = False, batch_size = 256, num_outputs=10)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b3MiaibYuPT"
      },
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "orange = \"#fcba03\"\n",
        "light_blue = \"#03bafc\"\n",
        "black = \"#080504\"\n",
        "\n",
        "plt.plot(accs_scratch_real_train, linestyle=\"-\", color=black)\n",
        "plt.plot(accs_upstream_real_train, linestyle=\"-\", color=light_blue)\n",
        "plt.plot(accs_covariance_real_train, linestyle=\"-\", color=orange)\n",
        "plt.plot(accs_noscale_real_train, linestyle=\"--\", color=black)\n",
        "#plt.plot(accs_scratch_real_test, linestyle=\"--\", color=orange)\n",
        "#plt.plot(accs_upstream_real_test, linestyle=\"--\", color=light_blue)\n",
        "plt.legend([\"from scratch (train)\", \"pretrained (train)\", \"covariance\",\"no scale\"], loc =\"best\")\n",
        "plt.xlabel('# of iterations')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}