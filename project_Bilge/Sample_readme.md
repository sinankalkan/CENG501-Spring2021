# Paper title

This readme file is an outcome of the [CENG501 (Spring 2021)](http://kovan.ceng.metu.edu.tr/~sinan/DL/) project for reproducing a paper without an implementation. See [CENG501 (Spring 2021) Project List](https://github.com/sinankalkan/CENG501-Spring2021) for a complete list of all paper reproduction projects.

# 1. Introduction

This repository describes and contains unoffical implementation of the [paper](https://papers.nips.cc/paper/2020/hash/b3b43aeeacb258365cc69cdaf42a68af-Abstract.html) "Calibrating CNNs for Lifelong Learning" published in NeurIPS 2020. Paper presents a way to train CNNs for different tasks contiually without getting caugt in the catastrophic forgetting phenomenon. Aim of the project is to implement the models described in the paper (Provided notebooks contains some of the experiments in the paper, the uncovered experiments can be realized by the reader by using the provided notebooks as reference) and shift the attention of the reader to the subject of "Continual Learning".

## 1.1. Paper summary

In lifelong learning, if the network mostly forgets information from old tasks in training on a new task, the network is called plastic network; on the contrary, if the network focuses mostly on older tasks, then the network is called stable network. In plastic networks, catastrophic forgetting is observed such that as the new taks is learned, older tasks are being forgotten. To overcome this issue, the network can be trained from the starting as the new task comes and store all the parameters for current task and repeat this procedure for each new task. 


In this respect, the [paper](https://papers.nips.cc/paper/2020/hash/b3b43aeeacb258365cc69cdaf42a68af-Abstract.html) proposes a balanced network between a stable network and a plastic network through a calibration modules added after each convolutional layer of the base model. The base layers of the network extracts the common features on the initial task so these layers are frozen at the end of the training of the first task. On the other hand, the new tasks are learned via training only the calibration modules to make older tasks relevant to the current task, hence, number of parameters and computational cost do not increase drastically. In a sense, the proposed method promotes transfer learning via sequentially trained calibration modules and offers an efficient way to deal with the catastrophic forgetting. 

The proposed calibration modules meet the following criteria: (1) near-zero catstrophic forgetting (2) not drastically increased number of parameters and (3) forward transfer learning. The activation maps generated by the base model's convolutional layers are (re)-calibrated using 'Spatial Calibration Module (SCM)' followed by 'Channel-wise Calibration Module (CCM)'. The spatial calibration module learns weights to calibrate each point in the activation map using group-convolution layers and element-wise addition, while 


So the article offers a novel method that involves (re)calibrating the activation maps generated by the network trained on older tasks. Each calibration module consists of a spatial calibration module (SCM) followed by a channel-wise calibration module (CCM). The spatial calibration module learns weights to calibrate each point in the activation maps while the channel-wise calibration module learns weights to calibrate each channel of the activation maps. 

# 2. The method and my interpretation

## 2.1. The original method

Explain the original method.

## 2.2. My interpretation 

Explain the parts that were not clearly explained in the original paper and how you interpreted them.

# 3. Experiments and results

## 3.1. Experimental setup

Describe the setup of the original paper and whether you changed any settings.

## 3.2. Running the code

Explain your code & directory structure and how other people can run it.

## 3.3. Results

Present your results and compare them to the original paper. Please number your figures & tables as if this is a paper.

# 4. Conclusion

Discuss the paper in relation to the results in the paper and your results.

# 5. References

Provide your references here.

# Contact

Provide your names & email addresses and any other info with which people can contact you.
