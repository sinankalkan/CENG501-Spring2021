{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJCA6P4wBCe9"
   },
   "source": [
    "## 0. Importing Modules\n",
    "\n",
    "The necessary modules are imported here. Python 3 is used with PyTorch that works with CUDA. Final training is done on Kagle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-29T10:36:36.478965Z",
     "iopub.status.busy": "2021-07-29T10:36:36.478543Z",
     "iopub.status.idle": "2021-07-29T10:38:13.040899Z",
     "shell.execute_reply": "2021-07-29T10:38:13.039387Z",
     "shell.execute_reply.started": "2021-07-29T10:36:36.478904Z"
    },
    "id": "rjG9HJmiBCfG",
    "outputId": "0e98c311-6047-43d4-fccf-5e83206236b3"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np              # NumPy, for working with arrays/tensors\n",
    "import matplotlib.pyplot as plt # For plotting\n",
    "\n",
    "# PyTorch libraries:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "!pip install torchinfo\n",
    "from torchinfo import summary\n",
    "\n",
    "!wget https://github.com/huyvnphan/PyTorch_CIFAR10/archive/refs/tags/v3.0.1.zip\n",
    "!unzip v3.0.1.zip\n",
    "!pip install pytorch_lightning\n",
    "!cp -r PyTorch_CIFAR10-3.0.1/cifar10_models cifar10_models\n",
    "!python PyTorch_CIFAR10-3.0.1/train.py --download_weights 1\n",
    "!rm -rf PyTorch_CIFAR10-3.0.1/\n",
    "!rm -f v3.0.1.zip\n",
    "!rm -f state_dicts.zip\n",
    "\n",
    "# For pre-trained ResNet on CIFAR-10\n",
    "from cifar10_models import resnet\n",
    "\n",
    "# enable CUDA support if possible\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Cuda (GPU support) is available and enabled!\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Cuda (GPU support) is not available :(\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHKesFH4BCfP"
   },
   "source": [
    "## 1. Image Classification\n",
    "\n",
    "This part of the implementation deals with reproducing Image Classification related experimental results of the original paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nr_vq9WwBCfS"
   },
   "source": [
    "### 1.1. Loading the Dataset\n",
    "\n",
    "For image classification, paper used the ImageNet dataset on training process, but it is a huge dataset in size and number of images. This would increase the training time due to limited computing resources of mine. So, I tried to reproduce the improvements with `CIFAR-10` in the image classification part of experiments. \n",
    "Using PyTorch utilities we are able to load the `CIFAR-10` dataset easily, since it is considered a standard dataset for Deep Learning applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-29T10:38:13.044161Z",
     "iopub.status.busy": "2021-07-29T10:38:13.043669Z",
     "iopub.status.idle": "2021-07-29T10:38:13.053886Z",
     "shell.execute_reply": "2021-07-29T10:38:13.051616Z",
     "shell.execute_reply.started": "2021-07-29T10:38:13.044114Z"
    },
    "id": "V26v4IJvBCfU"
   },
   "outputs": [],
   "source": [
    "def load_cifar10():\n",
    "        \"\"\"\n",
    "        Uses torchvision.datasets.ImageNet to load ImageNet.\n",
    "        Downloads the dataset when necessary.\n",
    "        Returns 2 datasets for train and validation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # This part was resizing to 256 before, this was the cause of dramatically high trianing time and faulty results, it is noticed very late\n",
    "        # So epoch size had to be decreased to obtain some result\n",
    "        TF = transforms.Compose([\n",
    "        transforms.Resize(32),\n",
    "        transforms.CenterCrop(31),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        trainset = torchvision.datasets.CIFAR10('./datasets/CIFAR10/', train=True, download=True, transform=TF)\n",
    "        valset = torchvision.datasets.CIFAR10('./datasets/CIFAR10/', train=False, download=True, transform=TF)\n",
    "\n",
    "        return trainset, valset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LN6yeZZwBCfX"
   },
   "source": [
    "### 1.2. Define Modified ResNet Models\n",
    "\n",
    "The paper uses a wider ResNet for the experimental setup on image classification part. It is `4x` wider than the original ResNet implementations. Imitating PyTorch's `2x` wider ResNet implementation I was able to acquire the model. Also, PyTorch Hooks are used to read intermadiate results between blocks.\n",
    "\n",
    "In general, paper describes layer-wise output matching is achived with appropriately sized linear transformations. But for convolutional layers, these cannot be merged after the process. ResNet output channels are same between layers on reference implementation of PyTorch anyway. So, no linear transformation is needed for this case.\n",
    "\n",
    "The code below is taken and modified from the ResNet implementation that is trained on CIFAR-10 where needed. The original repo for those networks can be reached from [here](https://github.com/huyvnphan/PyTorch_CIFAR10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-29T10:38:13.059446Z",
     "iopub.status.busy": "2021-07-29T10:38:13.058924Z",
     "iopub.status.idle": "2021-07-29T10:38:13.076359Z",
     "shell.execute_reply": "2021-07-29T10:38:13.075184Z",
     "shell.execute_reply.started": "2021-07-29T10:38:13.059417Z"
    },
    "id": "MhaPOKEpBCfZ"
   },
   "outputs": [],
   "source": [
    "def wide_resnet50_4(**kwargs):\n",
    "    kwargs['width_per_group'] = 64 * 4\n",
    "    model = resnet.ResNet(models.resnet.Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    return model\n",
    "\n",
    "def hook(module, input, output):\n",
    "    setattr(module, \"_value_hook\", output)\n",
    "\n",
    "# Override forward-pass and initialization\n",
    "class ResNetMod(resnet.ResNet):\n",
    "    def __init__(self, block, layers, num_classes=10, zero_init_residual=False, group=1, \n",
    "                 width_per_group=64, replace_stride_with_dilation=None, norm_layer=None):\n",
    "        super(ResNetMod, self).__init__(block, layers, num_classes, zero_init_residual, group, \n",
    "                 width_per_group, replace_stride_with_dilation, norm_layer)   \n",
    "        self.layer_idx = 5\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: input tensor\n",
    "        layer_idx: idx of the layer targetted, from 1 to 4\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        if (self.layer_idx >= 1):\n",
    "            x = self.layer1(x)\n",
    "            \n",
    "        if (self.layer_idx >= 2):\n",
    "            x = self.layer2(x)\n",
    "        \n",
    "        if (self.layer_idx >= 3):\n",
    "            x = self.layer3(x)\n",
    "        if (self.layer_idx >= 4):\n",
    "            x = self.layer4(x)\n",
    "        \n",
    "        if (self.layer_idx == 5):\n",
    "            x = self.avgpool(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def resnetmod50(**kwargs):\n",
    "    kwargs['width_per_group'] = 64 // 2\n",
    "    model = ResNetMod(models.resnet.Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsr6YUfPBCfb"
   },
   "source": [
    "### 1.3. Create ResNet Models\n",
    "\n",
    "We need to create baseline ResNet50, Wide Teacher ResNet50 and modified ResNet50 for layerwise imitation training. Also, hooks are registered to teacher model to acquire intermediate forward outputs between layers.\n",
    "\n",
    "For baseline, a pretrained ResNet50 with `1/2` width cannot be found, so student will be check against teacher. But, for ensuring there is no bug and teacher parameters do not get updated, a baseline is also created. To gain time pretrained models are used for teacher and baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-29T11:14:36.503325Z",
     "iopub.status.busy": "2021-07-29T11:14:36.502944Z",
     "iopub.status.idle": "2021-07-29T11:14:38.326757Z",
     "shell.execute_reply": "2021-07-29T11:14:38.325598Z",
     "shell.execute_reply.started": "2021-07-29T11:14:36.50329Z"
    },
    "id": "HtbnzMgXBCfe"
   },
   "outputs": [],
   "source": [
    "teacher = resnet.resnet50(pretrained=True)\n",
    "student = resnetmod50()\n",
    "baseline = resnet.resnet50(pretrained=True)\n",
    "\n",
    "for n, m in teacher.named_modules():\n",
    "    match = re.search('layer[1234]$', n)\n",
    "    if match:\n",
    "        m.register_forward_hook(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-29T10:38:14.539659Z",
     "iopub.status.busy": "2021-07-29T10:38:14.539214Z",
     "iopub.status.idle": "2021-07-29T10:38:14.57218Z",
     "shell.execute_reply": "2021-07-29T10:38:14.570664Z",
     "shell.execute_reply.started": "2021-07-29T10:38:14.539615Z"
    },
    "id": "CQZotnkaBCfg",
    "outputId": "0f792eb8-3562-49d6-a984-dba4769f99ab"
   },
   "outputs": [],
   "source": [
    "print('Teacher')\n",
    "summary(teacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-29T10:38:14.574587Z",
     "iopub.status.busy": "2021-07-29T10:38:14.57397Z",
     "iopub.status.idle": "2021-07-29T10:38:14.6095Z",
     "shell.execute_reply": "2021-07-29T10:38:14.608275Z",
     "shell.execute_reply.started": "2021-07-29T10:38:14.574543Z"
    },
    "id": "Mh668paLBCfh",
    "outputId": "7f3523a2-58ea-475a-913e-4d0a6144cd79"
   },
   "outputs": [],
   "source": [
    "print('Student')\n",
    "summary(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-29T10:38:14.612793Z",
     "iopub.status.busy": "2021-07-29T10:38:14.612052Z",
     "iopub.status.idle": "2021-07-29T10:38:14.643098Z",
     "shell.execute_reply": "2021-07-29T10:38:14.64178Z",
     "shell.execute_reply.started": "2021-07-29T10:38:14.612749Z"
    },
    "id": "jcEfBo5jBCfj",
    "outputId": "6333721d-d5cd-4df7-ac4b-dfb3a8f66fd1"
   },
   "outputs": [],
   "source": [
    "print('Baseline')\n",
    "summary(baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yn0v7p9eBCfk"
   },
   "source": [
    "### 1.4. Training Process\n",
    "\n",
    "We need to create a training method and apply training using `CIFAR-10` dataset. I made use of `CENG501` assignments for this part.\n",
    "\n",
    "Batch Normalization decay is `0.9` as a default to my understanding. It is `0.1` as default, but substracted from `1` in the source code. Also, it is phrased as `momentum` in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-29T11:14:44.821678Z",
     "iopub.status.busy": "2021-07-29T11:14:44.821321Z",
     "iopub.status.idle": "2021-07-29T11:51:41.558748Z",
     "shell.execute_reply": "2021-07-29T11:51:41.557321Z",
     "shell.execute_reply.started": "2021-07-29T11:14:44.821646Z"
    },
    "id": "mhiiUViGBCfk",
    "outputId": "733202ae-5fa1-4c5b-ae74-b0a0d19b31b1"
   },
   "outputs": [],
   "source": [
    "# epoch assumed to be starting from 0, ending at 89\n",
    "def vanilla_lr(epoch):\n",
    "    if (epoch < 5):\n",
    "        return 0.025 * epoch # linearly increasing from 0 to 0.1 at epochs from 0 to 4\n",
    "    if (epoch < 29):\n",
    "        return 0.1\n",
    "    if (epoch < 59):\n",
    "        return 0.01  # reduced 10x at 30\n",
    "    if (epoch < 79):\n",
    "        return 0.001 # reduced 10x at 60\n",
    "    return 0.0001    # reduced 10x at 80\n",
    "\n",
    "def vanilla_train(model, dataloader, multibatch=1, verbose=True):\n",
    "    \"\"\"\n",
    "    Returns: the loss history.\n",
    "    \"\"\"\n",
    "    loss_history = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(90):\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=vanilla_lr(epoch), momentum=0.9, weight_decay=1e-4)\n",
    "        for i, data in enumerate(dataloader, 0):    \n",
    "            print('Training, batch:', i)\n",
    "            # our batch:\n",
    "            inputs, truth = data\n",
    "            inputs = inputs.to(device)\n",
    "            truth = truth.to(device)\n",
    "\n",
    "            # zero the gradients as PyTorch accumulates them \n",
    "            # every 4 batch since original paper uses batch size of 256\n",
    "            if (i + 1) % multibatch == 0:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # obtain the scores\n",
    "            outputs = model(inputs)\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs.to(device), truth)\n",
    "\n",
    "            # backpropagate\n",
    "            loss.backward()\n",
    "\n",
    "            # update the weights\n",
    "            # every 4 batch since original paper uses batch size of 256\n",
    "            if (i + 1) % multibatch == 0:\n",
    "                optimizer.step()\n",
    "\n",
    "            loss_history.append(loss.item())\n",
    "\n",
    "        if verbose: print(f'Epoch {epoch + 1} / {epochs}: avg. loss of last 5 iterations {np.sum(loss_history[:-6:-1])/5}')\n",
    "\n",
    "    return loss_history\n",
    "\n",
    "def train_epoch(model, teacher, criterion, optimizer, dataloader, loss_history, multibatch=1):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('At step: ', i + 1)\n",
    "        # our batch:\n",
    "        inputs, _ = data\n",
    "        inputs = inputs.to(device)\n",
    "        # No gradient calculation is needed on teacher, reduce memory footprint\n",
    "        with torch.no_grad():\n",
    "            truth = teacher(inputs)\n",
    "            if model.layer_idx < 5:\n",
    "                for n, m in teacher.named_modules():\n",
    "                    if n == 'layer' + str(model.layer_idx):\n",
    "                        truth = m._value_hook\n",
    "                        break\n",
    "            truth = truth.cuda(non_blocking=True)\n",
    "                \n",
    "        # zero the gradients as PyTorch accumulates them \n",
    "        # every 4 batch since original paper uses batch size of 256\n",
    "        if (i + 1) % multibatch == 0:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # obtain the scores\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate loss\n",
    "        if model.layer_idx < 5:\n",
    "            loss = criterion(outputs.to(device), truth)\n",
    "        else:\n",
    "            loss = criterion(F.log_softmax(outputs.to(device), dim=1), F.log_softmax(truth, dim=1)) # dim=1 since log_prob should be calculated for each batch\n",
    "        \n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "\n",
    "        # update the weights\n",
    "        # every 4 batch since original paper uses batch size of 256\n",
    "        if (i + 1) % multibatch == 0:\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "def win_train(model, teacher, dataloader, multibatch=1, verbose=True):\n",
    "    \"\"\"\n",
    "    Returns: the loss history\n",
    "    \"\"\"\n",
    "    loss_history = []\n",
    "    criterion_s1 = nn.MSELoss()\n",
    "    criterion_s2 = nn.KLDivLoss(reduction='batchmean', log_target=True) # We are converting teacher model output to log probabilities also\n",
    "    criterion_s3 = nn.CrossEntropyLoss()\n",
    "    optimizer_s2 = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    optimizer_s3 = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "    scheduler_s2 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_s2, 4, eta_min=0, last_epoch=-1)\n",
    "    \n",
    "    for l in range(4): # for each layer block\n",
    "        optimizer_s1 = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "        scheduler_s1 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_s1, 2, eta_min=0, last_epoch=-1)\n",
    "        model.layer_idx = l + 1\n",
    "        for epoch in range(2):\n",
    "            train_epoch(model, teacher, criterion_s1, optimizer_s1, dataloader, loss_history, multibatch=multibatch)\n",
    "            scheduler_s1.step()\n",
    "            if verbose: \n",
    "                print(f'Epoch {epoch + 1} / {2}: avg. loss of last 5 iterations {np.sum(loss_history[:-6:-1])/5}')\n",
    "    \n",
    "    model.layer_idx = 5\n",
    "    \n",
    "    for epoch in range(4):\n",
    "        train_epoch(model, teacher, criterion_s2, optimizer_s2, dataloader, loss_history, multibatch=multibatch)\n",
    "        scheduler_s2.step()\n",
    "        if verbose: \n",
    "            print(f'Epoch {epoch + 1} / {4}: avg. loss of last 5 iterations {np.sum(loss_history[:-6:-1])/5}')\n",
    "\n",
    "    for epoch in range(4):\n",
    "        for i, data in enumerate(dataloader, 0):    \n",
    "            if (i + 1) % 100 == 0:\n",
    "                print('At step: ', i + 1)\n",
    "            # our batch:\n",
    "            inputs, truth = data\n",
    "            inputs = inputs.to(device)\n",
    "            truth = truth.to(device)\n",
    "\n",
    "            # zero the gradients as PyTorch accumulates them \n",
    "            # every 4 batch since original paper uses batch size of 256\n",
    "            if (i + 1) % multibatch == 0:\n",
    "                optimizer_s3.zero_grad()\n",
    "\n",
    "            # obtain the scores\n",
    "            outputs = model(inputs)\n",
    "            # Calculate loss\n",
    "            loss = criterion_s3(outputs.to(device), truth)\n",
    "\n",
    "            # backpropagate\n",
    "            loss.backward()\n",
    "\n",
    "            # update the weights\n",
    "            # every 4 batch since original paper uses batch size of 256\n",
    "            if (i + 1) % multibatch == 0:\n",
    "                optimizer_s3.step()\n",
    "\n",
    "            loss_history.append(loss.item())\n",
    "\n",
    "        if verbose: \n",
    "            print(f'Epoch {epoch + 1} / {4}: avg. loss of last 5 iterations {np.sum(loss_history[:-6:-1])/5}')\n",
    "\n",
    "    return loss_history\n",
    "\n",
    "# batch size used in the paper for Vanilla Training\n",
    "batch_size = 32\n",
    "# loaders for datasets\n",
    "train_dataset, val_dataset = load_cifar10()\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "eval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "model = student.to(device)\n",
    "teacher = teacher.to(device)\n",
    "loss_history = win_train(model, teacher, train_loader, multibatch=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-29T10:38:52.795679Z",
     "iopub.status.idle": "2021-07-29T10:38:52.796717Z"
    },
    "id": "muis0T-IBCfl"
   },
   "source": [
    "### 1.4. Quantitative Analysis\n",
    "\n",
    "Again, using the code from the assignments, I plotted the loss function.\n",
    "\n",
    "Actually longer training times were needed. But, initially a crutual mistake of resizing the input image wrongly to 256 since this ResNet is modified and works with `CIFAR-10` images natively, increased the training time. Since I noticed it late, I had to produce a result at least, despite the training time is shortened with not resizing to 256, remaining time were limited. Still, loss plot clearly indicates layerwise imitation is highly efficient since Kullback-Leibler Divergence stage is observed to be decreasing slower than the first stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-29T11:51:41.561763Z",
     "iopub.status.busy": "2021-07-29T11:51:41.561251Z",
     "iopub.status.idle": "2021-07-29T11:51:42.008313Z",
     "shell.execute_reply": "2021-07-29T11:51:42.007054Z",
     "shell.execute_reply.started": "2021-07-29T11:51:41.561713Z"
    },
    "id": "4n-gMKXUBCfm"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Accuracy Analysis\n",
    "\n",
    "Using the code from PAs of the lecture and modifying it slightly I was able to acquire the Top-1 accuracy values.\n",
    "\n",
    "Unfortunately, accuracy of generated student network is not high enough the draw conclusions. Still, despite 2 epochs per layer, 4 epochs of output imitation and 4 epochs of finetuning is pretty low number of epochs in total for a newly initialized network. And considering the total epoch count, good results are obtained.\n",
    "\n",
    "There seems to be a small difference between teacher and baseline despite `no_grad` is used for teacher at training. But it is very small difference and could be related to used implementation of ResNet for `CIFAR-10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-29T11:51:42.011682Z",
     "iopub.status.busy": "2021-07-29T11:51:42.010931Z",
     "iopub.status.idle": "2021-07-29T11:52:15.395453Z",
     "shell.execute_reply": "2021-07-29T11:52:15.393283Z",
     "shell.execute_reply.started": "2021-07-29T11:51:42.011633Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_top1(model, testloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            if (i + 1) % 50 == 0:\n",
    "                print('Eval at: ', i + 1)\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    return (100 * correct / total)\n",
    "\n",
    "student.eval()\n",
    "teacher.eval()\n",
    "baseline.eval()\n",
    "\n",
    "accuracy_student = calc_top1(student, eval_loader)\n",
    "print('Accuracy of the student: %d %%' % accuracy_student)\n",
    "accuracy_teacher = calc_top1(teacher, eval_loader)\n",
    "print('Accuracy of the teacher: %d %%' % accuracy_teacher)\n",
    "accuracy_baseline = calc_top1(baseline, eval_loader)\n",
    "print('Accuracy of the baseline: %d %%' % accuracy_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
