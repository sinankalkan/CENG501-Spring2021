{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 775,
     "status": "ok",
     "timestamp": 1626643190062,
     "user": {
      "displayName": "Ertuğrul Güngör",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicvIzQgyHfsqD4h979_wDnn11aI-XtLnxmnC_8=s64",
      "userId": "06165302180630202587"
     },
     "user_tz": -180
    },
    "id": "YBHMtNLN3GnS"
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class Identity(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Identity, self).__init__()\n",
    "      \n",
    "  def forward(self, x):\n",
    "    return x\n",
    "\n",
    "def get_vgg_model(num_outputs, init_scale):\n",
    "  vgg16 = models.vgg16()\n",
    "\n",
    "  # Remove avgpool since it's removed in the paper\n",
    "  vgg16.avgpool = Identity()\n",
    "\n",
    "  # In the paper, VGG16 model's 2 last FC layers are removed\n",
    "  # Output of last max pool layer is [batch_size,512,7,7] for 224x224 image\n",
    "  #vgg16.classifier = nn.Linear(512*7*7, num_outputs)\n",
    "\n",
    "  # For 32x32 image\n",
    "  vgg16.classifier = nn.Linear(512, num_outputs)\n",
    "\n",
    "  #for module in vgg16.modules():\n",
    "  #  if type(module) == nn.modules.pooling.MaxPool2d:\n",
    "  #    module = nn.MaxPool2d(2, stride=2, padding='same')\n",
    "    #if \"MaxPool2d\" is in str(type(module)):\n",
    "    #  print(\"module is : {}\".format(type(module)))\n",
    "  # Change weight initialization with a nested function\n",
    "  def init_weights(param):\n",
    "    if (type(param) == nn.Conv2d or type(param) == nn.Linear):\n",
    "      # Parameters are initialized with He initialization in the paper.\n",
    "\n",
    "      torch.nn.init.kaiming_normal_(param.weight, mode='fan_out', nonlinearity='relu')\n",
    "      # # ??\n",
    "      # param.weight = torch.nn.Parameter(param.weight * init_scale)\n",
    "\n",
    "      # print(type(param.weight))\n",
    "      # param.weight.data.fill_(param.weight * init_scale)\n",
    "      # torch.full(param.weight.size(), 3.141592)\n",
    "\n",
    "      # Default value of biases are already 0, just to be more verbose\n",
    "      # param.bias.data.fill_(0)\n",
    "\n",
    "      # print(param)\n",
    "      # print(type(param))\n",
    "\n",
    "  vgg16.apply(init_weights)\n",
    "  return vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7907,
     "status": "ok",
     "timestamp": 1626643197966,
     "user": {
      "displayName": "Ertuğrul Güngör",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicvIzQgyHfsqD4h979_wDnn11aI-XtLnxmnC_8=s64",
      "userId": "06165302180630202587"
     },
     "user_tz": -180
    },
    "id": "JZcqlcdLDtfM",
    "outputId": "7453edbd-e3ab-4b76-a8da-ed9b4634bdc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /opt/conda/include/python3.8/UNKNOWN\n",
      "sysconfig: /opt/conda/include/python3.8\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "Requirement already satisfied: torchinfo in /opt/conda/lib/python3.8/site-packages (1.5.2)\n",
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /opt/conda/include/python3.8/UNKNOWN\n",
      "sysconfig: /opt/conda/include/python3.8\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "VGG                                      --                        --\n",
      "├─Sequential: 1-1                        [1, 512, 1, 1]            --\n",
      "│    └─Conv2d: 2-1                       [1, 64, 32, 32]           1,792\n",
      "│    └─ReLU: 2-2                         [1, 64, 32, 32]           --\n",
      "│    └─Conv2d: 2-3                       [1, 64, 32, 32]           36,928\n",
      "│    └─ReLU: 2-4                         [1, 64, 32, 32]           --\n",
      "│    └─MaxPool2d: 2-5                    [1, 64, 16, 16]           --\n",
      "│    └─Conv2d: 2-6                       [1, 128, 16, 16]          73,856\n",
      "│    └─ReLU: 2-7                         [1, 128, 16, 16]          --\n",
      "│    └─Conv2d: 2-8                       [1, 128, 16, 16]          147,584\n",
      "│    └─ReLU: 2-9                         [1, 128, 16, 16]          --\n",
      "│    └─MaxPool2d: 2-10                   [1, 128, 8, 8]            --\n",
      "│    └─Conv2d: 2-11                      [1, 256, 8, 8]            295,168\n",
      "│    └─ReLU: 2-12                        [1, 256, 8, 8]            --\n",
      "│    └─Conv2d: 2-13                      [1, 256, 8, 8]            590,080\n",
      "│    └─ReLU: 2-14                        [1, 256, 8, 8]            --\n",
      "│    └─Conv2d: 2-15                      [1, 256, 8, 8]            590,080\n",
      "│    └─ReLU: 2-16                        [1, 256, 8, 8]            --\n",
      "│    └─MaxPool2d: 2-17                   [1, 256, 4, 4]            --\n",
      "│    └─Conv2d: 2-18                      [1, 512, 4, 4]            1,180,160\n",
      "│    └─ReLU: 2-19                        [1, 512, 4, 4]            --\n",
      "│    └─Conv2d: 2-20                      [1, 512, 4, 4]            2,359,808\n",
      "│    └─ReLU: 2-21                        [1, 512, 4, 4]            --\n",
      "│    └─Conv2d: 2-22                      [1, 512, 4, 4]            2,359,808\n",
      "│    └─ReLU: 2-23                        [1, 512, 4, 4]            --\n",
      "│    └─MaxPool2d: 2-24                   [1, 512, 2, 2]            --\n",
      "│    └─Conv2d: 2-25                      [1, 512, 2, 2]            2,359,808\n",
      "│    └─ReLU: 2-26                        [1, 512, 2, 2]            --\n",
      "│    └─Conv2d: 2-27                      [1, 512, 2, 2]            2,359,808\n",
      "│    └─ReLU: 2-28                        [1, 512, 2, 2]            --\n",
      "│    └─Conv2d: 2-29                      [1, 512, 2, 2]            2,359,808\n",
      "│    └─ReLU: 2-30                        [1, 512, 2, 2]            --\n",
      "│    └─MaxPool2d: 2-31                   [1, 512, 1, 1]            --\n",
      "├─Identity: 1-2                          [1, 512, 1, 1]            --\n",
      "├─Linear: 1-3                            [1, 13]                   6,669\n",
      "==========================================================================================\n",
      "Total params: 14,721,357\n",
      "Trainable params: 14,721,357\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 313.48\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 2.21\n",
      "Params size (MB): 58.89\n",
      "Estimated Total Size (MB): 61.11\n",
      "==========================================================================================\n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): Identity()\n",
      "  (classifier): Linear(in_features=512, out_features=13, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# install and import the torchinfo library\n",
    "!pip install torchinfo\n",
    "from torchinfo import summary\n",
    "\n",
    "vgg16 = get_vgg_model(13, 1)\n",
    "# batch_size = 256\n",
    "print(summary(vgg16, input_size=(1, 3, 32, 32)))\n",
    "#print(summary(vgg16, input_size=(1, 3, 224, 224)))\n",
    "\n",
    "# summary(vgg16, input_size=(batch_size, 3, 224, 224))\n",
    "# summary(models.vgg16(), input_size=(1, 3, 32, 32))\n",
    "print(vgg16)\n",
    "#vgg16 = models.vgg16()\n",
    "#print(vgg16)\n",
    "\n",
    "\n",
    "# print(vgg16)\n",
    "\n",
    "# vgg16.apply(init_weights)\n",
    "# print(\"dummy\")\n",
    "# for child in vgg16.named_children():\n",
    "#   # print(\"x\")\n",
    "#   print(child)\n",
    "\n",
    "# for name, param in vgg16.named_parameters():\n",
    "#   print(name)\n",
    "#   print(param.size())\n",
    "#   print(param[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1626643197966,
     "user": {
      "displayName": "Ertuğrul Güngör",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicvIzQgyHfsqD4h979_wDnn11aI-XtLnxmnC_8=s64",
      "userId": "06165302180630202587"
     },
     "user_tz": -180
    },
    "id": "6N3MbPVav4wt"
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "def get_cifar10_sets(num_examples_upstream, num_examples_downstream, num_classes_upstream, \\\n",
    "                     num_classes_downstream, is_downstream_random, batch_size):\n",
    "  # @TODO: Check if it's the same as paper\n",
    "  TF = transforms.Compose([\n",
    "      #transforms.Resize(256),\n",
    "      #transforms.CenterCrop(224),\n",
    "      transforms.ToTensor(),\n",
    "      # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "  ])\n",
    "\n",
    "  # num_examples_upstream = 20000\n",
    "  # num_examples_downstream = 10000\n",
    "  # num_classes_upstream = 30\n",
    "  # num_classes_downstream = 15\n",
    "  # is_downstream_random = False\n",
    "\n",
    "  trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=TF)\n",
    "  # test and transform??\n",
    "  testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=TF)\n",
    "\n",
    "  # Create random indices for splitting upstream and downstream tasks\n",
    "  indices = np.arange(0,len(trainset))\n",
    "  np.random.seed(12345)\n",
    "  np.random.shuffle(indices)\n",
    "  indices = indices.tolist()\n",
    "\n",
    "  # Make upstream labels random\n",
    "  temp = np.array(trainset.targets)\n",
    "  temp[indices[:num_examples_upstream]] = np.random.randint(num_classes_upstream, size=num_examples_upstream)\n",
    "\n",
    "  # If the downstream task uses random labels\n",
    "  if is_downstream_random:\n",
    "    temp[indices[-num_examples_downstream:]] = np.random.randint(num_classes_downstream, size=num_examples_downstream).tolist()\n",
    "\n",
    "  trainset.targets = [int(label) for label in temp]\n",
    "\n",
    "  # Build dataloaders for upstream and downstream tasks\n",
    "  train_upstream_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=False,\n",
    "                                                sampler=torch.utils.data.SubsetRandomSampler(indices[:num_examples_upstream]))\n",
    "\n",
    "  train_downstream_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=False,\n",
    "                                                sampler=torch.utils.data.SubsetRandomSampler(indices[-num_examples_downstream:]))\n",
    "\n",
    "  test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "  # To check the random labels\n",
    "  '''\n",
    "  u_batch = next(iter(train_upstream_loader))\n",
    "  d_batch = next(iter(train_downstream_loader))\n",
    "\n",
    "  u_labels = u_batch[1]\n",
    "  print(u_labels)\n",
    "  d_labels = d_batch[1]\n",
    "  print(d_labels)\n",
    "  '''\n",
    "  return (train_upstream_loader, train_downstream_loader, test_loader)\n",
    "\n",
    "\n",
    "def get_cifar10_orig():\n",
    "  transform = transforms.Compose(\n",
    "      [transforms.ToTensor(),\n",
    "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "  batch_size = 256\n",
    "\n",
    "\n",
    "  trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                          download=False, transform=transform)\n",
    "  indices = np.arange(0,len(trainset))\n",
    "  trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=2, \n",
    "                                            sampler=torch.utils.data.SubsetRandomSampler(indices[:5000]))\n",
    "\n",
    "  testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=False, transform=transform)\n",
    "  testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "  return trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1626643197967,
     "user": {
      "displayName": "Ertuğrul Güngör",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicvIzQgyHfsqD4h979_wDnn11aI-XtLnxmnC_8=s64",
      "userId": "06165302180630202587"
     },
     "user_tz": -180
    },
    "id": "oxh4UOW0g5rq"
   },
   "outputs": [],
   "source": [
    "def accuracy(model, data=\"train\"):\n",
    "  correct = 0\n",
    "  total = 0\n",
    "\n",
    "  if data== \"train\":\n",
    "    _, dataloader, __ = get_cifar10_sets(\n",
    "        num_examples_upstream = 5000,\n",
    "        num_examples_downstream = 20000,\n",
    "        num_classes_upstream = 5,\n",
    "        num_classes_downstream = 10,  \n",
    "        is_downstream_random = False,\n",
    "        batch_size = 20000\n",
    "    )\n",
    "  elif data == \"test\":\n",
    "    _, __, dataloader = get_cifar10_sets(\n",
    "        num_examples_upstream = 5000,\n",
    "        num_examples_downstream = 20000,\n",
    "        num_classes_upstream = 5,\n",
    "        num_classes_downstream = 10,  \n",
    "        is_downstream_random = False,\n",
    "        batch_size = 10000\n",
    "    )\n",
    "  else:\n",
    "    print(\"choose a proper data mode\")\n",
    "    return\n",
    "  with torch.no_grad():\n",
    "    for data in dataloader:\n",
    "      inputs, labels = data\n",
    "      outputs = model(inputs)\n",
    "      _, predicted = torch.max(outputs.data, 1)\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted == labels).sum().item()\n",
    "\n",
    "  return (correct / total)\n",
    "\n",
    "def train(model, criterion, optimizer, epochs, dataloader, device, scheduler=None, acc_mode=\"none\", verbose=True):\n",
    "  loss_history = []\n",
    "  train_accuracies = []\n",
    "  test_accuracies = []\n",
    "  for epoch in range(epochs):\n",
    "    count = 0\n",
    "    for i, data in enumerate(dataloader, 0):     \n",
    "      # print(\"{}.th batch\".format(i))\n",
    "      # Our batch:\n",
    "      inputs, labels = data\n",
    "      inputs = inputs.to(device)\n",
    "      labels = labels.to(device)\n",
    "      # print(\"batching!\")\n",
    "\n",
    "      # zero the gradients as PyTorch accumulates them\n",
    "      optimizer.zero_grad()\n",
    "      # print(\"zero grad!\")\n",
    "\n",
    "      # Obtain the scores\n",
    "      outputs = model(inputs)\n",
    "      # print(\"feedforward!\")\n",
    "\n",
    "      # Calculate loss\n",
    "      loss = criterion(outputs.to(device), labels)\n",
    "      # print(\"loss!\")\n",
    "\n",
    "      # Backpropagate\n",
    "      loss.backward()\n",
    "      # print(\"backward!\")\n",
    "\n",
    "      # Update the weights\n",
    "      optimizer.step()\n",
    "      # print(\"update!\")\n",
    "      \n",
    "      loss_history.append(loss.item())\n",
    "      count += 1\n",
    "      #print(\"mini batch counter: {}\".format(count))\n",
    "      if (count % 15 == 0):\n",
    "          if acc_mode==\"both\":\n",
    "            train_acc = accuracy(model, data=\"train\")\n",
    "            train_accuracies.append(train_acc)\n",
    "            test_acc = accuracy(model, data=\"test\")\n",
    "            test_accuracies.append(test_acc)\n",
    "          elif acc_mode==\"train\":\n",
    "            train_acc = accuracy(model, data=\"train\")\n",
    "            train_accuracies.append(train_acc)\n",
    "          elif acc_mode==\"test\":\n",
    "            test_acc = accuracy(model, data=\"test\")\n",
    "            test_accuracies.append(test_acc)\n",
    "          else:\n",
    "            print(\"invalid accuracy option\")\n",
    "    \n",
    "    if verbose: \n",
    "        print(f'Epoch {epoch} / {epochs}: avg. loss of last 5 iterations {np.sum(loss_history[:-6:-1])/5}')\n",
    "        if acc_mode==\"train\" or acc_mode ==\"both\":\n",
    "          print(f'Epoch {epoch} / {epochs}: Last train acc:{train_acc}')\n",
    "        if acc_mode==\"test\" or acc_mode ==\"both\":\n",
    "          print(f'Epoch {epoch} / {epochs}: Last test acc:{test_acc}')\n",
    "\n",
    "    if scheduler is not None:\n",
    "      scheduler.step()\n",
    "      # Print Learning Rate\n",
    "      print('Epoch:', epoch,'LR:', scheduler.get_last_lr())\n",
    "\n",
    "  if acc_mode==\"both\":\n",
    "    return loss_history, train_accuracies, test_accuracies\n",
    "  elif acc_mode==\"train\":\n",
    "    return loss_history, train_accuracies\n",
    "  elif acc_mode==\"test\":\n",
    "    return loss_history, test_accuracies\n",
    "  return loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1626643197968,
     "user": {
      "displayName": "Ertuğrul Güngör",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicvIzQgyHfsqD4h979_wDnn11aI-XtLnxmnC_8=s64",
      "userId": "06165302180630202587"
     },
     "user_tz": -180
    },
    "id": "3hhJodbVnyOi",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def get_training_components(learnable_params, init_lr, step_size):\n",
    "  '''\n",
    "  Function to prepare components of the training. These are all the same\n",
    "  throughout the experiments made.\n",
    "  '''\n",
    "  # The problem always be a image classification\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  # Which parameters to be updated depends on if the training is done from scratch\n",
    "  # or with pre-trained model.\n",
    "  optimizer = optim.SGD(learnable_params, lr=init_lr, momentum=0.9)\n",
    "  # Step size depends on the # of epoch, gamma is contant\n",
    "  scheduler = StepLR(optimizer, step_size=step_size, gamma=1/3)\n",
    "  return (criterion, optimizer, scheduler)\n",
    "\n",
    "def get_device():\n",
    "  if torch.cuda.is_available():\n",
    "    print(\"Cuda (GPU support) is available and enabled!\")\n",
    "    device = torch.device(\"cuda\")\n",
    "  else:\n",
    "    print(\"Cuda (GPU support) is not available :(\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "def get_learnable_parameters(model):\n",
    "  params_to_update = []\n",
    "  for name, param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "      #print(name)\n",
    "      params_to_update.append(param)\n",
    "  return params_to_update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1626643197968,
     "user": {
      "displayName": "Ertuğrul Güngör",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicvIzQgyHfsqD4h979_wDnn11aI-XtLnxmnC_8=s64",
      "userId": "06165302180630202587"
     },
     "user_tz": -180
    },
    "id": "Udp6bdgUi8A4",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def train_upstream():\n",
    "#   train_upstream_loader, train_downstream_loader, test_loader = get_cifar10_sets(\n",
    "#     num_examples_upstream = 5000,\n",
    "#     num_examples_downstream = 20000,\n",
    "#     num_classes_upstream = 5,\n",
    "#     num_classes_downstream = 5,\n",
    "#     is_downstream_random = False,\n",
    "#     batch_size = 256\n",
    "#   )\n",
    "\n",
    "#   model = get_vgg_model(5, 0.526)\n",
    "#   device = get_device()\n",
    "#   model = model.to(device)\n",
    "#   model.train()\n",
    "#   epochs = 30\n",
    "\n",
    "#   learnable_parameters = get_learnable_parameters(model)\n",
    "#   criterion, optimizer, scheduler = get_training_components(learnable_parameters, 0.01, (int)(epochs/3))\n",
    "#   print(\"before training\")\n",
    "#   loss_history = train(model, criterion, optimizer, epochs, train_upstream_loader, device, scheduler)\n",
    "\n",
    "#   torch.save(model.state_dict(), \"/content/pretrained_model_dict.pt\")\n",
    "#   torch.save(model, \"/content/pretrained_model.pt\")\n",
    "\n",
    "#   return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "kEy0gUI0n6_p",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "bfb71388-9147-4cde-cda6-e32f82f6753f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "before training\n",
      "Cuda (GPU support) is available and enabled!\n",
      "Epoch 0 / 120: avg. loss of last 5 iterations 1.6116274118423461\n",
      "Epoch: 0 LR: [0.001]\n",
      "Epoch 1 / 120: avg. loss of last 5 iterations 1.6148926734924316\n",
      "Epoch: 1 LR: [0.001]\n",
      "Epoch 2 / 120: avg. loss of last 5 iterations 1.610210108757019\n",
      "Epoch: 2 LR: [0.001]\n",
      "Epoch 3 / 120: avg. loss of last 5 iterations 1.602347183227539\n",
      "Epoch: 3 LR: [0.001]\n",
      "Epoch 4 / 120: avg. loss of last 5 iterations 1.6011620998382567\n",
      "Epoch: 4 LR: [0.001]\n",
      "Epoch 5 / 120: avg. loss of last 5 iterations 1.6027331590652465\n",
      "Epoch: 5 LR: [0.001]\n",
      "Epoch 6 / 120: avg. loss of last 5 iterations 1.5997007369995118\n",
      "Epoch: 6 LR: [0.001]\n",
      "Epoch 7 / 120: avg. loss of last 5 iterations 1.599612045288086\n",
      "Epoch: 7 LR: [0.001]\n",
      "Epoch 8 / 120: avg. loss of last 5 iterations 1.5869549036026\n",
      "Epoch: 8 LR: [0.001]\n",
      "Epoch 9 / 120: avg. loss of last 5 iterations 1.586445116996765\n",
      "Epoch: 9 LR: [0.001]\n",
      "Epoch 10 / 120: avg. loss of last 5 iterations 1.585192584991455\n",
      "Epoch: 10 LR: [0.001]\n",
      "Epoch 11 / 120: avg. loss of last 5 iterations 1.5792682409286498\n",
      "Epoch: 11 LR: [0.001]\n",
      "Epoch 12 / 120: avg. loss of last 5 iterations 1.5698668241500855\n",
      "Epoch: 12 LR: [0.001]\n",
      "Epoch 13 / 120: avg. loss of last 5 iterations 1.57263503074646\n",
      "Epoch: 13 LR: [0.001]\n",
      "Epoch 14 / 120: avg. loss of last 5 iterations 1.5579508066177368\n",
      "Epoch: 14 LR: [0.001]\n",
      "Epoch 15 / 120: avg. loss of last 5 iterations 1.5289263010025025\n",
      "Epoch: 15 LR: [0.001]\n",
      "Epoch 16 / 120: avg. loss of last 5 iterations 1.5241767644882203\n",
      "Epoch: 16 LR: [0.001]\n",
      "Epoch 17 / 120: avg. loss of last 5 iterations 1.510363483428955\n",
      "Epoch: 17 LR: [0.001]\n",
      "Epoch 18 / 120: avg. loss of last 5 iterations 1.5270854234695435\n",
      "Epoch: 18 LR: [0.001]\n",
      "Epoch 19 / 120: avg. loss of last 5 iterations 1.4784794569015502\n",
      "Epoch: 19 LR: [0.001]\n",
      "Epoch 20 / 120: avg. loss of last 5 iterations 1.4677688837051392\n",
      "Epoch: 20 LR: [0.001]\n",
      "Epoch 21 / 120: avg. loss of last 5 iterations 1.415785002708435\n",
      "Epoch: 21 LR: [0.001]\n",
      "Epoch 22 / 120: avg. loss of last 5 iterations 1.426246452331543\n",
      "Epoch: 22 LR: [0.001]\n",
      "Epoch 23 / 120: avg. loss of last 5 iterations 1.3841572046279906\n",
      "Epoch: 23 LR: [0.001]\n",
      "Epoch 24 / 120: avg. loss of last 5 iterations 1.3252039909362794\n",
      "Epoch: 24 LR: [0.001]\n",
      "Epoch 25 / 120: avg. loss of last 5 iterations 1.3312014579772948\n",
      "Epoch: 25 LR: [0.001]\n",
      "Epoch 26 / 120: avg. loss of last 5 iterations 1.2393404245376587\n",
      "Epoch: 26 LR: [0.001]\n",
      "Epoch 27 / 120: avg. loss of last 5 iterations 1.1749399662017823\n",
      "Epoch: 27 LR: [0.001]\n",
      "Epoch 28 / 120: avg. loss of last 5 iterations 1.0966962814331054\n",
      "Epoch: 28 LR: [0.001]\n",
      "Epoch 29 / 120: avg. loss of last 5 iterations 1.1857522487640382\n",
      "Epoch: 29 LR: [0.001]\n",
      "Epoch 30 / 120: avg. loss of last 5 iterations 0.9080344915390015\n",
      "Epoch: 30 LR: [0.001]\n",
      "Epoch 31 / 120: avg. loss of last 5 iterations 0.8228265523910523\n",
      "Epoch: 31 LR: [0.001]\n",
      "Epoch 32 / 120: avg. loss of last 5 iterations 0.7534050822257996\n",
      "Epoch: 32 LR: [0.001]\n",
      "Epoch 33 / 120: avg. loss of last 5 iterations 0.7273909389972687\n",
      "Epoch: 33 LR: [0.001]\n",
      "Epoch 34 / 120: avg. loss of last 5 iterations 0.7258173942565918\n",
      "Epoch: 34 LR: [0.001]\n",
      "Epoch 35 / 120: avg. loss of last 5 iterations 0.6513443768024445\n",
      "Epoch: 35 LR: [0.001]\n",
      "Epoch 36 / 120: avg. loss of last 5 iterations 0.637853741645813\n",
      "Epoch: 36 LR: [0.001]\n",
      "Epoch 37 / 120: avg. loss of last 5 iterations 0.5612288057804108\n",
      "Epoch: 37 LR: [0.001]\n",
      "Epoch 38 / 120: avg. loss of last 5 iterations 0.374536794424057\n",
      "Epoch: 38 LR: [0.001]\n",
      "Epoch 39 / 120: avg. loss of last 5 iterations 0.43993674516677855\n",
      "Epoch: 39 LR: [0.0003333333333333333]\n",
      "Epoch 40 / 120: avg. loss of last 5 iterations 0.05440500974655151\n",
      "Epoch: 40 LR: [0.0003333333333333333]\n",
      "Epoch 41 / 120: avg. loss of last 5 iterations 0.03398570232093334\n",
      "Epoch: 41 LR: [0.0003333333333333333]\n",
      "Epoch 42 / 120: avg. loss of last 5 iterations 0.01737873274832964\n",
      "Epoch: 42 LR: [0.0003333333333333333]\n",
      "Epoch 43 / 120: avg. loss of last 5 iterations 0.013122071139514446\n",
      "Epoch: 43 LR: [0.0003333333333333333]\n",
      "Epoch 44 / 120: avg. loss of last 5 iterations 0.008542199619114399\n",
      "Epoch: 44 LR: [0.0003333333333333333]\n",
      "Epoch 45 / 120: avg. loss of last 5 iterations 0.007195606920868158\n",
      "Epoch: 45 LR: [0.0003333333333333333]\n",
      "Epoch 46 / 120: avg. loss of last 5 iterations 0.006708457786589861\n",
      "Epoch: 46 LR: [0.0003333333333333333]\n",
      "Epoch 47 / 120: avg. loss of last 5 iterations 0.004212184110656381\n",
      "Epoch: 47 LR: [0.0003333333333333333]\n",
      "Epoch 48 / 120: avg. loss of last 5 iterations 0.004137201327830553\n",
      "Epoch: 48 LR: [0.0003333333333333333]\n",
      "Epoch 49 / 120: avg. loss of last 5 iterations 0.003934452310204506\n",
      "Epoch: 49 LR: [0.0003333333333333333]\n",
      "Epoch 50 / 120: avg. loss of last 5 iterations 0.0035444654989987613\n",
      "Epoch: 50 LR: [0.0003333333333333333]\n",
      "Epoch 51 / 120: avg. loss of last 5 iterations 0.003665304183959961\n",
      "Epoch: 51 LR: [0.0003333333333333333]\n",
      "Epoch 52 / 120: avg. loss of last 5 iterations 0.0032391814980655908\n",
      "Epoch: 52 LR: [0.0003333333333333333]\n",
      "Epoch 53 / 120: avg. loss of last 5 iterations 0.0026383074931800366\n",
      "Epoch: 53 LR: [0.0003333333333333333]\n",
      "Epoch 54 / 120: avg. loss of last 5 iterations 0.0025293110404163597\n",
      "Epoch: 54 LR: [0.0003333333333333333]\n",
      "Epoch 55 / 120: avg. loss of last 5 iterations 0.0019899022299796343\n",
      "Epoch: 55 LR: [0.0003333333333333333]\n",
      "Epoch 56 / 120: avg. loss of last 5 iterations 0.0024294915376231073\n",
      "Epoch: 56 LR: [0.0003333333333333333]\n",
      "Epoch 57 / 120: avg. loss of last 5 iterations 0.0018315328285098075\n",
      "Epoch: 57 LR: [0.0003333333333333333]\n",
      "Epoch 58 / 120: avg. loss of last 5 iterations 0.001792528317309916\n",
      "Epoch: 58 LR: [0.0003333333333333333]\n",
      "Epoch 59 / 120: avg. loss of last 5 iterations 0.0017744489246979355\n",
      "Epoch: 59 LR: [0.0003333333333333333]\n",
      "Epoch 60 / 120: avg. loss of last 5 iterations 0.001681758789345622\n",
      "Epoch: 60 LR: [0.0003333333333333333]\n",
      "Epoch 61 / 120: avg. loss of last 5 iterations 0.001342577813193202\n",
      "Epoch: 61 LR: [0.0003333333333333333]\n",
      "Epoch 62 / 120: avg. loss of last 5 iterations 0.0017178986920043827\n",
      "Epoch: 62 LR: [0.0003333333333333333]\n",
      "Epoch 63 / 120: avg. loss of last 5 iterations 0.0012248648214153945\n",
      "Epoch: 63 LR: [0.0003333333333333333]\n",
      "Epoch 64 / 120: avg. loss of last 5 iterations 0.0013008484849706293\n",
      "Epoch: 64 LR: [0.0003333333333333333]\n",
      "Epoch 65 / 120: avg. loss of last 5 iterations 0.0014715122524648905\n",
      "Epoch: 65 LR: [0.0003333333333333333]\n",
      "Epoch 66 / 120: avg. loss of last 5 iterations 0.001283877552486956\n",
      "Epoch: 66 LR: [0.0003333333333333333]\n",
      "Epoch 67 / 120: avg. loss of last 5 iterations 0.0013659506337717175\n",
      "Epoch: 67 LR: [0.0003333333333333333]\n",
      "Epoch 68 / 120: avg. loss of last 5 iterations 0.0011790233431383968\n",
      "Epoch: 68 LR: [0.0003333333333333333]\n",
      "Epoch 69 / 120: avg. loss of last 5 iterations 0.001353851007297635\n",
      "Epoch: 69 LR: [0.0003333333333333333]\n",
      "Epoch 70 / 120: avg. loss of last 5 iterations 0.0013176648644730449\n",
      "Epoch: 70 LR: [0.0003333333333333333]\n",
      "Epoch 71 / 120: avg. loss of last 5 iterations 0.0010891947778873146\n",
      "Epoch: 71 LR: [0.0003333333333333333]\n",
      "Epoch 72 / 120: avg. loss of last 5 iterations 0.0009235843317583203\n",
      "Epoch: 72 LR: [0.0003333333333333333]\n",
      "Epoch 73 / 120: avg. loss of last 5 iterations 0.0009976232540793717\n",
      "Epoch: 73 LR: [0.0003333333333333333]\n",
      "Epoch 74 / 120: avg. loss of last 5 iterations 0.0009302536491304636\n",
      "Epoch: 74 LR: [0.0003333333333333333]\n",
      "Epoch 75 / 120: avg. loss of last 5 iterations 0.0009837640332989394\n",
      "Epoch: 75 LR: [0.0003333333333333333]\n",
      "Epoch 76 / 120: avg. loss of last 5 iterations 0.0008317946107126772\n",
      "Epoch: 76 LR: [0.0003333333333333333]\n",
      "Epoch 77 / 120: avg. loss of last 5 iterations 0.0008286085911095142\n",
      "Epoch: 77 LR: [0.0003333333333333333]\n",
      "Epoch 78 / 120: avg. loss of last 5 iterations 0.0007316739414818585\n",
      "Epoch: 78 LR: [0.0003333333333333333]\n",
      "Epoch 79 / 120: avg. loss of last 5 iterations 0.0008300547953695059\n",
      "Epoch: 79 LR: [0.0001111111111111111]\n",
      "Epoch 80 / 120: avg. loss of last 5 iterations 0.0008184714708477259\n",
      "Epoch: 80 LR: [0.0001111111111111111]\n",
      "Epoch 81 / 120: avg. loss of last 5 iterations 0.0008866640971973538\n",
      "Epoch: 81 LR: [0.0001111111111111111]\n",
      "Epoch 82 / 120: avg. loss of last 5 iterations 0.0007636680151335895\n",
      "Epoch: 82 LR: [0.0001111111111111111]\n",
      "Epoch 83 / 120: avg. loss of last 5 iterations 0.0008256633998826146\n",
      "Epoch: 83 LR: [0.0001111111111111111]\n",
      "Epoch 84 / 120: avg. loss of last 5 iterations 0.0007867012405768037\n",
      "Epoch: 84 LR: [0.0001111111111111111]\n",
      "Epoch 85 / 120: avg. loss of last 5 iterations 0.0008692415896803141\n",
      "Epoch: 85 LR: [0.0001111111111111111]\n",
      "Epoch 86 / 120: avg. loss of last 5 iterations 0.0007354660890996456\n",
      "Epoch: 86 LR: [0.0001111111111111111]\n",
      "Epoch 87 / 120: avg. loss of last 5 iterations 0.0007206289097666741\n",
      "Epoch: 87 LR: [0.0001111111111111111]\n",
      "Epoch 88 / 120: avg. loss of last 5 iterations 0.0008155810763128101\n",
      "Epoch: 88 LR: [0.0001111111111111111]\n",
      "Epoch 89 / 120: avg. loss of last 5 iterations 0.0006980741338338703\n",
      "Epoch: 89 LR: [0.0001111111111111111]\n",
      "Epoch 90 / 120: avg. loss of last 5 iterations 0.0007005433551967144\n",
      "Epoch: 90 LR: [0.0001111111111111111]\n",
      "Epoch 91 / 120: avg. loss of last 5 iterations 0.0007385100587271153\n",
      "Epoch: 91 LR: [0.0001111111111111111]\n",
      "Epoch 92 / 120: avg. loss of last 5 iterations 0.0008191359229385853\n",
      "Epoch: 92 LR: [0.0001111111111111111]\n",
      "Epoch 93 / 120: avg. loss of last 5 iterations 0.0007214409881271422\n",
      "Epoch: 93 LR: [0.0001111111111111111]\n",
      "Epoch 94 / 120: avg. loss of last 5 iterations 0.0006240191520191729\n",
      "Epoch: 94 LR: [0.0001111111111111111]\n",
      "Epoch 95 / 120: avg. loss of last 5 iterations 0.0006870375713333487\n",
      "Epoch: 95 LR: [0.0001111111111111111]\n",
      "Epoch 96 / 120: avg. loss of last 5 iterations 0.0007036964292638004\n",
      "Epoch: 96 LR: [0.0001111111111111111]\n",
      "Epoch 97 / 120: avg. loss of last 5 iterations 0.0007927530095912516\n",
      "Epoch: 97 LR: [0.0001111111111111111]\n",
      "Epoch 98 / 120: avg. loss of last 5 iterations 0.000729653099551797\n",
      "Epoch: 98 LR: [0.0001111111111111111]\n",
      "Epoch 99 / 120: avg. loss of last 5 iterations 0.000749145052395761\n",
      "Epoch: 99 LR: [0.0001111111111111111]\n",
      "Epoch 100 / 120: avg. loss of last 5 iterations 0.0005884407146368176\n",
      "Epoch: 100 LR: [0.0001111111111111111]\n",
      "Epoch 101 / 120: avg. loss of last 5 iterations 0.0005849236680660397\n",
      "Epoch: 101 LR: [0.0001111111111111111]\n",
      "Epoch 102 / 120: avg. loss of last 5 iterations 0.0007185349939391017\n",
      "Epoch: 102 LR: [0.0001111111111111111]\n",
      "Epoch 103 / 120: avg. loss of last 5 iterations 0.0005699059984181076\n",
      "Epoch: 103 LR: [0.0001111111111111111]\n",
      "Epoch 104 / 120: avg. loss of last 5 iterations 0.0006768248858861625\n",
      "Epoch: 104 LR: [0.0001111111111111111]\n",
      "Epoch 105 / 120: avg. loss of last 5 iterations 0.0006298926775343717\n",
      "Epoch: 105 LR: [0.0001111111111111111]\n",
      "Epoch 106 / 120: avg. loss of last 5 iterations 0.0006625704350881279\n",
      "Epoch: 106 LR: [0.0001111111111111111]\n",
      "Epoch 107 / 120: avg. loss of last 5 iterations 0.0006537071079947054\n",
      "Epoch: 107 LR: [0.0001111111111111111]\n",
      "Epoch 108 / 120: avg. loss of last 5 iterations 0.0006276093190535903\n",
      "Epoch: 108 LR: [0.0001111111111111111]\n",
      "Epoch 109 / 120: avg. loss of last 5 iterations 0.000850564579013735\n",
      "Epoch: 109 LR: [0.0001111111111111111]\n",
      "Epoch 110 / 120: avg. loss of last 5 iterations 0.0005241116799879819\n",
      "Epoch: 110 LR: [0.0001111111111111111]\n",
      "Epoch 111 / 120: avg. loss of last 5 iterations 0.0008084095432423055\n",
      "Epoch: 111 LR: [0.0001111111111111111]\n",
      "Epoch 112 / 120: avg. loss of last 5 iterations 0.0005206139816436916\n",
      "Epoch: 112 LR: [0.0001111111111111111]\n",
      "Epoch 113 / 120: avg. loss of last 5 iterations 0.0005627731210552156\n",
      "Epoch: 113 LR: [0.0001111111111111111]\n",
      "Epoch 114 / 120: avg. loss of last 5 iterations 0.0006107896100729704\n",
      "Epoch: 114 LR: [0.0001111111111111111]\n",
      "Epoch 115 / 120: avg. loss of last 5 iterations 0.0005574273876845836\n",
      "Epoch: 115 LR: [0.0001111111111111111]\n",
      "Epoch 116 / 120: avg. loss of last 5 iterations 0.0006820730573963374\n",
      "Epoch: 116 LR: [0.0001111111111111111]\n",
      "Epoch 117 / 120: avg. loss of last 5 iterations 0.0005528347159270197\n",
      "Epoch: 117 LR: [0.0001111111111111111]\n",
      "Epoch 118 / 120: avg. loss of last 5 iterations 0.0005517038225661963\n",
      "Epoch: 118 LR: [0.0001111111111111111]\n",
      "Epoch 119 / 120: avg. loss of last 5 iterations 0.0005590659566223621\n",
      "Epoch: 119 LR: [3.703703703703703e-05]\n"
     ]
    }
   ],
   "source": [
    "train_upstream_loader, train_downstream_loader, test_loader = get_cifar10_sets(\n",
    "    num_examples_upstream = 20000,\n",
    "    num_examples_downstream = 20000,\n",
    "    num_classes_upstream = 5,\n",
    "    num_classes_downstream = 5,\n",
    "    is_downstream_random = False,\n",
    "    batch_size = 256\n",
    "  )\n",
    "\n",
    "model = get_vgg_model(5, 1)\n",
    "\n",
    "#train_upstream_loader = get_cifar10_orig()\n",
    "#model = get_vgg_model(10, 1)\n",
    "\n",
    "epochs = 120\n",
    "learnable_parameters = get_learnable_parameters(model)\n",
    "criterion, optimizer, scheduler = get_training_components(learnable_parameters, 0.001, (int)(epochs/3))\n",
    "print(\"before training\")\n",
    "device = get_device()\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "loss_history = train(model, criterion, optimizer, epochs, train_upstream_loader, device, scheduler=scheduler)\n",
    "#loss_history = train(model, criterion, optimizer, epochs, train_upstream_loader, device)\n",
    "\n",
    "torch.save(model.state_dict(), \"./pretrained_model_dict_upstream_1_2.pt\")\n",
    "torch.save(model, \"./pretrained_model_upstream_1_2.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def setup_and_train_downstream(epochs, lr, step_size, num_examples_upstream, num_examples_downstream,\n",
    "                   num_classes_upstream, num_classes_downstream, is_downstream_random,\n",
    "                   batch_size, num_outputs, upstream_model_path=None):\n",
    "    \n",
    "    train_upstream_loader, train_downstream_loader, test_loader = get_cifar10_sets(\n",
    "        num_examples_upstream, num_examples_downstream, num_classes_upstream,\n",
    "        num_classes_downstream, is_downstream_random, batch_size)\n",
    "\n",
    "    model = get_vgg_model(num_classes_downstream, 1)\n",
    "    if upstream_model_path is not None:\n",
    "        model.load_state_dict(torch.load(upstream_model_path), strict=False)\n",
    "        model.classifier = nn.Linear(512, num_outputs)\n",
    "        torch.nn.init.kaiming_normal_(model.classifier.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    learnable_parameters = get_learnable_parameters(model)\n",
    "    criterion, optimizer, scheduler = get_training_components(learnable_parameters, lr, step_size)\n",
    "    device = get_device()\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    if not is_downstream_random:\n",
    "      loss_history, train_accuracies, test_accuracies = train(model, criterion, optimizer, epochs, train_downstream_loader, \n",
    "                                                              device, scheduler=scheduler, acc_mode=\"both\")\n",
    "      return loss_history, train_accuracies, test_accuracies\n",
    "    loss_history, train_accuracies = train(model, criterion, optimizer, epochs, train_downstream_loader, \n",
    "                                           device, scheduler=scheduler, acc_mode=\"train\")\n",
    "    return loss_history, train_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda (GPU support) is available and enabled!\n",
      "Epoch 0 / 40: avg. loss of last 5 iterations 1.8936308383941651\n",
      "Epoch 0 / 40: Last train acc:0.32915\n",
      "Epoch 0 / 40: Last test acc:0.3258\n",
      "Epoch: 0 LR: [0.001]\n",
      "Epoch 1 / 40: avg. loss of last 5 iterations 1.5844439029693604\n",
      "Epoch 1 / 40: Last train acc:0.41025\n",
      "Epoch 1 / 40: Last test acc:0.4051\n",
      "Epoch: 1 LR: [0.001]\n",
      "Epoch 2 / 40: avg. loss of last 5 iterations 1.5211504697799683\n",
      "Epoch 2 / 40: Last train acc:0.45545\n",
      "Epoch 2 / 40: Last test acc:0.4507\n",
      "Epoch: 2 LR: [0.001]\n",
      "Epoch 3 / 40: avg. loss of last 5 iterations 1.485640859603882\n",
      "Epoch 3 / 40: Last train acc:0.4927\n",
      "Epoch 3 / 40: Last test acc:0.4766\n",
      "Epoch: 3 LR: [0.001]\n",
      "Epoch 4 / 40: avg. loss of last 5 iterations 1.44695246219635\n",
      "Epoch 4 / 40: Last train acc:0.5171\n",
      "Epoch 4 / 40: Last test acc:0.4975\n",
      "Epoch: 4 LR: [0.001]\n",
      "Epoch 5 / 40: avg. loss of last 5 iterations 1.3256017446517945\n",
      "Epoch 5 / 40: Last train acc:0.5547\n",
      "Epoch 5 / 40: Last test acc:0.5356\n",
      "Epoch: 5 LR: [0.001]\n",
      "Epoch 6 / 40: avg. loss of last 5 iterations 1.2086697816848755\n",
      "Epoch 6 / 40: Last train acc:0.5385\n",
      "Epoch 6 / 40: Last test acc:0.5111\n",
      "Epoch: 6 LR: [0.001]\n",
      "Epoch 7 / 40: avg. loss of last 5 iterations 1.1728015303611756\n",
      "Epoch 7 / 40: Last train acc:0.60195\n",
      "Epoch 7 / 40: Last test acc:0.5574\n",
      "Epoch: 7 LR: [0.001]\n",
      "Epoch 8 / 40: avg. loss of last 5 iterations 1.1191941857337953\n",
      "Epoch 8 / 40: Last train acc:0.59595\n",
      "Epoch 8 / 40: Last test acc:0.5521\n",
      "Epoch: 8 LR: [0.001]\n",
      "Epoch 9 / 40: avg. loss of last 5 iterations 1.0054638862609864\n",
      "Epoch 9 / 40: Last train acc:0.63105\n",
      "Epoch 9 / 40: Last test acc:0.5723\n",
      "Epoch: 9 LR: [0.001]\n",
      "Epoch 10 / 40: avg. loss of last 5 iterations 1.0300770998001099\n",
      "Epoch 10 / 40: Last train acc:0.65845\n",
      "Epoch 10 / 40: Last test acc:0.5854\n",
      "Epoch: 10 LR: [0.001]\n",
      "Epoch 11 / 40: avg. loss of last 5 iterations 0.9215341448783875\n",
      "Epoch 11 / 40: Last train acc:0.6826\n",
      "Epoch 11 / 40: Last test acc:0.5959\n",
      "Epoch: 11 LR: [0.001]\n",
      "Epoch 12 / 40: avg. loss of last 5 iterations 0.9330082178115845\n",
      "Epoch 12 / 40: Last train acc:0.7054\n",
      "Epoch 12 / 40: Last test acc:0.6079\n",
      "Epoch: 12 LR: [0.0003333333333333333]\n",
      "Epoch 13 / 40: avg. loss of last 5 iterations 0.8301390767097473\n",
      "Epoch 13 / 40: Last train acc:0.7559\n",
      "Epoch 13 / 40: Last test acc:0.6343\n",
      "Epoch: 13 LR: [0.0003333333333333333]\n",
      "Epoch 14 / 40: avg. loss of last 5 iterations 0.761053466796875\n",
      "Epoch 14 / 40: Last train acc:0.7722\n",
      "Epoch 14 / 40: Last test acc:0.6275\n",
      "Epoch: 14 LR: [0.0003333333333333333]\n",
      "Epoch 15 / 40: avg. loss of last 5 iterations 0.6657565712928772\n",
      "Epoch 15 / 40: Last train acc:0.78255\n",
      "Epoch 15 / 40: Last test acc:0.6304\n",
      "Epoch: 15 LR: [0.0003333333333333333]\n",
      "Epoch 16 / 40: avg. loss of last 5 iterations 0.6383933186531067\n",
      "Epoch 16 / 40: Last train acc:0.8058\n",
      "Epoch 16 / 40: Last test acc:0.6402\n",
      "Epoch: 16 LR: [0.0003333333333333333]\n",
      "Epoch 17 / 40: avg. loss of last 5 iterations 0.604211437702179\n",
      "Epoch 17 / 40: Last train acc:0.79445\n",
      "Epoch 17 / 40: Last test acc:0.6283\n",
      "Epoch: 17 LR: [0.0003333333333333333]\n",
      "Epoch 18 / 40: avg. loss of last 5 iterations 0.5581007897853851\n",
      "Epoch 18 / 40: Last train acc:0.80565\n",
      "Epoch 18 / 40: Last test acc:0.6254\n",
      "Epoch: 18 LR: [0.0003333333333333333]\n",
      "Epoch 19 / 40: avg. loss of last 5 iterations 0.5857184052467346\n",
      "Epoch 19 / 40: Last train acc:0.84675\n",
      "Epoch 19 / 40: Last test acc:0.6425\n",
      "Epoch: 19 LR: [0.0003333333333333333]\n",
      "Epoch 20 / 40: avg. loss of last 5 iterations 0.5469380676746368\n",
      "Epoch 20 / 40: Last train acc:0.8412\n",
      "Epoch 20 / 40: Last test acc:0.6388\n",
      "Epoch: 20 LR: [0.0003333333333333333]\n",
      "Epoch 21 / 40: avg. loss of last 5 iterations 0.5094327390193939\n",
      "Epoch 21 / 40: Last train acc:0.87635\n",
      "Epoch 21 / 40: Last test acc:0.6444\n",
      "Epoch: 21 LR: [0.0003333333333333333]\n",
      "Epoch 22 / 40: avg. loss of last 5 iterations 0.43625587224960327\n",
      "Epoch 22 / 40: Last train acc:0.876\n",
      "Epoch 22 / 40: Last test acc:0.6389\n",
      "Epoch: 22 LR: [0.0003333333333333333]\n",
      "Epoch 23 / 40: avg. loss of last 5 iterations 0.4305188059806824\n",
      "Epoch 23 / 40: Last train acc:0.90065\n",
      "Epoch 23 / 40: Last test acc:0.6431\n",
      "Epoch: 23 LR: [0.0003333333333333333]\n",
      "Epoch 24 / 40: avg. loss of last 5 iterations 0.40740395784378053\n",
      "Epoch 24 / 40: Last train acc:0.8948\n",
      "Epoch 24 / 40: Last test acc:0.6379\n",
      "Epoch: 24 LR: [0.0003333333333333333]\n",
      "Epoch 25 / 40: avg. loss of last 5 iterations 0.38404796123504636\n",
      "Epoch 25 / 40: Last train acc:0.8971\n",
      "Epoch 25 / 40: Last test acc:0.6324\n",
      "Epoch: 25 LR: [0.0001111111111111111]\n",
      "Epoch 26 / 40: avg. loss of last 5 iterations 0.31225555539131167\n",
      "Epoch 26 / 40: Last train acc:0.9463\n",
      "Epoch 26 / 40: Last test acc:0.645\n",
      "Epoch: 26 LR: [0.0001111111111111111]\n",
      "Epoch 27 / 40: avg. loss of last 5 iterations 0.21742714047431946\n",
      "Epoch 27 / 40: Last train acc:0.9558\n",
      "Epoch 27 / 40: Last test acc:0.6468\n",
      "Epoch: 27 LR: [0.0001111111111111111]\n",
      "Epoch 28 / 40: avg. loss of last 5 iterations 0.2304212599992752\n",
      "Epoch 28 / 40: Last train acc:0.9607\n",
      "Epoch 28 / 40: Last test acc:0.6481\n",
      "Epoch: 28 LR: [0.0001111111111111111]\n",
      "Epoch 29 / 40: avg. loss of last 5 iterations 0.19669824242591857\n",
      "Epoch 29 / 40: Last train acc:0.96785\n",
      "Epoch 29 / 40: Last test acc:0.6503\n",
      "Epoch: 29 LR: [0.0001111111111111111]\n",
      "Epoch 30 / 40: avg. loss of last 5 iterations 0.18610091209411622\n",
      "Epoch 30 / 40: Last train acc:0.96965\n",
      "Epoch 30 / 40: Last test acc:0.6474\n",
      "Epoch: 30 LR: [0.0001111111111111111]\n",
      "Epoch 31 / 40: avg. loss of last 5 iterations 0.20228471755981445\n",
      "Epoch 31 / 40: Last train acc:0.9678\n",
      "Epoch 31 / 40: Last test acc:0.6407\n",
      "Epoch: 31 LR: [0.0001111111111111111]\n",
      "Epoch 32 / 40: avg. loss of last 5 iterations 0.16632621884346008\n",
      "Epoch 32 / 40: Last train acc:0.974\n",
      "Epoch 32 / 40: Last test acc:0.6443\n",
      "Epoch: 32 LR: [0.0001111111111111111]\n",
      "Epoch 33 / 40: avg. loss of last 5 iterations 0.15505812168121338\n",
      "Epoch 33 / 40: Last train acc:0.9816\n",
      "Epoch 33 / 40: Last test acc:0.6433\n",
      "Epoch: 33 LR: [0.0001111111111111111]\n",
      "Epoch 34 / 40: avg. loss of last 5 iterations 0.14530747383832932\n",
      "Epoch 34 / 40: Last train acc:0.9806\n",
      "Epoch 34 / 40: Last test acc:0.6425\n",
      "Epoch: 34 LR: [0.0001111111111111111]\n",
      "Epoch 35 / 40: avg. loss of last 5 iterations 0.13178423345088958\n",
      "Epoch 35 / 40: Last train acc:0.9838\n",
      "Epoch 35 / 40: Last test acc:0.6429\n",
      "Epoch: 35 LR: [0.0001111111111111111]\n",
      "Epoch 36 / 40: avg. loss of last 5 iterations 0.1560791477560997\n",
      "Epoch 36 / 40: Last train acc:0.983\n",
      "Epoch 36 / 40: Last test acc:0.6449\n",
      "Epoch: 36 LR: [0.0001111111111111111]\n",
      "Epoch 37 / 40: avg. loss of last 5 iterations 0.15800162106752397\n",
      "Epoch 37 / 40: Last train acc:0.98815\n",
      "Epoch 37 / 40: Last test acc:0.6451\n",
      "Epoch: 37 LR: [0.0001111111111111111]\n",
      "Epoch 38 / 40: avg. loss of last 5 iterations 0.1151007890701294\n",
      "Epoch 38 / 40: Last train acc:0.9901\n",
      "Epoch 38 / 40: Last test acc:0.6438\n",
      "Epoch: 38 LR: [3.703703703703703e-05]\n",
      "Epoch 39 / 40: avg. loss of last 5 iterations 0.08985493630170822\n",
      "Epoch 39 / 40: Last train acc:0.9921\n",
      "Epoch 39 / 40: Last test acc:0.6432\n",
      "Epoch: 39 LR: [3.703703703703703e-05]\n"
     ]
    }
   ],
   "source": [
    "# From scratch with real labels\n",
    "loss_scratch_real, accs_scratch_real_train, accs_scratch_real_test = setup_and_train_downstream(\n",
    "    epochs=40, lr=0.001, step_size=(int)(40/3), num_examples_upstream=20000, \n",
    "    num_examples_downstream=20000, num_classes_upstream = 5, num_classes_downstream = 10,\n",
    "    is_downstream_random = False, batch_size = 256, num_outputs=10\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda (GPU support) is available and enabled!\n",
      "Epoch 0 / 40: avg. loss of last 5 iterations 1.8411335229873658\n",
      "Epoch 0 / 40: Last train acc:0.32445\n",
      "Epoch 0 / 40: Last test acc:0.3206\n",
      "Epoch: 0 LR: [0.001]\n",
      "Epoch 1 / 40: avg. loss of last 5 iterations 1.5794561624526977\n",
      "Epoch 1 / 40: Last train acc:0.45485\n",
      "Epoch 1 / 40: Last test acc:0.4333\n",
      "Epoch: 1 LR: [0.001]\n",
      "Epoch 2 / 40: avg. loss of last 5 iterations 1.4011873960494996\n",
      "Epoch 2 / 40: Last train acc:0.5126\n",
      "Epoch 2 / 40: Last test acc:0.4756\n",
      "Epoch: 2 LR: [0.001]\n",
      "Epoch 3 / 40: avg. loss of last 5 iterations 1.291429829597473\n",
      "Epoch 3 / 40: Last train acc:0.5703\n",
      "Epoch 3 / 40: Last test acc:0.5133\n",
      "Epoch: 3 LR: [0.001]\n",
      "Epoch 4 / 40: avg. loss of last 5 iterations 1.208871841430664\n",
      "Epoch 4 / 40: Last train acc:0.5895\n",
      "Epoch 4 / 40: Last test acc:0.517\n",
      "Epoch: 4 LR: [0.001]\n",
      "Epoch 5 / 40: avg. loss of last 5 iterations 1.1266013383865356\n",
      "Epoch 5 / 40: Last train acc:0.6286\n",
      "Epoch 5 / 40: Last test acc:0.5387\n",
      "Epoch: 5 LR: [0.001]\n",
      "Epoch 6 / 40: avg. loss of last 5 iterations 1.006551194190979\n",
      "Epoch 6 / 40: Last train acc:0.6716\n",
      "Epoch 6 / 40: Last test acc:0.5576\n",
      "Epoch: 6 LR: [0.001]\n",
      "Epoch 7 / 40: avg. loss of last 5 iterations 0.9420618176460266\n",
      "Epoch 7 / 40: Last train acc:0.7085\n",
      "Epoch 7 / 40: Last test acc:0.5625\n",
      "Epoch: 7 LR: [0.001]\n",
      "Epoch 8 / 40: avg. loss of last 5 iterations 0.9058167934417725\n",
      "Epoch 8 / 40: Last train acc:0.7305\n",
      "Epoch 8 / 40: Last test acc:0.5609\n",
      "Epoch: 8 LR: [0.001]\n",
      "Epoch 9 / 40: avg. loss of last 5 iterations 0.7542603135108947\n",
      "Epoch 9 / 40: Last train acc:0.78185\n",
      "Epoch 9 / 40: Last test acc:0.5835\n",
      "Epoch: 9 LR: [0.001]\n",
      "Epoch 10 / 40: avg. loss of last 5 iterations 0.6841422319412231\n",
      "Epoch 10 / 40: Last train acc:0.7771\n",
      "Epoch 10 / 40: Last test acc:0.5637\n",
      "Epoch: 10 LR: [0.001]\n",
      "Epoch 11 / 40: avg. loss of last 5 iterations 0.6498449444770813\n",
      "Epoch 11 / 40: Last train acc:0.8033\n",
      "Epoch 11 / 40: Last test acc:0.5671\n",
      "Epoch: 11 LR: [0.001]\n",
      "Epoch 12 / 40: avg. loss of last 5 iterations 0.479023277759552\n",
      "Epoch 12 / 40: Last train acc:0.9031\n",
      "Epoch 12 / 40: Last test acc:0.5991\n",
      "Epoch: 12 LR: [0.0003333333333333333]\n",
      "Epoch 13 / 40: avg. loss of last 5 iterations 0.24816687703132628\n",
      "Epoch 13 / 40: Last train acc:0.9554\n",
      "Epoch 13 / 40: Last test acc:0.6088\n",
      "Epoch: 13 LR: [0.0003333333333333333]\n",
      "Epoch 14 / 40: avg. loss of last 5 iterations 0.20342970490455628\n",
      "Epoch 14 / 40: Last train acc:0.9616\n",
      "Epoch 14 / 40: Last test acc:0.6011\n",
      "Epoch: 14 LR: [0.0003333333333333333]\n",
      "Epoch 15 / 40: avg. loss of last 5 iterations 0.131688691675663\n",
      "Epoch 15 / 40: Last train acc:0.9829\n",
      "Epoch 15 / 40: Last test acc:0.6088\n",
      "Epoch: 15 LR: [0.0003333333333333333]\n",
      "Epoch 16 / 40: avg. loss of last 5 iterations 0.11765881776809692\n",
      "Epoch 16 / 40: Last train acc:0.9895\n",
      "Epoch 16 / 40: Last test acc:0.6065\n",
      "Epoch: 16 LR: [0.0003333333333333333]\n",
      "Epoch 17 / 40: avg. loss of last 5 iterations 0.09107451438903809\n",
      "Epoch 17 / 40: Last train acc:0.99425\n",
      "Epoch 17 / 40: Last test acc:0.6047\n",
      "Epoch: 17 LR: [0.0003333333333333333]\n",
      "Epoch 18 / 40: avg. loss of last 5 iterations 0.07157175317406654\n",
      "Epoch 18 / 40: Last train acc:0.9971\n",
      "Epoch 18 / 40: Last test acc:0.605\n",
      "Epoch: 18 LR: [0.0003333333333333333]\n",
      "Epoch 19 / 40: avg. loss of last 5 iterations 0.05287686362862587\n",
      "Epoch 19 / 40: Last train acc:0.9991\n",
      "Epoch 19 / 40: Last test acc:0.6093\n",
      "Epoch: 19 LR: [0.0003333333333333333]\n",
      "Epoch 20 / 40: avg. loss of last 5 iterations 0.03764503635466099\n",
      "Epoch 20 / 40: Last train acc:0.9997\n",
      "Epoch 20 / 40: Last test acc:0.6076\n",
      "Epoch: 20 LR: [0.0003333333333333333]\n",
      "Epoch 21 / 40: avg. loss of last 5 iterations 0.026679512485861777\n",
      "Epoch 21 / 40: Last train acc:0.99965\n",
      "Epoch 21 / 40: Last test acc:0.6095\n",
      "Epoch: 21 LR: [0.0003333333333333333]\n",
      "Epoch 22 / 40: avg. loss of last 5 iterations 0.022731303796172143\n",
      "Epoch 22 / 40: Last train acc:0.99985\n",
      "Epoch 22 / 40: Last test acc:0.609\n",
      "Epoch: 22 LR: [0.0003333333333333333]\n",
      "Epoch 23 / 40: avg. loss of last 5 iterations 0.019488568231463433\n",
      "Epoch 23 / 40: Last train acc:0.99995\n",
      "Epoch 23 / 40: Last test acc:0.6073\n",
      "Epoch: 23 LR: [0.0003333333333333333]\n",
      "Epoch 24 / 40: avg. loss of last 5 iterations 0.015032203309237957\n",
      "Epoch 24 / 40: Last train acc:1.0\n",
      "Epoch 24 / 40: Last test acc:0.6073\n",
      "Epoch: 24 LR: [0.0003333333333333333]\n",
      "Epoch 25 / 40: avg. loss of last 5 iterations 0.015060470439493656\n",
      "Epoch 25 / 40: Last train acc:1.0\n",
      "Epoch 25 / 40: Last test acc:0.6066\n",
      "Epoch: 25 LR: [0.0001111111111111111]\n",
      "Epoch 26 / 40: avg. loss of last 5 iterations 0.01110679041594267\n",
      "Epoch 26 / 40: Last train acc:1.0\n",
      "Epoch 26 / 40: Last test acc:0.6102\n",
      "Epoch: 26 LR: [0.0001111111111111111]\n",
      "Epoch 27 / 40: avg. loss of last 5 iterations 0.010885417647659778\n",
      "Epoch 27 / 40: Last train acc:1.0\n",
      "Epoch 27 / 40: Last test acc:0.6083\n",
      "Epoch: 27 LR: [0.0001111111111111111]\n",
      "Epoch 28 / 40: avg. loss of last 5 iterations 0.011934387497603893\n",
      "Epoch 28 / 40: Last train acc:1.0\n",
      "Epoch 28 / 40: Last test acc:0.6095\n",
      "Epoch: 28 LR: [0.0001111111111111111]\n",
      "Epoch 29 / 40: avg. loss of last 5 iterations 0.010493945889174938\n",
      "Epoch 29 / 40: Last train acc:1.0\n",
      "Epoch 29 / 40: Last test acc:0.6085\n",
      "Epoch: 29 LR: [0.0001111111111111111]\n",
      "Epoch 30 / 40: avg. loss of last 5 iterations 0.009151734970510006\n",
      "Epoch 30 / 40: Last train acc:1.0\n",
      "Epoch 30 / 40: Last test acc:0.61\n",
      "Epoch: 30 LR: [0.0001111111111111111]\n",
      "Epoch 31 / 40: avg. loss of last 5 iterations 0.008630589954555035\n",
      "Epoch 31 / 40: Last train acc:1.0\n",
      "Epoch 31 / 40: Last test acc:0.6087\n",
      "Epoch: 31 LR: [0.0001111111111111111]\n",
      "Epoch 32 / 40: avg. loss of last 5 iterations 0.009624764416366815\n",
      "Epoch 32 / 40: Last train acc:1.0\n",
      "Epoch 32 / 40: Last test acc:0.6091\n",
      "Epoch: 32 LR: [0.0001111111111111111]\n",
      "Epoch 33 / 40: avg. loss of last 5 iterations 0.008347169775515795\n",
      "Epoch 33 / 40: Last train acc:1.0\n",
      "Epoch 33 / 40: Last test acc:0.6089\n",
      "Epoch: 33 LR: [0.0001111111111111111]\n",
      "Epoch 34 / 40: avg. loss of last 5 iterations 0.00928502194583416\n",
      "Epoch 34 / 40: Last train acc:1.0\n",
      "Epoch 34 / 40: Last test acc:0.6091\n",
      "Epoch: 34 LR: [0.0001111111111111111]\n",
      "Epoch 35 / 40: avg. loss of last 5 iterations 0.006691937055438757\n",
      "Epoch 35 / 40: Last train acc:1.0\n",
      "Epoch 35 / 40: Last test acc:0.6097\n",
      "Epoch: 35 LR: [0.0001111111111111111]\n",
      "Epoch 36 / 40: avg. loss of last 5 iterations 0.00830528773367405\n",
      "Epoch 36 / 40: Last train acc:1.0\n",
      "Epoch 36 / 40: Last test acc:0.6098\n",
      "Epoch: 36 LR: [0.0001111111111111111]\n",
      "Epoch 37 / 40: avg. loss of last 5 iterations 0.006677544489502907\n",
      "Epoch 37 / 40: Last train acc:1.0\n",
      "Epoch 37 / 40: Last test acc:0.6102\n",
      "Epoch: 37 LR: [0.0001111111111111111]\n",
      "Epoch 38 / 40: avg. loss of last 5 iterations 0.008073571883141995\n",
      "Epoch 38 / 40: Last train acc:1.0\n",
      "Epoch 38 / 40: Last test acc:0.609\n",
      "Epoch: 38 LR: [3.703703703703703e-05]\n",
      "Epoch 39 / 40: avg. loss of last 5 iterations 0.0069081390276551245\n",
      "Epoch 39 / 40: Last train acc:1.0\n",
      "Epoch 39 / 40: Last test acc:0.6101\n",
      "Epoch: 39 LR: [3.703703703703703e-05]\n"
     ]
    }
   ],
   "source": [
    "# Using upstream training's weights with real labels\n",
    "loss_upstream_real, accs_upstream_real_train, accs_upstream_real_test = setup_and_train_downstream(\n",
    "    epochs=40, lr=0.001, step_size=(int)(40/3), num_examples_upstream=20000, \n",
    "    num_examples_downstream=20000, num_classes_upstream = 5, num_classes_downstream = 5,\n",
    "    is_downstream_random = False, batch_size = 256, num_outputs=10,\n",
    "    upstream_model_path = \"pretrained_model_dict_1_2.pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "orange = \"#fcba03\" # from scr\n",
    "light_blue = \"#03bafc\" # pre\n",
    "\n",
    "plt.plot(accs_scratch_real_train, linestyle=\"-\", color=orange)\n",
    "plt.plot(accs_upstream_real_train, linestyle=\"-\", color=light_blue)\n",
    "plt.plot(accs_scratch_real_test, linestyle=\"--\", color=orange)\n",
    "plt.plot(accs_upstream_real_test, linestyle=\"--\", color=light_blue)\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of figure1.ipynb",
   "provenance": [
    {
     "file_id": "1BNwYcPcE3j7kX8PsVSfBpXp1s6Dwmfne",
     "timestamp": 1626623850743
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
