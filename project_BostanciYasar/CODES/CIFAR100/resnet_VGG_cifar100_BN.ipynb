{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"resnet_VGG_cifar100_BN.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"240eGBNsHFH7","executionInfo":{"status":"ok","timestamp":1627458955084,"user_tz":-180,"elapsed":493,"user":{"displayName":"CENG 501","photoUrl":"","userId":"15156543895114676389"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.backends.cudnn as cudnn\n","import numpy as np\n","import torchvision\n","import torchvision.transforms as transforms\n","import sys\n","\n","import matplotlib.pyplot as plt \n","import time   \n","\n","import os\n","import argparse\n","\n","\n"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cg9oDp-IEwZD"},"source":["### 1.1 Enable GPU\n","\n","From \"Edit -> Notebook Settings -> Hardware accelerator\" select GPU. With the following we will specify to PyTorch that we want to use the GPU."]},{"cell_type":"code","metadata":{"id":"0tNnBZeMvvoZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627458955449,"user_tz":-180,"elapsed":38,"user":{"displayName":"CENG 501","photoUrl":"","userId":"15156543895114676389"}},"outputId":"17489a99-0672-4003-f241-35a841b9f26b"},"source":["if torch.cuda.is_available():\n","  print(\"Cuda (GPU support) is available and enabled!\")\n","  device = torch.device(\"cuda\")\n","else:\n","  print(\"Cuda (GPU support) is not available :(\")\n","  device = torch.device(\"cpu\")"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Cuda (GPU support) is available and enabled!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"V_h9IIGnFb-k"},"source":["### 2 Model Definition\n","\n","\n","In the original paper there are two different frameworks; ResNet and VGG. In this section, different models for different frameworks are created. However, in the original paper different normalization methods are applied in these frameworks and as mentioned the authors of the paper propose a new method which is called momentum Batch Normalization. In ResNet and VGG frameworks a naive batch normalization block is used and modified according to the proposed Momentum Batch Normalization method. On the other hand, Pytorch functions are used directly for other normalization methods such as Group Normalization, Layer Normalization, Instance Normalization and Batch Normalization for ResNet. "]},{"cell_type":"markdown","metadata":{"id":"1DBPPzmN_PEQ"},"source":["### 2.1 ResNet\n","\n","ResNet18, ResNet34 and ResNet50 architectures are defined according to the original paper as folllows."]},{"cell_type":"code","metadata":{"id":"bH1akIYwtvKE","executionInfo":{"status":"ok","timestamp":1627458956173,"user_tz":-180,"elapsed":750,"user":{"displayName":"CENG 501","photoUrl":"","userId":"15156543895114676389"}}},"source":["class BasicBlock(nn.Module):\n","    coefficient = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(\n","            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n","                               stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.coefficient*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.coefficient*planes,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.coefficient*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    coefficient = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n","                               stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.coefficient *\n","                               planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.coefficient*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.coefficient*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.coefficient*planes,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.coefficient*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n","                               stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.coefficient, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.coefficient\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = F.avg_pool2d(out, 4)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(BasicBlock, [2, 2, 2, 2])\n","\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3, 4, 6, 3])\n","\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3, 4, 6, 3])\n"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bjIf8a4L_zwT"},"source":["### 2.2 VGG\n","\n","VGG11 and VGG16 architectures are defined according to the original paper as folllows.\n"]},{"cell_type":"code","metadata":{"id":"DikohAmZGQtx","executionInfo":{"status":"ok","timestamp":1627458956176,"user_tz":-180,"elapsed":28,"user":{"displayName":"CENG 501","photoUrl":"","userId":"15156543895114676389"}}},"source":["configuration = {\n","    'VGG11': [64, 'pool', 128, 'pool', 256, 256, 'pool', 512, 512, 'pool', 512, 512, 'pool'],\n","    'VGG16': [64, 64, 'pool', 128, 128, 'pool', 256, 256, 256, 'pool', 512, 512, 512, 'pool', 512, 512, 512, 'pool']\n","}\n","\n","\n","class VGG(nn.Module):\n","    def __init__(self, vgg_name):\n","        super(VGG, self).__init__()\n","        self.features = self._make_layers(configuration[vgg_name])\n","        self.classifier = nn.Linear(512, 100)\n","\n","    def forward(self, x):\n","        out = self.features(x)\n","        out = out.view(out.size(0), -1)\n","        out = self.classifier(out)\n","        return out\n","\n","    def _make_layers(self, configuration):\n","        layers = []\n","        in_channels = 3\n","        for x in configuration:\n","            if x == 'pool':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n","                           nn.BatchNorm2d(x),\n","                           nn.ReLU(inplace=True)]\n","                in_channels = x\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n","        return nn.Sequential(*layers)\n"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Bp01CBRE91M"},"source":["## 3 The Dataset\n","\n","In the original paper authors used CIFAR10 and CIFAR100 datasets. In this section torchvision function is used to load train/test datasets;  \n","\n","*   There are 50000 training images and 10000 test images for  10 different classes for CIFAR10\n","*   There are 50000 training images and 10000 test images for 100 different classes for CIFAR100.\n","\n","In the original paper, authors mentions standard data augmentation and preprocessing techniques are used but any specific parameters were not given in the paper or in the supplementary material of the paper. Therefore, data augmentation and preprocessing techniques are taken from the literature and applied to the datasets. \n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2onI3bMcLy8e","executionInfo":{"status":"ok","timestamp":1627458957904,"user_tz":-180,"elapsed":1752,"user":{"displayName":"CENG 501","photoUrl":"","userId":"15156543895114676389"}},"outputId":"c89c17ea-ca40-403e-b3c2-745a2243c5d7"},"source":["# Data\n","print('==> Preparing data..')\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0, 0, 0), (1, 1, 1)),\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0, 0, 0), (1, 1, 1)),\n","])\n","\n","###################\n","batch_size=32 #batch size (2 per gpu)\n","m0=32 #ideal batch size\n","######################\n","\n","\n","trainset = torchvision.datasets.CIFAR100(\n","    root='./data', train=True, download=True, transform=transform_train)\n","trainloader = torch.utils.data.DataLoader(\n","    trainset, batch_size=batch_size, shuffle=True, num_workers=2) # 4 gpu\n","\n","testset = torchvision.datasets.CIFAR100(\n","    root='./data', train=False, download=True, transform=transform_test)\n","testloader = torch.utils.data.DataLoader(\n","    testset, batch_size=batch_size, shuffle=False, num_workers=2)\n"],"execution_count":13,"outputs":[{"output_type":"stream","text":["==> Preparing data..\n","Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BPbSc-a2Aeva"},"source":["### 4 Define and Train Model\n","In this section user can choose the model between the ResNet18, ResNet34, ResNet50, VGG11 and VGG16 frameworks. "]},{"cell_type":"markdown","metadata":{"id":"YfC6tcf1igI_"},"source":["### 4.1 Define Loss Function and the Optimizer\n","\n","Instance is created for the model based on the choice.\n","\n","In the original paper authors used Stochastic Gradient Descent for the optimization with the momentum 0.9 and weight decay 0.0001 paramaeters set. In the experiments these parameters and optimization method from the paper are preserved.\n","\n","Since the criterion for the loss function is not specified exactly in the original paper Cross-Entropy Loss method is chosen in the experiments.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"twExnQMrR3Hb"},"source":["sys.argv=['']\n","del sys\n","\n","\n","best_acc = 0  # best test accuracy\n","start_epoch = 1  \n","\n","\n","# Model\n","print('==> Building model..')\n","\n","#net = VGG('VGG11')\n","# net = VGG('VGG16')\n","net = ResNet18()\n","# net = ResNet34()\n","# net = ResNet50()\n","\n","net = net.to(device)\n","if device == 'cuda':\n","    net = torch.nn.DataParallel(net)\n","    cudnn.benchmark = True\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","optimizer = optim.SGD(net.parameters(), lr=0.1*batch_size/64,\n","                      momentum=0.9, weight_decay=1e-4)\n","scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[60, 120, 180], gamma=0.1)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1JlyKsFGiCZl"},"source":["### 4.2 Train the Model\n","\n","In the original paper, authors propose a new method in order to train large datasets with small batch-sizes for insufficient memory resources. Therefore, in the experiments considerably smaller batch sizes are used and it extends the training times. In order to overcome high training times problem GPU usage becomes a necessity and cuda is used in the experiments with Colab Pro.\n","\n"]},{"cell_type":"code","metadata":{"id":"RmmPWjZ3KaH0","executionInfo":{"status":"aborted","timestamp":1627458957951,"user_tz":-180,"elapsed":99,"user":{"displayName":"CENG 501","photoUrl":"","userId":"15156543895114676389"}}},"source":["# Training and Accuracy Calculation\n","def train(verbose=True):\n","    \n","    loss_history=[]\n","    net.train()\n","    correct = 0\n","    total = 0\n","    for epoch in range(start_epoch, start_epoch+epoch_number):\n","      \n","      start = time.time()\n","      for batch_idx, (inputs, targets) in enumerate(trainloader):\n","          inputs, targets = inputs.to(device), targets.to(device)\n","          \n","          optimizer.zero_grad()\n","          outputs = net(inputs)\n","          loss = criterion(outputs, targets)\n","          loss.backward()\n","          optimizer.step()\n","\n","\n","          loss_history.append(loss.item()) \n","\n","      end = time.time()\n","      if verbose: print(f'Epoch {epoch} / {epoch_number}: avg. loss of last 5 iterations {np.sum(loss_history[:-6:-1])/5} , Time : {end - start} seconds')            \n","\n","      global best_acc\n","      global test_loss\n","      net.eval()\n","      test_loss = 0\n","      correct = 0\n","      total = 0\n","      with torch.no_grad():\n","          for batch_idx, (inputs, targets) in enumerate(testloader):\n","              inputs, targets = inputs.to(device), targets.to(device)\n","              outputs = net(inputs)  \n","              _, predicted = outputs.max(1)\n","              total += targets.size(0)\n","              correct += predicted.eq(targets).sum().item()       \n","\n","\n","      acc = 100.*correct/total\n","      historyy.append(acc)\n","\n","      print('acc:',acc,'best_acc:',best_acc)\n","      if acc > best_acc:\n","          print('Saving..' )\n","          \n","          state = {\n","              'net': net.state_dict(),\n","              'acc': acc,\n","              'epoch': epoch,\n","          }\n","          best_acc = acc\n","          \n","      print('\\n\\n') \n","      scheduler.step() \n","    return loss_history      "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hu5lXdfLt-hQ","executionInfo":{"status":"aborted","timestamp":1627458957968,"user_tz":-180,"elapsed":115,"user":{"displayName":"CENG 501","photoUrl":"","userId":"15156543895114676389"}}},"source":["epoch_number=200\n","historyy=[]\n","losses=[]\n","\n","\n","loss_history=train()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gnaT7_kfGxie"},"source":["### 4.3 The Loss Curve\n","Loss curve is visualized."]},{"cell_type":"code","metadata":{"id":"3fbhdotYius9","executionInfo":{"status":"aborted","timestamp":1627458957993,"user_tz":-180,"elapsed":139,"user":{"displayName":"CENG 501","photoUrl":"","userId":"15156543895114676389"}}},"source":["plt.plot(loss_history)\n","plt.xlabel('Iteration number')\n","plt.ylabel('Loss value')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uSvgOaIUzWCk"},"source":["### 4.4 Quantitative Analysis\n","\n","We can analyze the accuracy of the predictions as follows. "]},{"cell_type":"code","metadata":{"id":"kghpVDAly9Kf","executionInfo":{"status":"aborted","timestamp":1627458958012,"user_tz":-180,"elapsed":158,"user":{"displayName":"CENG 501","photoUrl":"","userId":"15156543895114676389"}}},"source":["correct = 0\n","total = 0\n","with torch.no_grad():\n","    for batch_idx, (inputs, targets) in enumerate(testloader):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        outputs = net(inputs)\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","print('Accuracy of the network on the 10000 test images: %d %%' % (\n","    100 * correct / total))  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KwEmxNBOgQsD","executionInfo":{"status":"aborted","timestamp":1627458958017,"user_tz":-180,"elapsed":162,"user":{"displayName":"CENG 501","photoUrl":"","userId":"15156543895114676389"}}},"source":["print('saving' ,'acc:',best_acc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f1fPq-45BI_8"},"source":["### 5 Results\n","\n","Accuracy vs epoch is plotted according to the test results. "]},{"cell_type":"code","metadata":{"id":"j9j3BViCkt5r","executionInfo":{"status":"aborted","timestamp":1627458958023,"user_tz":-180,"elapsed":168,"user":{"displayName":"CENG 501","photoUrl":"","userId":"15156543895114676389"}}},"source":["plt.plot(historyy)\n","plt.xlabel('Epoch number')\n","plt.ylabel('Accuracy')\n","plt.show()\n","print(historyy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yCJz-UkSgdzZ","executionInfo":{"status":"aborted","timestamp":1627458958027,"user_tz":-180,"elapsed":170,"user":{"displayName":"CENG 501","photoUrl":"","userId":"15156543895114676389"}}},"source":[""],"execution_count":null,"outputs":[]}]}