{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 775,
     "status": "ok",
     "timestamp": 1626643190062,
     "user": {
      "displayName": "Ertuğrul Güngör",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicvIzQgyHfsqD4h979_wDnn11aI-XtLnxmnC_8=s64",
      "userId": "06165302180630202587"
     },
     "user_tz": -180
    },
    "id": "YBHMtNLN3GnS"
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class Identity(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Identity, self).__init__()\n",
    "      \n",
    "  def forward(self, x):\n",
    "    return x\n",
    "\n",
    "def get_vgg_model(num_outputs, init_scale):\n",
    "  vgg16 = models.vgg16()\n",
    "\n",
    "  # Remove avgpool since it's removed in the paper\n",
    "  vgg16.avgpool = Identity()\n",
    "\n",
    "  # In the paper, VGG16 model's 2 last FC layers are removed\n",
    "  # Output of last max pool layer is [batch_size,512,7,7] for 224x224 image\n",
    "  #vgg16.classifier = nn.Linear(512*7*7, num_outputs)\n",
    "\n",
    "  # For 32x32 image\n",
    "  vgg16.classifier = nn.Linear(512, num_outputs)\n",
    "\n",
    "  #for module in vgg16.modules():\n",
    "  #  if type(module) == nn.modules.pooling.MaxPool2d:\n",
    "  #    module = nn.MaxPool2d(2, stride=2, padding='same')\n",
    "    #if \"MaxPool2d\" is in str(type(module)):\n",
    "    #  print(\"module is : {}\".format(type(module)))\n",
    "  # Change weight initialization with a nested function\n",
    "  def init_weights(param):\n",
    "    if (type(param) == nn.Conv2d or type(param) == nn.Linear):\n",
    "      # Parameters are initialized with He initialization in the paper.\n",
    "\n",
    "      torch.nn.init.kaiming_normal_(param.weight, mode='fan_out', nonlinearity='relu')\n",
    "      # # ??\n",
    "      # param.weight = torch.nn.Parameter(param.weight * init_scale)\n",
    "\n",
    "      # print(type(param.weight))\n",
    "      # param.weight.data.fill_(param.weight * init_scale)\n",
    "      # torch.full(param.weight.size(), 3.141592)\n",
    "\n",
    "      # Default value of biases are already 0, just to be more verbose\n",
    "      # param.bias.data.fill_(0)\n",
    "\n",
    "      # print(param)\n",
    "      # print(type(param))\n",
    "\n",
    "  vgg16.apply(init_weights)\n",
    "  return vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7907,
     "status": "ok",
     "timestamp": 1626643197966,
     "user": {
      "displayName": "Ertuğrul Güngör",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicvIzQgyHfsqD4h979_wDnn11aI-XtLnxmnC_8=s64",
      "userId": "06165302180630202587"
     },
     "user_tz": -180
    },
    "id": "JZcqlcdLDtfM",
    "outputId": "7453edbd-e3ab-4b76-a8da-ed9b4634bdc9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /opt/conda/include/python3.8/UNKNOWN\n",
      "sysconfig: /opt/conda/include/python3.8\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "Requirement already satisfied: torchinfo in /opt/conda/lib/python3.8/site-packages (1.5.2)\n",
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /opt/conda/include/python3.8/UNKNOWN\n",
      "sysconfig: /opt/conda/include/python3.8\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "VGG                                      --                        --\n",
      "├─Sequential: 1-1                        [1, 512, 1, 1]            --\n",
      "│    └─Conv2d: 2-1                       [1, 64, 32, 32]           1,792\n",
      "│    └─ReLU: 2-2                         [1, 64, 32, 32]           --\n",
      "│    └─Conv2d: 2-3                       [1, 64, 32, 32]           36,928\n",
      "│    └─ReLU: 2-4                         [1, 64, 32, 32]           --\n",
      "│    └─MaxPool2d: 2-5                    [1, 64, 16, 16]           --\n",
      "│    └─Conv2d: 2-6                       [1, 128, 16, 16]          73,856\n",
      "│    └─ReLU: 2-7                         [1, 128, 16, 16]          --\n",
      "│    └─Conv2d: 2-8                       [1, 128, 16, 16]          147,584\n",
      "│    └─ReLU: 2-9                         [1, 128, 16, 16]          --\n",
      "│    └─MaxPool2d: 2-10                   [1, 128, 8, 8]            --\n",
      "│    └─Conv2d: 2-11                      [1, 256, 8, 8]            295,168\n",
      "│    └─ReLU: 2-12                        [1, 256, 8, 8]            --\n",
      "│    └─Conv2d: 2-13                      [1, 256, 8, 8]            590,080\n",
      "│    └─ReLU: 2-14                        [1, 256, 8, 8]            --\n",
      "│    └─Conv2d: 2-15                      [1, 256, 8, 8]            590,080\n",
      "│    └─ReLU: 2-16                        [1, 256, 8, 8]            --\n",
      "│    └─MaxPool2d: 2-17                   [1, 256, 4, 4]            --\n",
      "│    └─Conv2d: 2-18                      [1, 512, 4, 4]            1,180,160\n",
      "│    └─ReLU: 2-19                        [1, 512, 4, 4]            --\n",
      "│    └─Conv2d: 2-20                      [1, 512, 4, 4]            2,359,808\n",
      "│    └─ReLU: 2-21                        [1, 512, 4, 4]            --\n",
      "│    └─Conv2d: 2-22                      [1, 512, 4, 4]            2,359,808\n",
      "│    └─ReLU: 2-23                        [1, 512, 4, 4]            --\n",
      "│    └─MaxPool2d: 2-24                   [1, 512, 2, 2]            --\n",
      "│    └─Conv2d: 2-25                      [1, 512, 2, 2]            2,359,808\n",
      "│    └─ReLU: 2-26                        [1, 512, 2, 2]            --\n",
      "│    └─Conv2d: 2-27                      [1, 512, 2, 2]            2,359,808\n",
      "│    └─ReLU: 2-28                        [1, 512, 2, 2]            --\n",
      "│    └─Conv2d: 2-29                      [1, 512, 2, 2]            2,359,808\n",
      "│    └─ReLU: 2-30                        [1, 512, 2, 2]            --\n",
      "│    └─MaxPool2d: 2-31                   [1, 512, 1, 1]            --\n",
      "├─Identity: 1-2                          [1, 512, 1, 1]            --\n",
      "├─Linear: 1-3                            [1, 13]                   6,669\n",
      "==========================================================================================\n",
      "Total params: 14,721,357\n",
      "Trainable params: 14,721,357\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 313.48\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 2.21\n",
      "Params size (MB): 58.89\n",
      "Estimated Total Size (MB): 61.11\n",
      "==========================================================================================\n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): Identity()\n",
      "  (classifier): Linear(in_features=512, out_features=13, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# install and import the torchinfo library\n",
    "!pip install torchinfo\n",
    "from torchinfo import summary\n",
    "\n",
    "vgg16 = get_vgg_model(13, 1)\n",
    "# batch_size = 256\n",
    "print(summary(vgg16, input_size=(1, 3, 32, 32)))\n",
    "#print(summary(vgg16, input_size=(1, 3, 224, 224)))\n",
    "\n",
    "# summary(vgg16, input_size=(batch_size, 3, 224, 224))\n",
    "# summary(models.vgg16(), input_size=(1, 3, 32, 32))\n",
    "print(vgg16)\n",
    "#vgg16 = models.vgg16()\n",
    "#print(vgg16)\n",
    "\n",
    "\n",
    "# print(vgg16)\n",
    "\n",
    "# vgg16.apply(init_weights)\n",
    "# print(\"dummy\")\n",
    "# for child in vgg16.named_children():\n",
    "#   # print(\"x\")\n",
    "#   print(child)\n",
    "\n",
    "# for name, param in vgg16.named_parameters():\n",
    "#   print(name)\n",
    "#   print(param.size())\n",
    "#   print(param[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1626643197966,
     "user": {
      "displayName": "Ertuğrul Güngör",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicvIzQgyHfsqD4h979_wDnn11aI-XtLnxmnC_8=s64",
      "userId": "06165302180630202587"
     },
     "user_tz": -180
    },
    "id": "6N3MbPVav4wt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "def get_cifar10_sets(num_examples_upstream, num_examples_downstream, num_classes_upstream, \\\n",
    "                     num_classes_downstream, is_downstream_random, batch_size):\n",
    "  # @TODO: Check if it's the same as paper\n",
    "  TF = transforms.Compose([\n",
    "      #transforms.Resize(256),\n",
    "      #transforms.CenterCrop(224),\n",
    "      transforms.ToTensor(),\n",
    "      # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "  ])\n",
    "\n",
    "  # num_examples_upstream = 20000\n",
    "  # num_examples_downstream = 10000\n",
    "  # num_classes_upstream = 30\n",
    "  # num_classes_downstream = 15\n",
    "  # is_downstream_random = False\n",
    "\n",
    "  trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=TF)\n",
    "  # test and transform??\n",
    "  testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=TF)\n",
    "\n",
    "  # Create random indices for splitting upstream and downstream tasks\n",
    "  indices = np.arange(0,len(trainset))\n",
    "  np.random.seed(12345)\n",
    "  np.random.shuffle(indices)\n",
    "  indices = indices.tolist()\n",
    "\n",
    "  # Make upstream labels random\n",
    "  temp = np.array(trainset.targets)\n",
    "  temp[indices[:num_examples_upstream]] = np.random.randint(num_classes_upstream, size=num_examples_upstream)\n",
    "  \n",
    "  \n",
    "  # If the downstream task uses random labels\n",
    "  if is_downstream_random:\n",
    "    np.random.seed(12345)\n",
    "    temp[indices[-num_examples_downstream:]] = np.random.randint(num_classes_downstream, size=num_examples_downstream).tolist()\n",
    "    # print(temp[indices[-10:]])\n",
    "    # print(temp[indices[-50:-40]])\n",
    "  trainset.targets = [int(label) for label in temp]\n",
    "\n",
    "  #print(indices[-25:])\n",
    "  # Build dataloaders for upstream and downstream tasks\n",
    "  train_upstream_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=False,\n",
    "                                                sampler=torch.utils.data.SubsetRandomSampler(indices[:num_examples_upstream]))\n",
    "\n",
    "  train_downstream_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=False,\n",
    "                                                sampler=torch.utils.data.SubsetRandomSampler(indices[-num_examples_downstream:]))\n",
    "\n",
    "  test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "  # To check the random labels\n",
    "  '''\n",
    "  u_batch = next(iter(train_upstream_loader))\n",
    "  d_batch = next(iter(train_downstream_loader))\n",
    "\n",
    "  u_labels = u_batch[1]\n",
    "  print(u_labels)\n",
    "  d_labels = d_batch[1]\n",
    "  print(d_labels)\n",
    "  '''\n",
    "  return (train_upstream_loader, train_downstream_loader, test_loader)\n",
    "\n",
    "\n",
    "def get_cifar10_orig():\n",
    "  transform = transforms.Compose(\n",
    "      [transforms.ToTensor(),\n",
    "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "  batch_size = 256\n",
    "\n",
    "\n",
    "  trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                          download=False, transform=transform)\n",
    "  indices = np.arange(0,len(trainset))\n",
    "  trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=2, \n",
    "                                            sampler=torch.utils.data.SubsetRandomSampler(indices[:5000]))\n",
    "\n",
    "  testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=False, transform=transform)\n",
    "  testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "  return trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1626643197967,
     "user": {
      "displayName": "Ertuğrul Güngör",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicvIzQgyHfsqD4h979_wDnn11aI-XtLnxmnC_8=s64",
      "userId": "06165302180630202587"
     },
     "user_tz": -180
    },
    "id": "oxh4UOW0g5rq"
   },
   "outputs": [],
   "source": [
    "def accuracy(model, dataloader):\n",
    "  correct = 0\n",
    "  total = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "      inputs, labels = data\n",
    "      outputs = model(inputs)\n",
    "      _, predicted = torch.max(outputs.data, 1)\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted == labels).sum().item()\n",
    "\n",
    "  return (correct / total)\n",
    "\n",
    "def train(model, criterion, optimizer, epochs, dataloader, device, scheduler=None,\n",
    "          acc_mode=\"none\", acc_dataloaders=None, verbose=True):\n",
    "  loss_history = []\n",
    "  train_accuracies = []\n",
    "  test_accuracies = []\n",
    "  for epoch in range(1,epochs+1):\n",
    "    count = 0\n",
    "    for i, data in enumerate(dataloader, 0):     \n",
    "      # print(\"{}.th batch\".format(i))\n",
    "      # Our batch:\n",
    "      inputs, labels = data\n",
    "      inputs = inputs.to(device)\n",
    "      labels = labels.to(device)\n",
    "      # print(\"batching!\")\n",
    "\n",
    "      # zero the gradients as PyTorch accumulates them\n",
    "      optimizer.zero_grad()\n",
    "      # print(\"zero grad!\")\n",
    "\n",
    "      # Obtain the scores\n",
    "      outputs = model(inputs)\n",
    "      # print(\"feedforward!\")\n",
    "\n",
    "      # Calculate loss\n",
    "      loss = criterion(outputs.to(device), labels)\n",
    "      # print(\"loss!\")\n",
    "\n",
    "      # Backpropagate\n",
    "      loss.backward()\n",
    "      # print(\"backward!\")\n",
    "\n",
    "      # Update the weights\n",
    "      optimizer.step()\n",
    "      # print(\"update!\")\n",
    "      \n",
    "      loss_history.append(loss.item())\n",
    "      count += 1\n",
    "      #print(\"mini batch counter: {}\".format(count))\n",
    "      if (count % 15 == 0):\n",
    "          if acc_mode==\"both\":\n",
    "            train_acc = accuracy(model, acc_dataloaders[0])\n",
    "            train_accuracies.append(train_acc)\n",
    "            test_acc = accuracy(model, acc_dataloaders[1])\n",
    "            test_accuracies.append(test_acc)\n",
    "          elif acc_mode==\"train\":\n",
    "            train_acc = accuracy(model, acc_dataloaders[0])\n",
    "            train_accuracies.append(train_acc)\n",
    "          elif acc_mode==\"test\":\n",
    "            test_acc = accuracy(model, acc_dataloaders[1])\n",
    "            test_accuracies.append(test_acc)\n",
    "    \n",
    "    if verbose: \n",
    "        print(f'Epoch {epoch} / {epochs}: avg. loss of last 5 iterations {np.sum(loss_history[:-6:-1])/5}')\n",
    "        if acc_mode==\"train\" or acc_mode ==\"both\":\n",
    "          print(f'Epoch {epoch} / {epochs}: Last train acc:{train_acc}')\n",
    "        if acc_mode==\"test\" or acc_mode ==\"both\":\n",
    "          print(f'Epoch {epoch} / {epochs}: Last test acc:{test_acc}')\n",
    "\n",
    "    if scheduler is not None:\n",
    "      scheduler.step()\n",
    "      # Print Learning Rate\n",
    "      print('Epoch:', epoch,'LR:', scheduler.get_last_lr())\n",
    "\n",
    "  if acc_mode==\"both\":\n",
    "    return loss_history, train_accuracies, test_accuracies\n",
    "  elif acc_mode==\"train\":\n",
    "    return loss_history, train_accuracies\n",
    "  elif acc_mode==\"test\":\n",
    "    return loss_history, test_accuracies\n",
    "  return loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1626643197968,
     "user": {
      "displayName": "Ertuğrul Güngör",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicvIzQgyHfsqD4h979_wDnn11aI-XtLnxmnC_8=s64",
      "userId": "06165302180630202587"
     },
     "user_tz": -180
    },
    "id": "3hhJodbVnyOi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def get_training_components(learnable_params, init_lr, step_size):\n",
    "  '''\n",
    "  Function to prepare components of the training. These are all the same\n",
    "  throughout the experiments made.\n",
    "  '''\n",
    "  # The problem always be a image classification\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  # Which parameters to be updated depends on if the training is done from scratch\n",
    "  # or with pre-trained model.\n",
    "  optimizer = optim.SGD(learnable_params, lr=init_lr, momentum=0.9)\n",
    "  # Step size depends on the # of epoch, gamma is contant\n",
    "  scheduler = StepLR(optimizer, step_size=step_size, gamma=1/3)\n",
    "  return (criterion, optimizer, scheduler)\n",
    "\n",
    "def get_device():\n",
    "  if torch.cuda.is_available():\n",
    "    print(\"Cuda (GPU support) is available and enabled!\")\n",
    "    device = torch.device(\"cuda\")\n",
    "  else:\n",
    "    print(\"Cuda (GPU support) is not available :(\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "def get_learnable_parameters(model):\n",
    "  params_to_update = []\n",
    "  for name, param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "      #print(name)\n",
    "      params_to_update.append(param)\n",
    "  return params_to_update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "kEy0gUI0n6_p",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "bfb71388-9147-4cde-cda6-e32f82f6753f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda (GPU support) is available and enabled!\n",
      "Epoch 1 / 120: avg. loss of last 5 iterations 1.6133390665054321\n",
      "Epoch: 1 LR: [0.001]\n",
      "Epoch 2 / 120: avg. loss of last 5 iterations 1.6077460765838623\n",
      "Epoch: 2 LR: [0.001]\n",
      "Epoch 3 / 120: avg. loss of last 5 iterations 1.605721402168274\n",
      "Epoch: 3 LR: [0.001]\n",
      "Epoch 4 / 120: avg. loss of last 5 iterations 1.6093560218811036\n",
      "Epoch: 4 LR: [0.001]\n",
      "Epoch 5 / 120: avg. loss of last 5 iterations 1.6039263248443603\n",
      "Epoch: 5 LR: [0.001]\n",
      "Epoch 6 / 120: avg. loss of last 5 iterations 1.5962279558181762\n",
      "Epoch: 6 LR: [0.001]\n",
      "Epoch 7 / 120: avg. loss of last 5 iterations 1.606328511238098\n",
      "Epoch: 7 LR: [0.001]\n",
      "Epoch 8 / 120: avg. loss of last 5 iterations 1.5934141874313354\n",
      "Epoch: 8 LR: [0.001]\n",
      "Epoch 9 / 120: avg. loss of last 5 iterations 1.5921808719635009\n",
      "Epoch: 9 LR: [0.001]\n",
      "Epoch 10 / 120: avg. loss of last 5 iterations 1.5870641946792603\n",
      "Epoch: 10 LR: [0.001]\n",
      "Epoch 11 / 120: avg. loss of last 5 iterations 1.585596537590027\n",
      "Epoch: 11 LR: [0.001]\n",
      "Epoch 12 / 120: avg. loss of last 5 iterations 1.5687595844268798\n",
      "Epoch: 12 LR: [0.001]\n",
      "Epoch 13 / 120: avg. loss of last 5 iterations 1.5809925079345704\n",
      "Epoch: 13 LR: [0.001]\n",
      "Epoch 14 / 120: avg. loss of last 5 iterations 1.5783725023269652\n",
      "Epoch: 14 LR: [0.001]\n",
      "Epoch 15 / 120: avg. loss of last 5 iterations 1.5735558032989503\n",
      "Epoch: 15 LR: [0.001]\n",
      "Epoch 16 / 120: avg. loss of last 5 iterations 1.5668370008468628\n",
      "Epoch: 16 LR: [0.001]\n",
      "Epoch 17 / 120: avg. loss of last 5 iterations 1.5739986181259156\n",
      "Epoch: 17 LR: [0.001]\n",
      "Epoch 18 / 120: avg. loss of last 5 iterations 1.5552118301391602\n",
      "Epoch: 18 LR: [0.001]\n",
      "Epoch 19 / 120: avg. loss of last 5 iterations 1.5475941896438599\n",
      "Epoch: 19 LR: [0.001]\n",
      "Epoch 20 / 120: avg. loss of last 5 iterations 1.5235903978347778\n",
      "Epoch: 20 LR: [0.001]\n",
      "Epoch 21 / 120: avg. loss of last 5 iterations 1.5242175102233886\n",
      "Epoch: 21 LR: [0.001]\n",
      "Epoch 22 / 120: avg. loss of last 5 iterations 1.5049087285995484\n",
      "Epoch: 22 LR: [0.001]\n",
      "Epoch 23 / 120: avg. loss of last 5 iterations 1.4980645895004272\n",
      "Epoch: 23 LR: [0.001]\n",
      "Epoch 24 / 120: avg. loss of last 5 iterations 1.420781183242798\n",
      "Epoch: 24 LR: [0.001]\n",
      "Epoch 25 / 120: avg. loss of last 5 iterations 1.454353666305542\n",
      "Epoch: 25 LR: [0.001]\n",
      "Epoch 26 / 120: avg. loss of last 5 iterations 1.3920888662338258\n",
      "Epoch: 26 LR: [0.001]\n",
      "Epoch 27 / 120: avg. loss of last 5 iterations 1.3393589496612548\n",
      "Epoch: 27 LR: [0.001]\n",
      "Epoch 28 / 120: avg. loss of last 5 iterations 1.4434041738510133\n",
      "Epoch: 28 LR: [0.001]\n",
      "Epoch 29 / 120: avg. loss of last 5 iterations 1.3439536333084106\n",
      "Epoch: 29 LR: [0.001]\n",
      "Epoch 30 / 120: avg. loss of last 5 iterations 1.2786612033843994\n",
      "Epoch: 30 LR: [0.001]\n",
      "Epoch 31 / 120: avg. loss of last 5 iterations 1.1995983123779297\n",
      "Epoch: 31 LR: [0.001]\n",
      "Epoch 32 / 120: avg. loss of last 5 iterations 1.141989493370056\n",
      "Epoch: 32 LR: [0.001]\n",
      "Epoch 33 / 120: avg. loss of last 5 iterations 1.1191924333572387\n",
      "Epoch: 33 LR: [0.001]\n",
      "Epoch 34 / 120: avg. loss of last 5 iterations 0.941384243965149\n",
      "Epoch: 34 LR: [0.001]\n",
      "Epoch 35 / 120: avg. loss of last 5 iterations 1.0341204404830933\n",
      "Epoch: 35 LR: [0.001]\n",
      "Epoch 36 / 120: avg. loss of last 5 iterations 0.9813701748847962\n",
      "Epoch: 36 LR: [0.001]\n",
      "Epoch 37 / 120: avg. loss of last 5 iterations 0.9093497514724731\n",
      "Epoch: 37 LR: [0.001]\n",
      "Epoch 38 / 120: avg. loss of last 5 iterations 0.7521415710449219\n",
      "Epoch: 38 LR: [0.001]\n",
      "Epoch 39 / 120: avg. loss of last 5 iterations 0.7433053731918335\n",
      "Epoch: 39 LR: [0.001]\n",
      "Epoch 40 / 120: avg. loss of last 5 iterations 0.5083985984325409\n",
      "Epoch: 40 LR: [0.0003333333333333333]\n",
      "Epoch 41 / 120: avg. loss of last 5 iterations 0.16531588435173034\n",
      "Epoch: 41 LR: [0.0003333333333333333]\n",
      "Epoch 42 / 120: avg. loss of last 5 iterations 0.08771532624959946\n",
      "Epoch: 42 LR: [0.0003333333333333333]\n",
      "Epoch 43 / 120: avg. loss of last 5 iterations 0.06177257373929024\n",
      "Epoch: 43 LR: [0.0003333333333333333]\n",
      "Epoch 44 / 120: avg. loss of last 5 iterations 0.05253495424985886\n",
      "Epoch: 44 LR: [0.0003333333333333333]\n",
      "Epoch 45 / 120: avg. loss of last 5 iterations 0.0317880779504776\n",
      "Epoch: 45 LR: [0.0003333333333333333]\n",
      "Epoch 46 / 120: avg. loss of last 5 iterations 0.023367162607610226\n",
      "Epoch: 46 LR: [0.0003333333333333333]\n",
      "Epoch 47 / 120: avg. loss of last 5 iterations 0.020608646981418132\n",
      "Epoch: 47 LR: [0.0003333333333333333]\n",
      "Epoch 48 / 120: avg. loss of last 5 iterations 0.013434073887765408\n",
      "Epoch: 48 LR: [0.0003333333333333333]\n",
      "Epoch 49 / 120: avg. loss of last 5 iterations 0.010446204245090485\n",
      "Epoch: 49 LR: [0.0003333333333333333]\n",
      "Epoch 50 / 120: avg. loss of last 5 iterations 0.010063219256699086\n",
      "Epoch: 50 LR: [0.0003333333333333333]\n",
      "Epoch 51 / 120: avg. loss of last 5 iterations 0.005871854070574045\n",
      "Epoch: 51 LR: [0.0003333333333333333]\n",
      "Epoch 52 / 120: avg. loss of last 5 iterations 0.0058335836976766585\n",
      "Epoch: 52 LR: [0.0003333333333333333]\n",
      "Epoch 53 / 120: avg. loss of last 5 iterations 0.005639694817364216\n",
      "Epoch: 53 LR: [0.0003333333333333333]\n",
      "Epoch 54 / 120: avg. loss of last 5 iterations 0.00445226514711976\n",
      "Epoch: 54 LR: [0.0003333333333333333]\n",
      "Epoch 55 / 120: avg. loss of last 5 iterations 0.005265107844024897\n",
      "Epoch: 55 LR: [0.0003333333333333333]\n",
      "Epoch 56 / 120: avg. loss of last 5 iterations 0.003107399633154273\n",
      "Epoch: 56 LR: [0.0003333333333333333]\n",
      "Epoch 57 / 120: avg. loss of last 5 iterations 0.004198432713747024\n",
      "Epoch: 57 LR: [0.0003333333333333333]\n",
      "Epoch 58 / 120: avg. loss of last 5 iterations 0.004680575849488377\n",
      "Epoch: 58 LR: [0.0003333333333333333]\n",
      "Epoch 59 / 120: avg. loss of last 5 iterations 0.0037512749433517454\n",
      "Epoch: 59 LR: [0.0003333333333333333]\n",
      "Epoch 60 / 120: avg. loss of last 5 iterations 0.003122447314672172\n",
      "Epoch: 60 LR: [0.0003333333333333333]\n",
      "Epoch 61 / 120: avg. loss of last 5 iterations 0.002919738367199898\n",
      "Epoch: 61 LR: [0.0003333333333333333]\n",
      "Epoch 62 / 120: avg. loss of last 5 iterations 0.0022822163300588727\n",
      "Epoch: 62 LR: [0.0003333333333333333]\n",
      "Epoch 63 / 120: avg. loss of last 5 iterations 0.002941936859861016\n",
      "Epoch: 63 LR: [0.0003333333333333333]\n",
      "Epoch 64 / 120: avg. loss of last 5 iterations 0.0021619923179969193\n",
      "Epoch: 64 LR: [0.0003333333333333333]\n",
      "Epoch 65 / 120: avg. loss of last 5 iterations 0.0025573860853910445\n",
      "Epoch: 65 LR: [0.0003333333333333333]\n",
      "Epoch 66 / 120: avg. loss of last 5 iterations 0.0022911303443834186\n",
      "Epoch: 66 LR: [0.0003333333333333333]\n",
      "Epoch 67 / 120: avg. loss of last 5 iterations 0.0024990621022880077\n",
      "Epoch: 67 LR: [0.0003333333333333333]\n",
      "Epoch 68 / 120: avg. loss of last 5 iterations 0.0019228694960474967\n",
      "Epoch: 68 LR: [0.0003333333333333333]\n",
      "Epoch 69 / 120: avg. loss of last 5 iterations 0.0017832560581155122\n",
      "Epoch: 69 LR: [0.0003333333333333333]\n",
      "Epoch 70 / 120: avg. loss of last 5 iterations 0.0015876588295213878\n",
      "Epoch: 70 LR: [0.0003333333333333333]\n",
      "Epoch 71 / 120: avg. loss of last 5 iterations 0.0017819527070969342\n",
      "Epoch: 71 LR: [0.0003333333333333333]\n",
      "Epoch 72 / 120: avg. loss of last 5 iterations 0.0015759447356685995\n",
      "Epoch: 72 LR: [0.0003333333333333333]\n",
      "Epoch 73 / 120: avg. loss of last 5 iterations 0.0014344155322760343\n",
      "Epoch: 73 LR: [0.0003333333333333333]\n",
      "Epoch 74 / 120: avg. loss of last 5 iterations 0.001556929771322757\n",
      "Epoch: 74 LR: [0.0003333333333333333]\n",
      "Epoch 75 / 120: avg. loss of last 5 iterations 0.0014289276441559196\n",
      "Epoch: 75 LR: [0.0003333333333333333]\n",
      "Epoch 76 / 120: avg. loss of last 5 iterations 0.0013736643828451633\n",
      "Epoch: 76 LR: [0.0003333333333333333]\n",
      "Epoch 77 / 120: avg. loss of last 5 iterations 0.0015072840498760343\n",
      "Epoch: 77 LR: [0.0003333333333333333]\n",
      "Epoch 78 / 120: avg. loss of last 5 iterations 0.0012448810273781418\n",
      "Epoch: 78 LR: [0.0003333333333333333]\n",
      "Epoch 79 / 120: avg. loss of last 5 iterations 0.0011751772835850716\n",
      "Epoch: 79 LR: [0.0003333333333333333]\n",
      "Epoch 80 / 120: avg. loss of last 5 iterations 0.001176391076296568\n",
      "Epoch: 80 LR: [0.0001111111111111111]\n",
      "Epoch 81 / 120: avg. loss of last 5 iterations 0.0011303410166874528\n",
      "Epoch: 81 LR: [0.0001111111111111111]\n",
      "Epoch 82 / 120: avg. loss of last 5 iterations 0.0012265273137018085\n",
      "Epoch: 82 LR: [0.0001111111111111111]\n",
      "Epoch 83 / 120: avg. loss of last 5 iterations 0.0012022346490994096\n",
      "Epoch: 83 LR: [0.0001111111111111111]\n",
      "Epoch 84 / 120: avg. loss of last 5 iterations 0.0013010873226448894\n",
      "Epoch: 84 LR: [0.0001111111111111111]\n",
      "Epoch 85 / 120: avg. loss of last 5 iterations 0.0012377497972920538\n",
      "Epoch: 85 LR: [0.0001111111111111111]\n",
      "Epoch 86 / 120: avg. loss of last 5 iterations 0.0012621544301509857\n",
      "Epoch: 86 LR: [0.0001111111111111111]\n",
      "Epoch 87 / 120: avg. loss of last 5 iterations 0.001103963830973953\n",
      "Epoch: 87 LR: [0.0001111111111111111]\n",
      "Epoch 88 / 120: avg. loss of last 5 iterations 0.0010658274055458604\n",
      "Epoch: 88 LR: [0.0001111111111111111]\n",
      "Epoch 89 / 120: avg. loss of last 5 iterations 0.0009763428883161396\n",
      "Epoch: 89 LR: [0.0001111111111111111]\n",
      "Epoch 90 / 120: avg. loss of last 5 iterations 0.000948563520796597\n",
      "Epoch: 90 LR: [0.0001111111111111111]\n",
      "Epoch 91 / 120: avg. loss of last 5 iterations 0.0010439090081490576\n",
      "Epoch: 91 LR: [0.0001111111111111111]\n",
      "Epoch 92 / 120: avg. loss of last 5 iterations 0.0011610556743107737\n",
      "Epoch: 92 LR: [0.0001111111111111111]\n",
      "Epoch 93 / 120: avg. loss of last 5 iterations 0.0009884675848297775\n",
      "Epoch: 93 LR: [0.0001111111111111111]\n",
      "Epoch 94 / 120: avg. loss of last 5 iterations 0.000958879035897553\n",
      "Epoch: 94 LR: [0.0001111111111111111]\n",
      "Epoch 95 / 120: avg. loss of last 5 iterations 0.0009473009500652552\n",
      "Epoch: 95 LR: [0.0001111111111111111]\n",
      "Epoch 96 / 120: avg. loss of last 5 iterations 0.001093560189474374\n",
      "Epoch: 96 LR: [0.0001111111111111111]\n",
      "Epoch 97 / 120: avg. loss of last 5 iterations 0.0009911023429594935\n",
      "Epoch: 97 LR: [0.0001111111111111111]\n",
      "Epoch 98 / 120: avg. loss of last 5 iterations 0.0012914520455524325\n",
      "Epoch: 98 LR: [0.0001111111111111111]\n",
      "Epoch 99 / 120: avg. loss of last 5 iterations 0.0009948133258149028\n",
      "Epoch: 99 LR: [0.0001111111111111111]\n",
      "Epoch 100 / 120: avg. loss of last 5 iterations 0.000973532721400261\n",
      "Epoch: 100 LR: [0.0001111111111111111]\n",
      "Epoch 101 / 120: avg. loss of last 5 iterations 0.0009923173813149333\n",
      "Epoch: 101 LR: [0.0001111111111111111]\n",
      "Epoch 102 / 120: avg. loss of last 5 iterations 0.0009507615119218827\n",
      "Epoch: 102 LR: [0.0001111111111111111]\n",
      "Epoch 103 / 120: avg. loss of last 5 iterations 0.0010904768481850625\n",
      "Epoch: 103 LR: [0.0001111111111111111]\n",
      "Epoch 104 / 120: avg. loss of last 5 iterations 0.0009849007590673863\n",
      "Epoch: 104 LR: [0.0001111111111111111]\n",
      "Epoch 105 / 120: avg. loss of last 5 iterations 0.0008487807004712522\n",
      "Epoch: 105 LR: [0.0001111111111111111]\n",
      "Epoch 106 / 120: avg. loss of last 5 iterations 0.000931233202572912\n",
      "Epoch: 106 LR: [0.0001111111111111111]\n",
      "Epoch 107 / 120: avg. loss of last 5 iterations 0.001007501222193241\n",
      "Epoch: 107 LR: [0.0001111111111111111]\n",
      "Epoch 108 / 120: avg. loss of last 5 iterations 0.0008692001225426793\n",
      "Epoch: 108 LR: [0.0001111111111111111]\n",
      "Epoch 109 / 120: avg. loss of last 5 iterations 0.0009375131106935441\n",
      "Epoch: 109 LR: [0.0001111111111111111]\n",
      "Epoch 110 / 120: avg. loss of last 5 iterations 0.0010114225442521274\n",
      "Epoch: 110 LR: [0.0001111111111111111]\n",
      "Epoch 111 / 120: avg. loss of last 5 iterations 0.0008923195651732385\n",
      "Epoch: 111 LR: [0.0001111111111111111]\n",
      "Epoch 112 / 120: avg. loss of last 5 iterations 0.000938081310596317\n",
      "Epoch: 112 LR: [0.0001111111111111111]\n",
      "Epoch 113 / 120: avg. loss of last 5 iterations 0.0008143839775584638\n",
      "Epoch: 113 LR: [0.0001111111111111111]\n",
      "Epoch 114 / 120: avg. loss of last 5 iterations 0.000898011855315417\n",
      "Epoch: 114 LR: [0.0001111111111111111]\n",
      "Epoch 115 / 120: avg. loss of last 5 iterations 0.0008408130262978375\n",
      "Epoch: 115 LR: [0.0001111111111111111]\n",
      "Epoch 116 / 120: avg. loss of last 5 iterations 0.0008666026871651411\n",
      "Epoch: 116 LR: [0.0001111111111111111]\n",
      "Epoch 117 / 120: avg. loss of last 5 iterations 0.000956130085978657\n",
      "Epoch: 117 LR: [0.0001111111111111111]\n",
      "Epoch 118 / 120: avg. loss of last 5 iterations 0.0008709275512956083\n",
      "Epoch: 118 LR: [0.0001111111111111111]\n",
      "Epoch 119 / 120: avg. loss of last 5 iterations 0.0009223309927619994\n",
      "Epoch: 119 LR: [0.0001111111111111111]\n",
      "Epoch 120 / 120: avg. loss of last 5 iterations 0.0007970271864905954\n",
      "Epoch: 120 LR: [3.703703703703703e-05]\n"
     ]
    }
   ],
   "source": [
    "from torch import linalg as LA\n",
    "\n",
    "train_upstream_loader, train_downstream_loader, test_loader = get_cifar10_sets(\n",
    "    num_examples_upstream = 20000,\n",
    "    num_examples_downstream = 20000,\n",
    "    num_classes_upstream = 5,\n",
    "    num_classes_downstream = 5,\n",
    "    is_downstream_random = False,\n",
    "    batch_size = 256\n",
    "  )\n",
    "\n",
    "model = get_vgg_model(5, 1)\n",
    "norm_dict = {}\n",
    "for name, param in model.named_parameters():\n",
    "  norm_dict[name] = LA.norm(param)\n",
    "\n",
    "epochs = 120\n",
    "learnable_parameters = get_learnable_parameters(model)\n",
    "criterion, optimizer, scheduler = get_training_components(learnable_parameters, 0.001, (int)(epochs/3))\n",
    "\n",
    "device = get_device()\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "loss_history = train(model, criterion, optimizer, epochs, train_upstream_loader, device, scheduler=scheduler)\n",
    "\n",
    "# scale norms back to initial values after upstream training\n",
    "for name, param in model.named_parameters():\n",
    "  scale = norm_dict[name]\n",
    "  param.data.copy_(param*(scale.item()/LA.norm(param)))\n",
    "\n",
    "torch.save(model.state_dict(), \"./pretrained_model_dict_upstream_1_2.pt\")\n",
    "torch.save(model, \"./pretrained_model_upstream_1_2.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def setup_and_train_downstream(epochs, lr, step_size, num_examples_upstream, num_examples_downstream,\n",
    "                   num_classes_upstream, num_classes_downstream, is_downstream_random,\n",
    "                   batch_size, num_outputs, upstream_model_path=None):\n",
    "    \n",
    "    train_upstream_loader, train_downstream_loader, test_loader = get_cifar10_sets(\n",
    "        num_examples_upstream, num_examples_downstream, num_classes_upstream,\n",
    "        num_classes_downstream, is_downstream_random, batch_size)\n",
    "    \n",
    "    _, acc_train_loader, acc_test_loader = get_cifar10_sets(\n",
    "        num_examples_upstream = 5000,\n",
    "        num_examples_downstream = num_examples_downstream,\n",
    "        num_classes_upstream = 5,\n",
    "        num_classes_downstream = num_classes_downstream,  \n",
    "        is_downstream_random = is_downstream_random,\n",
    "        batch_size = num_examples_downstream\n",
    "    )\n",
    "\n",
    "    model = get_vgg_model(num_classes_downstream, 1)\n",
    "    if upstream_model_path is not None:\n",
    "        model.load_state_dict(torch.load(upstream_model_path), strict=False)\n",
    "        model.classifier = nn.Linear(512, num_outputs)\n",
    "        torch.nn.init.kaiming_normal_(model.classifier.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    learnable_parameters = get_learnable_parameters(model)\n",
    "    criterion, optimizer, scheduler = get_training_components(learnable_parameters, lr, step_size)\n",
    "    device = get_device()\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    if not is_downstream_random:\n",
    "      loss_history, train_accuracies, test_accuracies = train(model, criterion, optimizer, epochs, train_downstream_loader, \n",
    "                                                              device, scheduler, \"both\",\n",
    "                                                              [acc_train_loader, acc_test_loader])\n",
    "      return loss_history, train_accuracies, test_accuracies\n",
    "    loss_history, train_accuracies = train(model, criterion, optimizer, epochs, train_downstream_loader, \n",
    "                                           device, scheduler, \"train\",\n",
    "                                           [acc_train_loader])\n",
    "    return loss_history, train_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda (GPU support) is available and enabled!\n",
      "Epoch 1 / 40: avg. loss of last 5 iterations 1.9065651416778564\n",
      "Epoch 1 / 40: Last train acc:0.29745\n",
      "Epoch 1 / 40: Last test acc:0.3024\n",
      "Epoch: 1 LR: [0.001]\n",
      "Epoch 2 / 40: avg. loss of last 5 iterations 1.6211234331130981\n",
      "Epoch 2 / 40: Last train acc:0.42845\n",
      "Epoch 2 / 40: Last test acc:0.428\n",
      "Epoch: 2 LR: [0.001]\n",
      "Epoch 3 / 40: avg. loss of last 5 iterations 1.5648954391479493\n",
      "Epoch 3 / 40: Last train acc:0.42495\n",
      "Epoch 3 / 40: Last test acc:0.4159\n",
      "Epoch: 3 LR: [0.001]\n",
      "Epoch 4 / 40: avg. loss of last 5 iterations 1.3323709487915039\n",
      "Epoch 4 / 40: Last train acc:0.51525\n",
      "Epoch 4 / 40: Last test acc:0.4925\n",
      "Epoch: 4 LR: [0.001]\n",
      "Epoch 5 / 40: avg. loss of last 5 iterations 1.3845954179763793\n",
      "Epoch 5 / 40: Last train acc:0.5284\n",
      "Epoch 5 / 40: Last test acc:0.5056\n",
      "Epoch: 5 LR: [0.001]\n",
      "Epoch 6 / 40: avg. loss of last 5 iterations 1.308006715774536\n",
      "Epoch 6 / 40: Last train acc:0.5417\n",
      "Epoch 6 / 40: Last test acc:0.5077\n",
      "Epoch: 6 LR: [0.001]\n",
      "Epoch 7 / 40: avg. loss of last 5 iterations 1.258984589576721\n",
      "Epoch 7 / 40: Last train acc:0.5807\n",
      "Epoch 7 / 40: Last test acc:0.5383\n",
      "Epoch: 7 LR: [0.001]\n",
      "Epoch 8 / 40: avg. loss of last 5 iterations 1.1498520851135254\n",
      "Epoch 8 / 40: Last train acc:0.607\n",
      "Epoch 8 / 40: Last test acc:0.5521\n",
      "Epoch: 8 LR: [0.001]\n",
      "Epoch 9 / 40: avg. loss of last 5 iterations 1.0485374808311463\n",
      "Epoch 9 / 40: Last train acc:0.5965\n",
      "Epoch 9 / 40: Last test acc:0.5424\n",
      "Epoch: 9 LR: [0.001]\n",
      "Epoch 10 / 40: avg. loss of last 5 iterations 1.167632246017456\n",
      "Epoch 10 / 40: Last train acc:0.6491\n",
      "Epoch 10 / 40: Last test acc:0.5827\n",
      "Epoch: 10 LR: [0.001]\n",
      "Epoch 11 / 40: avg. loss of last 5 iterations 1.0720869302749634\n",
      "Epoch 11 / 40: Last train acc:0.6713\n",
      "Epoch 11 / 40: Last test acc:0.5856\n",
      "Epoch: 11 LR: [0.001]\n",
      "Epoch 12 / 40: avg. loss of last 5 iterations 0.9823662519454956\n",
      "Epoch 12 / 40: Last train acc:0.7016\n",
      "Epoch 12 / 40: Last test acc:0.6023\n",
      "Epoch: 12 LR: [0.001]\n",
      "Epoch 13 / 40: avg. loss of last 5 iterations 0.9022251963615417\n",
      "Epoch 13 / 40: Last train acc:0.6841\n",
      "Epoch 13 / 40: Last test acc:0.5808\n",
      "Epoch: 13 LR: [0.0003333333333333333]\n",
      "Epoch 14 / 40: avg. loss of last 5 iterations 0.7242270946502686\n",
      "Epoch 14 / 40: Last train acc:0.77335\n",
      "Epoch 14 / 40: Last test acc:0.6211\n",
      "Epoch: 14 LR: [0.0003333333333333333]\n",
      "Epoch 15 / 40: avg. loss of last 5 iterations 0.6802717924118042\n",
      "Epoch 15 / 40: Last train acc:0.78345\n",
      "Epoch 15 / 40: Last test acc:0.6227\n",
      "Epoch: 15 LR: [0.0003333333333333333]\n",
      "Epoch 16 / 40: avg. loss of last 5 iterations 0.5834593951702118\n",
      "Epoch 16 / 40: Last train acc:0.7901\n",
      "Epoch 16 / 40: Last test acc:0.6122\n",
      "Epoch: 16 LR: [0.0003333333333333333]\n",
      "Epoch 17 / 40: avg. loss of last 5 iterations 0.6310210227966309\n",
      "Epoch 17 / 40: Last train acc:0.829\n",
      "Epoch 17 / 40: Last test acc:0.631\n",
      "Epoch: 17 LR: [0.0003333333333333333]\n",
      "Epoch 18 / 40: avg. loss of last 5 iterations 0.5477145910263062\n",
      "Epoch 18 / 40: Last train acc:0.82415\n",
      "Epoch 18 / 40: Last test acc:0.6269\n",
      "Epoch: 18 LR: [0.0003333333333333333]\n",
      "Epoch 19 / 40: avg. loss of last 5 iterations 0.521478682756424\n",
      "Epoch 19 / 40: Last train acc:0.8443\n",
      "Epoch 19 / 40: Last test acc:0.6231\n",
      "Epoch: 19 LR: [0.0003333333333333333]\n",
      "Epoch 20 / 40: avg. loss of last 5 iterations 0.5333707571029663\n",
      "Epoch 20 / 40: Last train acc:0.8461\n",
      "Epoch 20 / 40: Last test acc:0.6124\n",
      "Epoch: 20 LR: [0.0003333333333333333]\n",
      "Epoch 21 / 40: avg. loss of last 5 iterations 0.43401015996932985\n",
      "Epoch 21 / 40: Last train acc:0.89015\n",
      "Epoch 21 / 40: Last test acc:0.6281\n",
      "Epoch: 21 LR: [0.0003333333333333333]\n",
      "Epoch 22 / 40: avg. loss of last 5 iterations 0.42203956842422485\n",
      "Epoch 22 / 40: Last train acc:0.87955\n",
      "Epoch 22 / 40: Last test acc:0.6147\n",
      "Epoch: 22 LR: [0.0003333333333333333]\n",
      "Epoch 23 / 40: avg. loss of last 5 iterations 0.3800545632839203\n",
      "Epoch 23 / 40: Last train acc:0.90195\n",
      "Epoch 23 / 40: Last test acc:0.6223\n",
      "Epoch: 23 LR: [0.0003333333333333333]\n",
      "Epoch 24 / 40: avg. loss of last 5 iterations 0.3354245722293854\n",
      "Epoch 24 / 40: Last train acc:0.91765\n",
      "Epoch 24 / 40: Last test acc:0.6273\n",
      "Epoch: 24 LR: [0.0003333333333333333]\n",
      "Epoch 25 / 40: avg. loss of last 5 iterations 0.31047523617744444\n",
      "Epoch 25 / 40: Last train acc:0.925\n",
      "Epoch 25 / 40: Last test acc:0.6224\n",
      "Epoch: 25 LR: [0.0003333333333333333]\n",
      "Epoch 26 / 40: avg. loss of last 5 iterations 0.26400187611579895\n",
      "Epoch 26 / 40: Last train acc:0.94325\n",
      "Epoch 26 / 40: Last test acc:0.6229\n",
      "Epoch: 26 LR: [0.0001111111111111111]\n",
      "Epoch 27 / 40: avg. loss of last 5 iterations 0.14208401441574098\n",
      "Epoch 27 / 40: Last train acc:0.9743\n",
      "Epoch 27 / 40: Last test acc:0.6314\n",
      "Epoch: 27 LR: [0.0001111111111111111]\n",
      "Epoch 28 / 40: avg. loss of last 5 iterations 0.1685919314622879\n",
      "Epoch 28 / 40: Last train acc:0.97575\n",
      "Epoch 28 / 40: Last test acc:0.6338\n",
      "Epoch: 28 LR: [0.0001111111111111111]\n",
      "Epoch 29 / 40: avg. loss of last 5 iterations 0.1237382173538208\n",
      "Epoch 29 / 40: Last train acc:0.98205\n",
      "Epoch 29 / 40: Last test acc:0.6325\n",
      "Epoch: 29 LR: [0.0001111111111111111]\n",
      "Epoch 30 / 40: avg. loss of last 5 iterations 0.13877274543046952\n",
      "Epoch 30 / 40: Last train acc:0.9806\n",
      "Epoch 30 / 40: Last test acc:0.6305\n",
      "Epoch: 30 LR: [0.0001111111111111111]\n",
      "Epoch 31 / 40: avg. loss of last 5 iterations 0.10527764111757279\n",
      "Epoch 31 / 40: Last train acc:0.9854\n",
      "Epoch 31 / 40: Last test acc:0.6311\n",
      "Epoch: 31 LR: [0.0001111111111111111]\n",
      "Epoch 32 / 40: avg. loss of last 5 iterations 0.11101250797510147\n",
      "Epoch 32 / 40: Last train acc:0.9877\n",
      "Epoch 32 / 40: Last test acc:0.6308\n",
      "Epoch: 32 LR: [0.0001111111111111111]\n",
      "Epoch 33 / 40: avg. loss of last 5 iterations 0.1126952663064003\n",
      "Epoch 33 / 40: Last train acc:0.9877\n",
      "Epoch 33 / 40: Last test acc:0.6291\n",
      "Epoch: 33 LR: [0.0001111111111111111]\n",
      "Epoch 34 / 40: avg. loss of last 5 iterations 0.11471676826477051\n",
      "Epoch 34 / 40: Last train acc:0.99165\n",
      "Epoch 34 / 40: Last test acc:0.6286\n",
      "Epoch: 34 LR: [0.0001111111111111111]\n",
      "Epoch 35 / 40: avg. loss of last 5 iterations 0.08657175526022912\n",
      "Epoch 35 / 40: Last train acc:0.9931\n",
      "Epoch 35 / 40: Last test acc:0.6286\n",
      "Epoch: 35 LR: [0.0001111111111111111]\n",
      "Epoch 36 / 40: avg. loss of last 5 iterations 0.07375832945108414\n",
      "Epoch 36 / 40: Last train acc:0.9936\n",
      "Epoch 36 / 40: Last test acc:0.6304\n",
      "Epoch: 36 LR: [0.0001111111111111111]\n",
      "Epoch 37 / 40: avg. loss of last 5 iterations 0.08441999405622483\n",
      "Epoch 37 / 40: Last train acc:0.99555\n",
      "Epoch 37 / 40: Last test acc:0.6292\n",
      "Epoch: 37 LR: [0.0001111111111111111]\n",
      "Epoch 38 / 40: avg. loss of last 5 iterations 0.06580742746591568\n",
      "Epoch 38 / 40: Last train acc:0.9961\n",
      "Epoch 38 / 40: Last test acc:0.6289\n",
      "Epoch: 38 LR: [0.0001111111111111111]\n",
      "Epoch 39 / 40: avg. loss of last 5 iterations 0.07080544009804726\n",
      "Epoch 39 / 40: Last train acc:0.9972\n",
      "Epoch 39 / 40: Last test acc:0.6309\n",
      "Epoch: 39 LR: [3.703703703703703e-05]\n",
      "Epoch 40 / 40: avg. loss of last 5 iterations 0.06409489661455155\n",
      "Epoch 40 / 40: Last train acc:0.9975\n",
      "Epoch 40 / 40: Last test acc:0.6313\n",
      "Epoch: 40 LR: [3.703703703703703e-05]\n"
     ]
    }
   ],
   "source": [
    "# From scratch with real labels\n",
    "loss_scratch_real, accs_scratch_real_train, accs_scratch_real_test = setup_and_train_downstream(\n",
    "    epochs=40, lr=0.001, step_size=(int)(40/3), num_examples_upstream=20000, \n",
    "    num_examples_downstream=20000, num_classes_upstream = 5, num_classes_downstream = 10,\n",
    "    is_downstream_random = False, batch_size = 256, num_outputs=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda (GPU support) is available and enabled!\n",
      "Epoch 1 / 40: avg. loss of last 5 iterations 1.8364093780517579\n",
      "Epoch 1 / 40: Last train acc:0.35295\n",
      "Epoch 1 / 40: Last test acc:0.3466\n",
      "Epoch: 1 LR: [0.001]\n",
      "Epoch 2 / 40: avg. loss of last 5 iterations 1.5792887926101684\n",
      "Epoch 2 / 40: Last train acc:0.44275\n",
      "Epoch 2 / 40: Last test acc:0.4279\n",
      "Epoch: 2 LR: [0.001]\n",
      "Epoch 3 / 40: avg. loss of last 5 iterations 1.4937051296234132\n",
      "Epoch 3 / 40: Last train acc:0.49845\n",
      "Epoch 3 / 40: Last test acc:0.466\n",
      "Epoch: 3 LR: [0.001]\n",
      "Epoch 4 / 40: avg. loss of last 5 iterations 1.3794410705566407\n",
      "Epoch 4 / 40: Last train acc:0.5465\n",
      "Epoch 4 / 40: Last test acc:0.4972\n",
      "Epoch: 4 LR: [0.001]\n",
      "Epoch 5 / 40: avg. loss of last 5 iterations 1.3808815956115723\n",
      "Epoch 5 / 40: Last train acc:0.54975\n",
      "Epoch 5 / 40: Last test acc:0.4885\n",
      "Epoch: 5 LR: [0.001]\n",
      "Epoch 6 / 40: avg. loss of last 5 iterations 1.1478238105773926\n",
      "Epoch 6 / 40: Last train acc:0.61995\n",
      "Epoch 6 / 40: Last test acc:0.5242\n",
      "Epoch: 6 LR: [0.001]\n",
      "Epoch 7 / 40: avg. loss of last 5 iterations 1.154151201248169\n",
      "Epoch 7 / 40: Last train acc:0.60165\n",
      "Epoch 7 / 40: Last test acc:0.5123\n",
      "Epoch: 7 LR: [0.001]\n",
      "Epoch 8 / 40: avg. loss of last 5 iterations 1.151511788368225\n",
      "Epoch 8 / 40: Last train acc:0.6624\n",
      "Epoch 8 / 40: Last test acc:0.5438\n",
      "Epoch: 8 LR: [0.001]\n",
      "Epoch 9 / 40: avg. loss of last 5 iterations 0.9794971585273743\n",
      "Epoch 9 / 40: Last train acc:0.68015\n",
      "Epoch 9 / 40: Last test acc:0.5362\n",
      "Epoch: 9 LR: [0.001]\n",
      "Epoch 10 / 40: avg. loss of last 5 iterations 0.884083890914917\n",
      "Epoch 10 / 40: Last train acc:0.7287\n",
      "Epoch 10 / 40: Last test acc:0.5514\n",
      "Epoch: 10 LR: [0.001]\n",
      "Epoch 11 / 40: avg. loss of last 5 iterations 0.9371485471725464\n",
      "Epoch 11 / 40: Last train acc:0.7417\n",
      "Epoch 11 / 40: Last test acc:0.5533\n",
      "Epoch: 11 LR: [0.001]\n",
      "Epoch 12 / 40: avg. loss of last 5 iterations 0.7716424584388732\n",
      "Epoch 12 / 40: Last train acc:0.82575\n",
      "Epoch 12 / 40: Last test acc:0.5702\n",
      "Epoch: 12 LR: [0.001]\n",
      "Epoch 13 / 40: avg. loss of last 5 iterations 0.617200493812561\n",
      "Epoch 13 / 40: Last train acc:0.84095\n",
      "Epoch 13 / 40: Last test acc:0.5646\n",
      "Epoch: 13 LR: [0.0003333333333333333]\n",
      "Epoch 14 / 40: avg. loss of last 5 iterations 0.33164559602737426\n",
      "Epoch 14 / 40: Last train acc:0.91665\n",
      "Epoch 14 / 40: Last test acc:0.5793\n",
      "Epoch: 14 LR: [0.0003333333333333333]\n",
      "Epoch 15 / 40: avg. loss of last 5 iterations 0.2649250328540802\n",
      "Epoch 15 / 40: Last train acc:0.94335\n",
      "Epoch 15 / 40: Last test acc:0.581\n",
      "Epoch: 15 LR: [0.0003333333333333333]\n",
      "Epoch 16 / 40: avg. loss of last 5 iterations 0.22374641448259353\n",
      "Epoch 16 / 40: Last train acc:0.9569\n",
      "Epoch 16 / 40: Last test acc:0.5791\n",
      "Epoch: 16 LR: [0.0003333333333333333]\n",
      "Epoch 17 / 40: avg. loss of last 5 iterations 0.19537097513675689\n",
      "Epoch 17 / 40: Last train acc:0.9744\n",
      "Epoch 17 / 40: Last test acc:0.579\n",
      "Epoch: 17 LR: [0.0003333333333333333]\n",
      "Epoch 18 / 40: avg. loss of last 5 iterations 0.13590787202119828\n",
      "Epoch 18 / 40: Last train acc:0.98375\n",
      "Epoch 18 / 40: Last test acc:0.5787\n",
      "Epoch: 18 LR: [0.0003333333333333333]\n",
      "Epoch 19 / 40: avg. loss of last 5 iterations 0.10450277924537658\n",
      "Epoch 19 / 40: Last train acc:0.98985\n",
      "Epoch 19 / 40: Last test acc:0.5791\n",
      "Epoch: 19 LR: [0.0003333333333333333]\n",
      "Epoch 20 / 40: avg. loss of last 5 iterations 0.1075274258852005\n",
      "Epoch 20 / 40: Last train acc:0.9937\n",
      "Epoch 20 / 40: Last test acc:0.5745\n",
      "Epoch: 20 LR: [0.0003333333333333333]\n",
      "Epoch 21 / 40: avg. loss of last 5 iterations 0.05451536253094673\n",
      "Epoch 21 / 40: Last train acc:0.9983\n",
      "Epoch 21 / 40: Last test acc:0.578\n",
      "Epoch: 21 LR: [0.0003333333333333333]\n",
      "Epoch 22 / 40: avg. loss of last 5 iterations 0.04254300221800804\n",
      "Epoch 22 / 40: Last train acc:0.99885\n",
      "Epoch 22 / 40: Last test acc:0.5797\n",
      "Epoch: 22 LR: [0.0003333333333333333]\n",
      "Epoch 23 / 40: avg. loss of last 5 iterations 0.03193826451897621\n",
      "Epoch 23 / 40: Last train acc:0.9996\n",
      "Epoch 23 / 40: Last test acc:0.5767\n",
      "Epoch: 23 LR: [0.0003333333333333333]\n",
      "Epoch 24 / 40: avg. loss of last 5 iterations 0.02372908964753151\n",
      "Epoch 24 / 40: Last train acc:0.9997\n",
      "Epoch 24 / 40: Last test acc:0.5778\n",
      "Epoch: 24 LR: [0.0003333333333333333]\n",
      "Epoch 25 / 40: avg. loss of last 5 iterations 0.023943078145384787\n",
      "Epoch 25 / 40: Last train acc:0.9998\n",
      "Epoch 25 / 40: Last test acc:0.5773\n",
      "Epoch: 25 LR: [0.0003333333333333333]\n",
      "Epoch 26 / 40: avg. loss of last 5 iterations 0.018114637397229672\n",
      "Epoch 26 / 40: Last train acc:1.0\n",
      "Epoch 26 / 40: Last test acc:0.5772\n",
      "Epoch: 26 LR: [0.0001111111111111111]\n",
      "Epoch 27 / 40: avg. loss of last 5 iterations 0.011991365440189838\n",
      "Epoch 27 / 40: Last train acc:1.0\n",
      "Epoch 27 / 40: Last test acc:0.578\n",
      "Epoch: 27 LR: [0.0001111111111111111]\n",
      "Epoch 28 / 40: avg. loss of last 5 iterations 0.01159639861434698\n",
      "Epoch 28 / 40: Last train acc:1.0\n",
      "Epoch 28 / 40: Last test acc:0.5777\n",
      "Epoch: 28 LR: [0.0001111111111111111]\n",
      "Epoch 29 / 40: avg. loss of last 5 iterations 0.016239523701369763\n",
      "Epoch 29 / 40: Last train acc:1.0\n",
      "Epoch 29 / 40: Last test acc:0.5775\n",
      "Epoch: 29 LR: [0.0001111111111111111]\n",
      "Epoch 30 / 40: avg. loss of last 5 iterations 0.01087412890046835\n",
      "Epoch 30 / 40: Last train acc:1.0\n",
      "Epoch 30 / 40: Last test acc:0.578\n",
      "Epoch: 30 LR: [0.0001111111111111111]\n",
      "Epoch 31 / 40: avg. loss of last 5 iterations 0.010993512906134128\n",
      "Epoch 31 / 40: Last train acc:1.0\n",
      "Epoch 31 / 40: Last test acc:0.5771\n",
      "Epoch: 31 LR: [0.0001111111111111111]\n",
      "Epoch 32 / 40: avg. loss of last 5 iterations 0.010375968925654889\n",
      "Epoch 32 / 40: Last train acc:1.0\n",
      "Epoch 32 / 40: Last test acc:0.5774\n",
      "Epoch: 32 LR: [0.0001111111111111111]\n",
      "Epoch 33 / 40: avg. loss of last 5 iterations 0.010259741265326739\n",
      "Epoch 33 / 40: Last train acc:1.0\n",
      "Epoch 33 / 40: Last test acc:0.5778\n",
      "Epoch: 33 LR: [0.0001111111111111111]\n",
      "Epoch 34 / 40: avg. loss of last 5 iterations 0.008895833510905504\n",
      "Epoch 34 / 40: Last train acc:1.0\n",
      "Epoch 34 / 40: Last test acc:0.5767\n",
      "Epoch: 34 LR: [0.0001111111111111111]\n",
      "Epoch 35 / 40: avg. loss of last 5 iterations 0.009220062382519245\n",
      "Epoch 35 / 40: Last train acc:1.0\n",
      "Epoch 35 / 40: Last test acc:0.5771\n",
      "Epoch: 35 LR: [0.0001111111111111111]\n",
      "Epoch 36 / 40: avg. loss of last 5 iterations 0.00788457915186882\n",
      "Epoch 36 / 40: Last train acc:1.0\n",
      "Epoch 36 / 40: Last test acc:0.577\n",
      "Epoch: 36 LR: [0.0001111111111111111]\n",
      "Epoch 37 / 40: avg. loss of last 5 iterations 0.008642307668924331\n",
      "Epoch 37 / 40: Last train acc:1.0\n",
      "Epoch 37 / 40: Last test acc:0.5769\n",
      "Epoch: 37 LR: [0.0001111111111111111]\n",
      "Epoch 38 / 40: avg. loss of last 5 iterations 0.008994876593351363\n",
      "Epoch 38 / 40: Last train acc:1.0\n",
      "Epoch 38 / 40: Last test acc:0.5758\n",
      "Epoch: 38 LR: [0.0001111111111111111]\n",
      "Epoch 39 / 40: avg. loss of last 5 iterations 0.007816429436206817\n",
      "Epoch 39 / 40: Last train acc:1.0\n",
      "Epoch 39 / 40: Last test acc:0.5757\n",
      "Epoch: 39 LR: [3.703703703703703e-05]\n",
      "Epoch 40 / 40: avg. loss of last 5 iterations 0.008312130533158778\n",
      "Epoch 40 / 40: Last train acc:1.0\n",
      "Epoch 40 / 40: Last test acc:0.5772\n",
      "Epoch: 40 LR: [3.703703703703703e-05]\n"
     ]
    }
   ],
   "source": [
    "# Using upstream training's weights with real labels\n",
    "loss_upstream_real, accs_upstream_real_train, accs_upstream_real_test = setup_and_train_downstream(\n",
    "    epochs=40, lr=0.001, step_size=(int)(40/3), num_examples_upstream=20000, \n",
    "    num_examples_downstream=20000, num_classes_upstream = 5, num_classes_downstream = 10,\n",
    "    is_downstream_random = False, batch_size = 256, num_outputs=10,\n",
    "    upstream_model_path = \"pretrained_model_dict_upstream_1_2.pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "orange = \"#fcba03\"\n",
    "light_blue = \"#03bafc\"\n",
    "\n",
    "plt.plot(accs_scratch_real_train, linestyle=\"-\", color=orange)\n",
    "plt.plot(accs_upstream_real_train, linestyle=\"-\", color=light_blue)\n",
    "plt.plot(accs_scratch_real_test, linestyle=\"--\", color=orange)\n",
    "plt.plot(accs_upstream_real_test, linestyle=\"--\", color=light_blue)\n",
    "plt.legend([\"from scratch (train)\", \"pretrained (train)\", \"from scratch (test)\", \"pretrained (test)\"], loc =\"best\")\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()ZZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda (GPU support) is available and enabled!\n",
      "Epoch 1 / 75: avg. loss of last 5 iterations 1.6176288604736329\n",
      "Epoch 1 / 75: Last train acc:0.2188\n",
      "Epoch: 1 LR: [0.001]\n",
      "Epoch 2 / 75: avg. loss of last 5 iterations 1.615944743156433\n",
      "Epoch 2 / 75: Last train acc:0.23425\n",
      "Epoch: 2 LR: [0.001]\n",
      "Epoch 3 / 75: avg. loss of last 5 iterations 1.6133175373077393\n",
      "Epoch 3 / 75: Last train acc:0.23545\n",
      "Epoch: 3 LR: [0.001]\n",
      "Epoch 4 / 75: avg. loss of last 5 iterations 1.6103538990020752\n",
      "Epoch 4 / 75: Last train acc:0.2226\n",
      "Epoch: 4 LR: [0.001]\n",
      "Epoch 5 / 75: avg. loss of last 5 iterations 1.610700750350952\n",
      "Epoch 5 / 75: Last train acc:0.23595\n",
      "Epoch: 5 LR: [0.001]\n",
      "Epoch 6 / 75: avg. loss of last 5 iterations 1.5970714807510376\n",
      "Epoch 6 / 75: Last train acc:0.27575\n",
      "Epoch: 6 LR: [0.001]\n",
      "Epoch 7 / 75: avg. loss of last 5 iterations 1.606911063194275\n",
      "Epoch 7 / 75: Last train acc:0.24925\n",
      "Epoch: 7 LR: [0.001]\n",
      "Epoch 8 / 75: avg. loss of last 5 iterations 1.6066181421279908\n",
      "Epoch 8 / 75: Last train acc:0.23315\n",
      "Epoch: 8 LR: [0.001]\n",
      "Epoch 9 / 75: avg. loss of last 5 iterations 1.5879597425460816\n",
      "Epoch 9 / 75: Last train acc:0.27525\n",
      "Epoch: 9 LR: [0.001]\n",
      "Epoch 10 / 75: avg. loss of last 5 iterations 1.5846590757369996\n",
      "Epoch 10 / 75: Last train acc:0.2378\n",
      "Epoch: 10 LR: [0.001]\n",
      "Epoch 11 / 75: avg. loss of last 5 iterations 1.588914394378662\n",
      "Epoch 11 / 75: Last train acc:0.2455\n",
      "Epoch: 11 LR: [0.001]\n",
      "Epoch 12 / 75: avg. loss of last 5 iterations 1.5786575078964233\n",
      "Epoch 12 / 75: Last train acc:0.26625\n",
      "Epoch: 12 LR: [0.001]\n",
      "Epoch 13 / 75: avg. loss of last 5 iterations 1.5830726623535156\n",
      "Epoch 13 / 75: Last train acc:0.2742\n",
      "Epoch: 13 LR: [0.001]\n",
      "Epoch 14 / 75: avg. loss of last 5 iterations 1.5724016666412353\n",
      "Epoch 14 / 75: Last train acc:0.2897\n",
      "Epoch: 14 LR: [0.001]\n",
      "Epoch 15 / 75: avg. loss of last 5 iterations 1.5770665884017945\n",
      "Epoch 15 / 75: Last train acc:0.28655\n",
      "Epoch: 15 LR: [0.001]\n",
      "Epoch 16 / 75: avg. loss of last 5 iterations 1.5523901462554932\n",
      "Epoch 16 / 75: Last train acc:0.3475\n",
      "Epoch: 16 LR: [0.001]\n",
      "Epoch 17 / 75: avg. loss of last 5 iterations 1.5483497619628905\n",
      "Epoch 17 / 75: Last train acc:0.34415\n",
      "Epoch: 17 LR: [0.001]\n",
      "Epoch 18 / 75: avg. loss of last 5 iterations 1.5418817758560182\n",
      "Epoch 18 / 75: Last train acc:0.3064\n",
      "Epoch: 18 LR: [0.001]\n",
      "Epoch 19 / 75: avg. loss of last 5 iterations 1.5268389940261842\n",
      "Epoch 19 / 75: Last train acc:0.3038\n",
      "Epoch: 19 LR: [0.001]\n",
      "Epoch 20 / 75: avg. loss of last 5 iterations 1.5128090143203736\n",
      "Epoch 20 / 75: Last train acc:0.3585\n",
      "Epoch: 20 LR: [0.001]\n",
      "Epoch 21 / 75: avg. loss of last 5 iterations 1.515253520011902\n",
      "Epoch 21 / 75: Last train acc:0.3621\n",
      "Epoch: 21 LR: [0.001]\n",
      "Epoch 22 / 75: avg. loss of last 5 iterations 1.561459517478943\n",
      "Epoch 22 / 75: Last train acc:0.34885\n",
      "Epoch: 22 LR: [0.001]\n",
      "Epoch 23 / 75: avg. loss of last 5 iterations 1.489149308204651\n",
      "Epoch 23 / 75: Last train acc:0.2838\n",
      "Epoch: 23 LR: [0.001]\n",
      "Epoch 24 / 75: avg. loss of last 5 iterations 1.4674085617065429\n",
      "Epoch 24 / 75: Last train acc:0.43225\n",
      "Epoch: 24 LR: [0.001]\n",
      "Epoch 25 / 75: avg. loss of last 5 iterations 1.4366816520690917\n",
      "Epoch 25 / 75: Last train acc:0.46055\n",
      "Epoch: 25 LR: [0.0003333333333333333]\n",
      "Epoch 26 / 75: avg. loss of last 5 iterations 1.2121783018112182\n",
      "Epoch 26 / 75: Last train acc:0.57755\n",
      "Epoch: 26 LR: [0.0003333333333333333]\n",
      "Epoch 27 / 75: avg. loss of last 5 iterations 1.2446080684661864\n",
      "Epoch 27 / 75: Last train acc:0.5834\n",
      "Epoch: 27 LR: [0.0003333333333333333]\n",
      "Epoch 28 / 75: avg. loss of last 5 iterations 1.203658103942871\n",
      "Epoch 28 / 75: Last train acc:0.6148\n",
      "Epoch: 28 LR: [0.0003333333333333333]\n",
      "Epoch 29 / 75: avg. loss of last 5 iterations 1.1442749977111817\n",
      "Epoch 29 / 75: Last train acc:0.6367\n",
      "Epoch: 29 LR: [0.0003333333333333333]\n",
      "Epoch 30 / 75: avg. loss of last 5 iterations 1.1388172149658202\n",
      "Epoch 30 / 75: Last train acc:0.66815\n",
      "Epoch: 30 LR: [0.0003333333333333333]\n",
      "Epoch 31 / 75: avg. loss of last 5 iterations 1.013868474960327\n",
      "Epoch 31 / 75: Last train acc:0.6691\n",
      "Epoch: 31 LR: [0.0003333333333333333]\n",
      "Epoch 32 / 75: avg. loss of last 5 iterations 0.9535986065864563\n",
      "Epoch 32 / 75: Last train acc:0.70395\n",
      "Epoch: 32 LR: [0.0003333333333333333]\n",
      "Epoch 33 / 75: avg. loss of last 5 iterations 0.9526089191436767\n",
      "Epoch 33 / 75: Last train acc:0.69465\n",
      "Epoch: 33 LR: [0.0003333333333333333]\n",
      "Epoch 34 / 75: avg. loss of last 5 iterations 0.8865596771240234\n",
      "Epoch 34 / 75: Last train acc:0.7219\n",
      "Epoch: 34 LR: [0.0003333333333333333]\n",
      "Epoch 35 / 75: avg. loss of last 5 iterations 0.8927282810211181\n",
      "Epoch 35 / 75: Last train acc:0.6434\n",
      "Epoch: 35 LR: [0.0003333333333333333]\n",
      "Epoch 36 / 75: avg. loss of last 5 iterations 0.7611173272132874\n",
      "Epoch 36 / 75: Last train acc:0.8071\n",
      "Epoch: 36 LR: [0.0003333333333333333]\n",
      "Epoch 37 / 75: avg. loss of last 5 iterations 0.71439688205719\n",
      "Epoch 37 / 75: Last train acc:0.82085\n",
      "Epoch: 37 LR: [0.0003333333333333333]\n",
      "Epoch 38 / 75: avg. loss of last 5 iterations 0.6469405055046081\n",
      "Epoch 38 / 75: Last train acc:0.8316\n",
      "Epoch: 38 LR: [0.0003333333333333333]\n",
      "Epoch 39 / 75: avg. loss of last 5 iterations 0.576626205444336\n",
      "Epoch 39 / 75: Last train acc:0.8582\n",
      "Epoch: 39 LR: [0.0003333333333333333]\n",
      "Epoch 40 / 75: avg. loss of last 5 iterations 0.47078346610069277\n",
      "Epoch 40 / 75: Last train acc:0.8628\n",
      "Epoch: 40 LR: [0.0003333333333333333]\n",
      "Epoch 41 / 75: avg. loss of last 5 iterations 0.4342993974685669\n",
      "Epoch 41 / 75: Last train acc:0.895\n",
      "Epoch: 41 LR: [0.0003333333333333333]\n",
      "Epoch 42 / 75: avg. loss of last 5 iterations 0.4395918071269989\n",
      "Epoch 42 / 75: Last train acc:0.8885\n",
      "Epoch: 42 LR: [0.0003333333333333333]\n",
      "Epoch 43 / 75: avg. loss of last 5 iterations 0.3421332061290741\n",
      "Epoch 43 / 75: Last train acc:0.9526\n",
      "Epoch: 43 LR: [0.0003333333333333333]\n",
      "Epoch 44 / 75: avg. loss of last 5 iterations 0.3986288607120514\n",
      "Epoch 44 / 75: Last train acc:0.93495\n",
      "Epoch: 44 LR: [0.0003333333333333333]\n",
      "Epoch 45 / 75: avg. loss of last 5 iterations 0.20248891413211823\n",
      "Epoch 45 / 75: Last train acc:0.969\n",
      "Epoch: 45 LR: [0.0003333333333333333]\n",
      "Epoch 46 / 75: avg. loss of last 5 iterations 0.17948776483535767\n",
      "Epoch 46 / 75: Last train acc:0.97565\n",
      "Epoch: 46 LR: [0.0003333333333333333]\n",
      "Epoch 47 / 75: avg. loss of last 5 iterations 0.395428740978241\n",
      "Epoch 47 / 75: Last train acc:0.7667\n",
      "Epoch: 47 LR: [0.0003333333333333333]\n",
      "Epoch 48 / 75: avg. loss of last 5 iterations 0.14148204028606415\n",
      "Epoch 48 / 75: Last train acc:0.99165\n",
      "Epoch: 48 LR: [0.0003333333333333333]\n",
      "Epoch 49 / 75: avg. loss of last 5 iterations 0.10402075946331024\n",
      "Epoch 49 / 75: Last train acc:0.99335\n",
      "Epoch: 49 LR: [0.0003333333333333333]\n",
      "Epoch 50 / 75: avg. loss of last 5 iterations 0.131295907497406\n",
      "Epoch 50 / 75: Last train acc:0.99175\n",
      "Epoch: 50 LR: [0.0001111111111111111]\n",
      "Epoch 51 / 75: avg. loss of last 5 iterations 0.038154485449194905\n",
      "Epoch 51 / 75: Last train acc:0.99945\n",
      "Epoch: 51 LR: [0.0001111111111111111]\n",
      "Epoch 52 / 75: avg. loss of last 5 iterations 0.024486425891518594\n",
      "Epoch 52 / 75: Last train acc:0.99985\n",
      "Epoch: 52 LR: [0.0001111111111111111]\n",
      "Epoch 53 / 75: avg. loss of last 5 iterations 0.02180206272751093\n",
      "Epoch 53 / 75: Last train acc:0.9999\n",
      "Epoch: 53 LR: [0.0001111111111111111]\n",
      "Epoch 54 / 75: avg. loss of last 5 iterations 0.017613958567380905\n",
      "Epoch 54 / 75: Last train acc:0.99995\n",
      "Epoch: 54 LR: [0.0001111111111111111]\n",
      "Epoch 55 / 75: avg. loss of last 5 iterations 0.018707383051514627\n",
      "Epoch 55 / 75: Last train acc:0.99995\n",
      "Epoch: 55 LR: [0.0001111111111111111]\n",
      "Epoch 56 / 75: avg. loss of last 5 iterations 0.014772442169487476\n",
      "Epoch 56 / 75: Last train acc:0.99995\n",
      "Epoch: 56 LR: [0.0001111111111111111]\n",
      "Epoch 57 / 75: avg. loss of last 5 iterations 0.014558631740510464\n",
      "Epoch 57 / 75: Last train acc:0.99995\n",
      "Epoch: 57 LR: [0.0001111111111111111]\n",
      "Epoch 58 / 75: avg. loss of last 5 iterations 0.014657605066895485\n",
      "Epoch 58 / 75: Last train acc:0.99995\n",
      "Epoch: 58 LR: [0.0001111111111111111]\n",
      "Epoch 59 / 75: avg. loss of last 5 iterations 0.012516012415289879\n",
      "Epoch 59 / 75: Last train acc:0.99995\n",
      "Epoch: 59 LR: [0.0001111111111111111]\n",
      "Epoch 60 / 75: avg. loss of last 5 iterations 0.015212207473814488\n",
      "Epoch 60 / 75: Last train acc:0.99995\n",
      "Epoch: 60 LR: [0.0001111111111111111]\n",
      "Epoch 61 / 75: avg. loss of last 5 iterations 0.011564572155475617\n",
      "Epoch 61 / 75: Last train acc:0.99995\n",
      "Epoch: 61 LR: [0.0001111111111111111]\n",
      "Epoch 62 / 75: avg. loss of last 5 iterations 0.009441191330552101\n",
      "Epoch 62 / 75: Last train acc:1.0\n",
      "Epoch: 62 LR: [0.0001111111111111111]\n",
      "Epoch 63 / 75: avg. loss of last 5 iterations 0.010987600497901439\n",
      "Epoch 63 / 75: Last train acc:1.0\n",
      "Epoch: 63 LR: [0.0001111111111111111]\n",
      "Epoch 64 / 75: avg. loss of last 5 iterations 0.009895570389926434\n",
      "Epoch 64 / 75: Last train acc:1.0\n",
      "Epoch: 64 LR: [0.0001111111111111111]\n",
      "Epoch 65 / 75: avg. loss of last 5 iterations 0.008080086018890143\n",
      "Epoch 65 / 75: Last train acc:1.0\n",
      "Epoch: 65 LR: [0.0001111111111111111]\n",
      "Epoch 66 / 75: avg. loss of last 5 iterations 0.016320507787168027\n",
      "Epoch 66 / 75: Last train acc:1.0\n",
      "Epoch: 66 LR: [0.0001111111111111111]\n",
      "Epoch 67 / 75: avg. loss of last 5 iterations 0.008344544377177954\n",
      "Epoch 67 / 75: Last train acc:1.0\n",
      "Epoch: 67 LR: [0.0001111111111111111]\n",
      "Epoch 68 / 75: avg. loss of last 5 iterations 0.007817070931196213\n",
      "Epoch 68 / 75: Last train acc:1.0\n",
      "Epoch: 68 LR: [0.0001111111111111111]\n",
      "Epoch 69 / 75: avg. loss of last 5 iterations 0.008156060054898263\n",
      "Epoch 69 / 75: Last train acc:1.0\n",
      "Epoch: 69 LR: [0.0001111111111111111]\n",
      "Epoch 70 / 75: avg. loss of last 5 iterations 0.0071620275266468525\n",
      "Epoch 70 / 75: Last train acc:1.0\n",
      "Epoch: 70 LR: [0.0001111111111111111]\n",
      "Epoch 71 / 75: avg. loss of last 5 iterations 0.006673320382833481\n",
      "Epoch 71 / 75: Last train acc:1.0\n",
      "Epoch: 71 LR: [0.0001111111111111111]\n",
      "Epoch 72 / 75: avg. loss of last 5 iterations 0.0066617075353860855\n",
      "Epoch 72 / 75: Last train acc:1.0\n",
      "Epoch: 72 LR: [0.0001111111111111111]\n",
      "Epoch 73 / 75: avg. loss of last 5 iterations 0.007407072093337774\n",
      "Epoch 73 / 75: Last train acc:1.0\n",
      "Epoch: 73 LR: [0.0001111111111111111]\n",
      "Epoch 74 / 75: avg. loss of last 5 iterations 0.006451909337192774\n",
      "Epoch 74 / 75: Last train acc:1.0\n",
      "Epoch: 74 LR: [0.0001111111111111111]\n",
      "Epoch 75 / 75: avg. loss of last 5 iterations 0.006188919395208358\n",
      "Epoch 75 / 75: Last train acc:1.0\n",
      "Epoch: 75 LR: [3.703703703703703e-05]\n"
     ]
    }
   ],
   "source": [
    "# From scratch with random labels\n",
    "loss_upstream_real, accs_scratch_random_train = setup_and_train_downstream(\n",
    "    epochs=75, lr=0.001, step_size=(int)(75/3), num_examples_upstream=20000, \n",
    "    num_examples_downstream=20000, num_classes_upstream = 5, num_classes_downstream = 5,\n",
    "    is_downstream_random = True, batch_size = 256, num_outputs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda (GPU support) is available and enabled!\n",
      "Epoch 1 / 75: avg. loss of last 5 iterations 1.6134973764419556\n",
      "Epoch 1 / 75: Last train acc:0.2014\n",
      "Epoch: 1 LR: [0.001]\n",
      "Epoch 2 / 75: avg. loss of last 5 iterations 1.6034074544906616\n",
      "Epoch 2 / 75: Last train acc:0.2278\n",
      "Epoch: 2 LR: [0.001]\n",
      "Epoch 3 / 75: avg. loss of last 5 iterations 1.6118242263793945\n",
      "Epoch 3 / 75: Last train acc:0.2101\n",
      "Epoch: 3 LR: [0.001]\n",
      "Epoch 4 / 75: avg. loss of last 5 iterations 1.6010621547698975\n",
      "Epoch 4 / 75: Last train acc:0.2071\n",
      "Epoch: 4 LR: [0.001]\n",
      "Epoch 5 / 75: avg. loss of last 5 iterations 1.6046802282333374\n",
      "Epoch 5 / 75: Last train acc:0.223\n",
      "Epoch: 5 LR: [0.001]\n",
      "Epoch 6 / 75: avg. loss of last 5 iterations 1.6100910663604737\n",
      "Epoch 6 / 75: Last train acc:0.2347\n",
      "Epoch: 6 LR: [0.001]\n",
      "Epoch 7 / 75: avg. loss of last 5 iterations 1.597103190422058\n",
      "Epoch 7 / 75: Last train acc:0.27335\n",
      "Epoch: 7 LR: [0.001]\n",
      "Epoch 8 / 75: avg. loss of last 5 iterations 1.6006527662277221\n",
      "Epoch 8 / 75: Last train acc:0.242\n",
      "Epoch: 8 LR: [0.001]\n",
      "Epoch 9 / 75: avg. loss of last 5 iterations 1.5936862230300903\n",
      "Epoch 9 / 75: Last train acc:0.23045\n",
      "Epoch: 9 LR: [0.001]\n",
      "Epoch 10 / 75: avg. loss of last 5 iterations 1.6023208856582642\n",
      "Epoch 10 / 75: Last train acc:0.244\n",
      "Epoch: 10 LR: [0.001]\n",
      "Epoch 11 / 75: avg. loss of last 5 iterations 1.5934565782546997\n",
      "Epoch 11 / 75: Last train acc:0.26835\n",
      "Epoch: 11 LR: [0.001]\n",
      "Epoch 12 / 75: avg. loss of last 5 iterations 1.587694764137268\n",
      "Epoch 12 / 75: Last train acc:0.2955\n",
      "Epoch: 12 LR: [0.001]\n",
      "Epoch 13 / 75: avg. loss of last 5 iterations 1.6053711414337157\n",
      "Epoch 13 / 75: Last train acc:0.25435\n",
      "Epoch: 13 LR: [0.001]\n",
      "Epoch 14 / 75: avg. loss of last 5 iterations 1.5932225704193115\n",
      "Epoch 14 / 75: Last train acc:0.2612\n",
      "Epoch: 14 LR: [0.001]\n",
      "Epoch 15 / 75: avg. loss of last 5 iterations 1.5743821859359741\n",
      "Epoch 15 / 75: Last train acc:0.29345\n",
      "Epoch: 15 LR: [0.001]\n",
      "Epoch 16 / 75: avg. loss of last 5 iterations 1.578311014175415\n",
      "Epoch 16 / 75: Last train acc:0.3005\n",
      "Epoch: 16 LR: [0.001]\n",
      "Epoch 17 / 75: avg. loss of last 5 iterations 1.5777766942977904\n",
      "Epoch 17 / 75: Last train acc:0.2975\n",
      "Epoch: 17 LR: [0.001]\n",
      "Epoch 18 / 75: avg. loss of last 5 iterations 1.5832091569900513\n",
      "Epoch 18 / 75: Last train acc:0.28115\n",
      "Epoch: 18 LR: [0.001]\n",
      "Epoch 19 / 75: avg. loss of last 5 iterations 1.5504009485244752\n",
      "Epoch 19 / 75: Last train acc:0.34945\n",
      "Epoch: 19 LR: [0.001]\n",
      "Epoch 20 / 75: avg. loss of last 5 iterations 1.5617971181869508\n",
      "Epoch 20 / 75: Last train acc:0.3136\n",
      "Epoch: 20 LR: [0.001]\n",
      "Epoch 21 / 75: avg. loss of last 5 iterations 1.5453816175460815\n",
      "Epoch 21 / 75: Last train acc:0.2738\n",
      "Epoch: 21 LR: [0.001]\n",
      "Epoch 22 / 75: avg. loss of last 5 iterations 1.527676033973694\n",
      "Epoch 22 / 75: Last train acc:0.34985\n",
      "Epoch: 22 LR: [0.001]\n",
      "Epoch 23 / 75: avg. loss of last 5 iterations 1.5590716361999513\n",
      "Epoch 23 / 75: Last train acc:0.3444\n",
      "Epoch: 23 LR: [0.001]\n",
      "Epoch 24 / 75: avg. loss of last 5 iterations 1.5057550907135009\n",
      "Epoch 24 / 75: Last train acc:0.4317\n",
      "Epoch: 24 LR: [0.001]\n",
      "Epoch 25 / 75: avg. loss of last 5 iterations 1.473897886276245\n",
      "Epoch 25 / 75: Last train acc:0.4314\n",
      "Epoch: 25 LR: [0.0003333333333333333]\n",
      "Epoch 26 / 75: avg. loss of last 5 iterations 1.35228750705719\n",
      "Epoch 26 / 75: Last train acc:0.48335\n",
      "Epoch: 26 LR: [0.0003333333333333333]\n",
      "Epoch 27 / 75: avg. loss of last 5 iterations 1.3291882753372193\n",
      "Epoch 27 / 75: Last train acc:0.48225\n",
      "Epoch: 27 LR: [0.0003333333333333333]\n",
      "Epoch 28 / 75: avg. loss of last 5 iterations 1.3379786729812622\n",
      "Epoch 28 / 75: Last train acc:0.521\n",
      "Epoch: 28 LR: [0.0003333333333333333]\n",
      "Epoch 29 / 75: avg. loss of last 5 iterations 1.3224127531051635\n",
      "Epoch 29 / 75: Last train acc:0.47245\n",
      "Epoch: 29 LR: [0.0003333333333333333]\n",
      "Epoch 30 / 75: avg. loss of last 5 iterations 1.2334921836853028\n",
      "Epoch 30 / 75: Last train acc:0.56435\n",
      "Epoch: 30 LR: [0.0003333333333333333]\n",
      "Epoch 31 / 75: avg. loss of last 5 iterations 1.2346478700637817\n",
      "Epoch 31 / 75: Last train acc:0.53525\n",
      "Epoch: 31 LR: [0.0003333333333333333]\n",
      "Epoch 32 / 75: avg. loss of last 5 iterations 1.1917169332504272\n",
      "Epoch 32 / 75: Last train acc:0.5971\n",
      "Epoch: 32 LR: [0.0003333333333333333]\n",
      "Epoch 33 / 75: avg. loss of last 5 iterations 1.2095375061035156\n",
      "Epoch 33 / 75: Last train acc:0.58345\n",
      "Epoch: 33 LR: [0.0003333333333333333]\n",
      "Epoch 34 / 75: avg. loss of last 5 iterations 1.0953126907348634\n",
      "Epoch 34 / 75: Last train acc:0.65795\n",
      "Epoch: 34 LR: [0.0003333333333333333]\n",
      "Epoch 35 / 75: avg. loss of last 5 iterations 1.1698443174362183\n",
      "Epoch 35 / 75: Last train acc:0.57925\n",
      "Epoch: 35 LR: [0.0003333333333333333]\n",
      "Epoch 36 / 75: avg. loss of last 5 iterations 1.1262265920639039\n",
      "Epoch 36 / 75: Last train acc:0.6696\n",
      "Epoch: 36 LR: [0.0003333333333333333]\n",
      "Epoch 37 / 75: avg. loss of last 5 iterations 1.0169651985168457\n",
      "Epoch 37 / 75: Last train acc:0.69435\n",
      "Epoch: 37 LR: [0.0003333333333333333]\n",
      "Epoch 38 / 75: avg. loss of last 5 iterations 0.933229672908783\n",
      "Epoch 38 / 75: Last train acc:0.74\n",
      "Epoch: 38 LR: [0.0003333333333333333]\n",
      "Epoch 39 / 75: avg. loss of last 5 iterations 0.8749019145965576\n",
      "Epoch 39 / 75: Last train acc:0.73075\n",
      "Epoch: 39 LR: [0.0003333333333333333]\n",
      "Epoch 40 / 75: avg. loss of last 5 iterations 0.791568911075592\n",
      "Epoch 40 / 75: Last train acc:0.79315\n",
      "Epoch: 40 LR: [0.0003333333333333333]\n",
      "Epoch 41 / 75: avg. loss of last 5 iterations 0.7557801246643067\n",
      "Epoch 41 / 75: Last train acc:0.73575\n",
      "Epoch: 41 LR: [0.0003333333333333333]\n",
      "Epoch 42 / 75: avg. loss of last 5 iterations 0.6492997169494629\n",
      "Epoch 42 / 75: Last train acc:0.8441\n",
      "Epoch: 42 LR: [0.0003333333333333333]\n",
      "Epoch 43 / 75: avg. loss of last 5 iterations 0.6316975474357605\n",
      "Epoch 43 / 75: Last train acc:0.8652\n",
      "Epoch: 43 LR: [0.0003333333333333333]\n",
      "Epoch 44 / 75: avg. loss of last 5 iterations 0.8321298480033874\n",
      "Epoch 44 / 75: Last train acc:0.75795\n",
      "Epoch: 44 LR: [0.0003333333333333333]\n",
      "Epoch 45 / 75: avg. loss of last 5 iterations 0.4654591679573059\n",
      "Epoch 45 / 75: Last train acc:0.8868\n",
      "Epoch: 45 LR: [0.0003333333333333333]\n",
      "Epoch 46 / 75: avg. loss of last 5 iterations 0.4128780424594879\n",
      "Epoch 46 / 75: Last train acc:0.90765\n",
      "Epoch: 46 LR: [0.0003333333333333333]\n",
      "Epoch 47 / 75: avg. loss of last 5 iterations 0.4031446099281311\n",
      "Epoch 47 / 75: Last train acc:0.943\n",
      "Epoch: 47 LR: [0.0003333333333333333]\n",
      "Epoch 48 / 75: avg. loss of last 5 iterations 0.31139086186885834\n",
      "Epoch 48 / 75: Last train acc:0.93105\n",
      "Epoch: 48 LR: [0.0003333333333333333]\n",
      "Epoch 49 / 75: avg. loss of last 5 iterations 0.23823314905166626\n",
      "Epoch 49 / 75: Last train acc:0.95915\n",
      "Epoch: 49 LR: [0.0003333333333333333]\n",
      "Epoch 50 / 75: avg. loss of last 5 iterations 0.34436572194099424\n",
      "Epoch 50 / 75: Last train acc:0.8682\n",
      "Epoch: 50 LR: [0.0001111111111111111]\n",
      "Epoch 51 / 75: avg. loss of last 5 iterations 0.0894136220216751\n",
      "Epoch 51 / 75: Last train acc:0.9963\n",
      "Epoch: 51 LR: [0.0001111111111111111]\n",
      "Epoch 52 / 75: avg. loss of last 5 iterations 0.0571529284119606\n",
      "Epoch 52 / 75: Last train acc:0.998\n",
      "Epoch: 52 LR: [0.0001111111111111111]\n",
      "Epoch 53 / 75: avg. loss of last 5 iterations 0.05013945177197456\n",
      "Epoch 53 / 75: Last train acc:0.99865\n",
      "Epoch: 53 LR: [0.0001111111111111111]\n",
      "Epoch 54 / 75: avg. loss of last 5 iterations 0.03758194632828236\n",
      "Epoch 54 / 75: Last train acc:0.999\n",
      "Epoch: 54 LR: [0.0001111111111111111]\n",
      "Epoch 55 / 75: avg. loss of last 5 iterations 0.034619349986314774\n",
      "Epoch 55 / 75: Last train acc:0.99945\n",
      "Epoch: 55 LR: [0.0001111111111111111]\n",
      "Epoch 56 / 75: avg. loss of last 5 iterations 0.03227519094944\n",
      "Epoch 56 / 75: Last train acc:0.9995\n",
      "Epoch: 56 LR: [0.0001111111111111111]\n",
      "Epoch 57 / 75: avg. loss of last 5 iterations 0.02920128181576729\n",
      "Epoch 57 / 75: Last train acc:0.99975\n",
      "Epoch: 57 LR: [0.0001111111111111111]\n",
      "Epoch 58 / 75: avg. loss of last 5 iterations 0.027419088780879973\n",
      "Epoch 58 / 75: Last train acc:0.99975\n",
      "Epoch: 58 LR: [0.0001111111111111111]\n",
      "Epoch 59 / 75: avg. loss of last 5 iterations 0.025859159976243974\n",
      "Epoch 59 / 75: Last train acc:0.9998\n",
      "Epoch: 59 LR: [0.0001111111111111111]\n",
      "Epoch 60 / 75: avg. loss of last 5 iterations 0.02358405590057373\n",
      "Epoch 60 / 75: Last train acc:0.99985\n",
      "Epoch: 60 LR: [0.0001111111111111111]\n",
      "Epoch 61 / 75: avg. loss of last 5 iterations 0.022200296446681023\n",
      "Epoch 61 / 75: Last train acc:0.99995\n",
      "Epoch: 61 LR: [0.0001111111111111111]\n",
      "Epoch 62 / 75: avg. loss of last 5 iterations 0.018047434650361537\n",
      "Epoch 62 / 75: Last train acc:0.99995\n",
      "Epoch: 62 LR: [0.0001111111111111111]\n",
      "Epoch 63 / 75: avg. loss of last 5 iterations 0.01606537736952305\n",
      "Epoch 63 / 75: Last train acc:0.99995\n",
      "Epoch: 63 LR: [0.0001111111111111111]\n",
      "Epoch 64 / 75: avg. loss of last 5 iterations 0.015016480721533299\n",
      "Epoch 64 / 75: Last train acc:0.99995\n",
      "Epoch: 64 LR: [0.0001111111111111111]\n",
      "Epoch 65 / 75: avg. loss of last 5 iterations 0.016383341141045095\n",
      "Epoch 65 / 75: Last train acc:0.99995\n",
      "Epoch: 65 LR: [0.0001111111111111111]\n",
      "Epoch 66 / 75: avg. loss of last 5 iterations 0.013744728825986385\n",
      "Epoch 66 / 75: Last train acc:1.0\n",
      "Epoch: 66 LR: [0.0001111111111111111]\n",
      "Epoch 67 / 75: avg. loss of last 5 iterations 0.01268884465098381\n",
      "Epoch 67 / 75: Last train acc:1.0\n",
      "Epoch: 67 LR: [0.0001111111111111111]\n",
      "Epoch 68 / 75: avg. loss of last 5 iterations 0.012356173805892467\n",
      "Epoch 68 / 75: Last train acc:1.0\n",
      "Epoch: 68 LR: [0.0001111111111111111]\n",
      "Epoch 69 / 75: avg. loss of last 5 iterations 0.013038583658635616\n",
      "Epoch 69 / 75: Last train acc:1.0\n",
      "Epoch: 69 LR: [0.0001111111111111111]\n",
      "Epoch 70 / 75: avg. loss of last 5 iterations 0.011886516958475113\n",
      "Epoch 70 / 75: Last train acc:1.0\n",
      "Epoch: 70 LR: [0.0001111111111111111]\n",
      "Epoch 71 / 75: avg. loss of last 5 iterations 0.009513831976801158\n",
      "Epoch 71 / 75: Last train acc:1.0\n",
      "Epoch: 71 LR: [0.0001111111111111111]\n",
      "Epoch 72 / 75: avg. loss of last 5 iterations 0.008876321092247962\n",
      "Epoch 72 / 75: Last train acc:1.0\n",
      "Epoch: 72 LR: [0.0001111111111111111]\n",
      "Epoch 73 / 75: avg. loss of last 5 iterations 0.00953872948884964\n",
      "Epoch 73 / 75: Last train acc:1.0\n",
      "Epoch: 73 LR: [0.0001111111111111111]\n",
      "Epoch 74 / 75: avg. loss of last 5 iterations 0.007793361134827137\n",
      "Epoch 74 / 75: Last train acc:1.0\n",
      "Epoch: 74 LR: [0.0001111111111111111]\n",
      "Epoch 75 / 75: avg. loss of last 5 iterations 0.008536493498831987\n",
      "Epoch 75 / 75: Last train acc:1.0\n",
      "Epoch: 75 LR: [3.703703703703703e-05]\n"
     ]
    }
   ],
   "source": [
    "# Using upstream training's weights with random labels\n",
    "loss_upstream_real, accs_upstream_random_train = setup_and_train_downstream(\n",
    "    epochs=75, lr=0.001, step_size=(int)(75/3), num_examples_upstream=20000,\n",
    "    num_examples_downstream=20000, num_classes_upstream = 5, num_classes_downstream = 5,\n",
    "    is_downstream_random = True, batch_size = 256, num_outputs=5,\n",
    "    upstream_model_path = \"pretrained_model_dict_upstream_1_2.pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "orange = \"#fcba03\"\n",
    "light_blue = \"#03bafc\"\n",
    "\n",
    "plt.plot(accs_scratch_random_train, linestyle=\"-\", color=orange)\n",
    "plt.plot(accs_upstream_random_train, linestyle=\"-\", color=light_blue)\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend([\"from scratch (train)\", \"pretrained (train)\"], loc =\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of figure1.ipynb",
   "provenance": [
    {
     "file_id": "1BNwYcPcE3j7kX8PsVSfBpXp1s6Dwmfne",
     "timestamp": 1626623850743
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
