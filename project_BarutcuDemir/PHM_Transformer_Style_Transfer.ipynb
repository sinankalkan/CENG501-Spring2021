{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PHM Transformer Style Transfer",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "321b68039d3446bcaf4ad4f74be492dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_609413c75891404dbcb13ecbb67ad69c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c53dfea99a844785b86c3b990bd1d58a",
              "IPY_MODEL_4ffed0daf0db4da7900412ee735ee457"
            ]
          }
        },
        "609413c75891404dbcb13ecbb67ad69c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c53dfea99a844785b86c3b990bd1d58a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_2a9af9b6fc9f42d5b3d67e2c05c6279f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.63MB of 0.63MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_41b0171d39474d88ac61797ec06b71e5"
          }
        },
        "4ffed0daf0db4da7900412ee735ee457": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9ff0cbfb452543a2a9e33a2a6c3cad7c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4179f9701091458a86efcfb70b792b0c"
          }
        },
        "2a9af9b6fc9f42d5b3d67e2c05c6279f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "41b0171d39474d88ac61797ec06b71e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9ff0cbfb452543a2a9e33a2a6c3cad7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4179f9701091458a86efcfb70b792b0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWbwaoVKx4hM"
      },
      "source": [
        "This is the Colab to reproduce our reproduction of the text style transfer task in [Beyond Fully Connected Layers with Quaternions: Parametrization of Hypercomplex Multiplications with 1/n Parameters](https://openreview.net/pdf?id=rcQdycl0zyk). We will translate modern English to Shakespearean English, using a PHM Transformer. You can find the detailed explanation in our [Github repo](https://github.com/sinankalkan/CENG501-Spring2021/tree/main/project_BarutcuDemir). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22lF85SNx3bv"
      },
      "source": [
        "# Install & Import Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TYOWv66tAul"
      },
      "source": [
        "You should use a GPU. Although it should be enabled by default, check if GPU is selected as the hardware accelerator by going to Runtime->Change Runtime Type. Make sure you get a T4 or P100. We have not tested the code on other GPU's (they have lower performance)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKz_Fz6CuBzh",
        "outputId": "f987fa72-6e07-4a9a-85a3-61b459e3da91"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jul 28 10:19:46 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P8    12W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDl78wqMtRzZ"
      },
      "source": [
        "Install the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9tL0RgKwiAF",
        "outputId": "77eb11e6-c942-41de-f730-eb4af7b5151e"
      },
      "source": [
        "!pip install torchinfo wandb"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.5.2-py3-none-any.whl (18 kB)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.11.0-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 36.8 MB/s \n",
            "\u001b[?25hCollecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 8.1 MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=0.4.0\n",
            "  Downloading sentry_sdk-1.3.1-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 53.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting urllib3>=1.26.5\n",
            "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 59.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.20-py3-none-any.whl (178 kB)\n",
            "\u001b[K     |████████████████████████████████| 178 kB 62.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Collecting requests<3,>=2.0.0\n",
            "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.2)\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=2e72f082b5701c936a18833933a15fc3f5c2f80d0db574aa62c751875170e39d\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=2ee18d02389fbfecfd098b79f4cb76b9edf5dc214935d27e5d8fb162e404162d\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: smmap, urllib3, gitdb, subprocess32, shortuuid, sentry-sdk, requests, pathtools, GitPython, docker-pycreds, configparser, wandb, torchinfo\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.20 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 requests-2.26.0 sentry-sdk-1.3.1 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 torchinfo-1.5.2 urllib3-1.26.6 wandb-0.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HXEy5V6tVY8"
      },
      "source": [
        "We use particular version of Torch because of a CUDA issue. Also some parts of the code such as the Vocabulary are radically different in Torch 1.9 so be sure to use this version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhhT37pBSgeQ",
        "outputId": "4438275a-f10e-475b-80da-64e429348300"
      },
      "source": [
        "!pip install torch==1.8.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.8.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.8.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (1982.2 MB)\n",
            "\u001b[K     |█████████████▌                  | 834.1 MB 1.4 MB/s eta 0:13:57tcmalloc: large alloc 1147494400 bytes == 0x557e44f10000 @  0x7f8219a4e615 0x557e0bd2c02c 0x557e0be0c17a 0x557e0bd2ee4d 0x557e0be20c0d 0x557e0bda30d8 0x557e0bd9dc35 0x557e0bd3073a 0x557e0bda2f40 0x557e0bd9dc35 0x557e0bd3073a 0x557e0bd9f93b 0x557e0be21a56 0x557e0bd9efb3 0x557e0be21a56 0x557e0bd9efb3 0x557e0be21a56 0x557e0bd9efb3 0x557e0bd30b99 0x557e0bd73e79 0x557e0bd2f7b2 0x557e0bda2e65 0x557e0bd9dc35 0x557e0bd3073a 0x557e0bd9f93b 0x557e0bd9dc35 0x557e0bd3073a 0x557e0bd9eb0e 0x557e0bd3065a 0x557e0bd9ed67 0x557e0bd9dc35\n",
            "\u001b[K     |█████████████████               | 1055.7 MB 1.5 MB/s eta 0:10:21tcmalloc: large alloc 1434370048 bytes == 0x557e89566000 @  0x7f8219a4e615 0x557e0bd2c02c 0x557e0be0c17a 0x557e0bd2ee4d 0x557e0be20c0d 0x557e0bda30d8 0x557e0bd9dc35 0x557e0bd3073a 0x557e0bda2f40 0x557e0bd9dc35 0x557e0bd3073a 0x557e0bd9f93b 0x557e0be21a56 0x557e0bd9efb3 0x557e0be21a56 0x557e0bd9efb3 0x557e0be21a56 0x557e0bd9efb3 0x557e0bd30b99 0x557e0bd73e79 0x557e0bd2f7b2 0x557e0bda2e65 0x557e0bd9dc35 0x557e0bd3073a 0x557e0bd9f93b 0x557e0bd9dc35 0x557e0bd3073a 0x557e0bd9eb0e 0x557e0bd3065a 0x557e0bd9ed67 0x557e0bd9dc35\n",
            "\u001b[K     |█████████████████████▋          | 1336.2 MB 1.4 MB/s eta 0:07:33tcmalloc: large alloc 1792966656 bytes == 0x557e0e398000 @  0x7f8219a4e615 0x557e0bd2c02c 0x557e0be0c17a 0x557e0bd2ee4d 0x557e0be20c0d 0x557e0bda30d8 0x557e0bd9dc35 0x557e0bd3073a 0x557e0bda2f40 0x557e0bd9dc35 0x557e0bd3073a 0x557e0bd9f93b 0x557e0be21a56 0x557e0bd9efb3 0x557e0be21a56 0x557e0bd9efb3 0x557e0be21a56 0x557e0bd9efb3 0x557e0bd30b99 0x557e0bd73e79 0x557e0bd2f7b2 0x557e0bda2e65 0x557e0bd9dc35 0x557e0bd3073a 0x557e0bd9f93b 0x557e0bd9dc35 0x557e0bd3073a 0x557e0bd9eb0e 0x557e0bd3065a 0x557e0bd9ed67 0x557e0bd9dc35\n",
            "\u001b[K     |███████████████████████████▎    | 1691.1 MB 1.2 MB/s eta 0:04:04tcmalloc: large alloc 2241208320 bytes == 0x557e79180000 @  0x7f8219a4e615 0x557e0bd2c02c 0x557e0be0c17a 0x557e0bd2ee4d 0x557e0be20c0d 0x557e0bda30d8 0x557e0bd9dc35 0x557e0bd3073a 0x557e0bda2f40 0x557e0bd9dc35 0x557e0bd3073a 0x557e0bd9f93b 0x557e0be21a56 0x557e0bd9efb3 0x557e0be21a56 0x557e0bd9efb3 0x557e0be21a56 0x557e0bd9efb3 0x557e0bd30b99 0x557e0bd73e79 0x557e0bd2f7b2 0x557e0bda2e65 0x557e0bd9dc35 0x557e0bd3073a 0x557e0bd9f93b 0x557e0bd9dc35 0x557e0bd3073a 0x557e0bd9eb0e 0x557e0bd3065a 0x557e0bd9ed67 0x557e0bd9dc35\n",
            "\u001b[K     |████████████████████████████████| 1982.2 MB 1.2 MB/s eta 0:00:01tcmalloc: large alloc 1982251008 bytes == 0x557efeae2000 @  0x7f8219a4d1e7 0x557e0bd61ae7 0x557e0bd2c02c 0x557e0be0c17a 0x557e0bd2ee4d 0x557e0be20c0d 0x557e0bda30d8 0x557e0bd9dc35 0x557e0bd3073a 0x557e0bd9ed67 0x557e0bd9dc35 0x557e0bd3073a 0x557e0bd9ed67 0x557e0bd9dc35 0x557e0bd3073a 0x557e0bd9ed67 0x557e0bd9dc35 0x557e0bd3073a 0x557e0bd9ed67 0x557e0bd9dc35 0x557e0bd3073a 0x557e0bd9ed67 0x557e0bd3065a 0x557e0bd9ed67 0x557e0bd9dc35 0x557e0bd3073a 0x557e0bd9f93b 0x557e0bd9dc35 0x557e0bd3073a 0x557e0bd9f93b 0x557e0bd9dc35\n",
            "tcmalloc: large alloc 2477817856 bytes == 0x557f74d4e000 @  0x7f8219a4e615 0x557e0bd2c02c 0x557e0be0c17a 0x557e0bd2ee4d 0x557e0be20c0d 0x557e0bda30d8 0x557e0bd9dc35 0x557e0bd3073a 0x557e0bd9ed67 0x557e0bd9dc35 0x557e0bd3073a 0x557e0bd9ed67 0x557e0bd9dc35 0x557e0bd3073a 0x557e0bd9ed67 0x557e0bd9dc35 0x557e0bd3073a 0x557e0bd9ed67 0x557e0bd9dc35 0x557e0bd3073a 0x557e0bd9ed67 0x557e0bd3065a 0x557e0bd9ed67 0x557e0bd9dc35 0x557e0bd3073a 0x557e0bd9f93b 0x557e0bd9dc35 0x557e0bd3073a 0x557e0bd9f93b 0x557e0bd9dc35 0x557e0bd30dd1\n",
            "\u001b[K     |████████████████████████████████| 1982.2 MB 2.2 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu111) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu111) (1.19.5)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.8.0+cu111 which is incompatible.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.8.0+cu111 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.0+cu111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI8nuePstlic"
      },
      "source": [
        "The proper torchtext version to accompany the older torch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynMDU2DBEdk2",
        "outputId": "4863a6f9-5db5-4f8a-ceb1-8c57493c95dd"
      },
      "source": [
        "!pip install torchtext==0.9.0"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.9.0\n",
            "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 17.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (4.41.1)\n",
            "Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (1.8.0+cu111)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (2.26.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->torchtext==0.9.0) (3.7.4.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2.0.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (1.26.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2021.5.30)\n",
            "Installing collected packages: torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.10.0\n",
            "    Uninstalling torchtext-0.10.0:\n",
            "      Successfully uninstalled torchtext-0.10.0\n",
            "Successfully installed torchtext-0.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPUxbZf1xtD3"
      },
      "source": [
        "Import the necessary libraries, functions and classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nQUQIFE1G2b"
      },
      "source": [
        "import io\n",
        "import time\n",
        "import copy\n",
        "import math\n",
        "import wandb\n",
        "import torch\n",
        "import warnings\n",
        "import torchtext\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from torch.nn import Module\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.parameter import Parameter, UninitializedParameter\n",
        "from torch.nn import init\n",
        "from torch.nn.init import constant_\n",
        "from torch.nn.init import xavier_normal_\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import MultiheadAttention\n",
        "from torch.nn import ModuleList\n",
        "from torch.nn.init import xavier_uniform_\n",
        "from torch.nn import Dropout\n",
        "from torch.nn import Linear\n",
        "from torch.nn import LayerNorm\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "from torch.nn import (TransformerEncoder, TransformerDecoder,\n",
        "                      TransformerEncoderLayer, TransformerDecoderLayer)\n",
        "from torch.nn.functional import *\n",
        "\n",
        "from torchtext.vocab import Vocab\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.utils import download_from_url, extract_archive\n",
        "\n",
        "from torchinfo import summary\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from typing import Optional, Any\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "#from torch import nn\n",
        "#from torch.nn.functional import multi_head_attention_forward"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vMfnb5QxcH6"
      },
      "source": [
        "Set seed for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1ez-YKmxY_3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6c63661-46f1-489a-d2d4-24bf06853e45"
      },
      "source": [
        "torch.manual_seed(0)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f4ca0a89a10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qvyw2aRzbtT"
      },
      "source": [
        "Select GPU to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-6nmXdLzZoz"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGcCu4bMx-rg"
      },
      "source": [
        "# Data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zitjiif1tquH"
      },
      "source": [
        "This is optional. You might want to connect your Google Drive and pull your data from there if you are going to repeatedly run this notebook, so that you do not download the data over and over again. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UHTPvN-Wd13",
        "outputId": "db41a558-fa3a-47f3-e75b-5611539ed98d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tbGNyhFqKrH"
      },
      "source": [
        "The data is available in [this repo](https://github.com/tlatkowski/st). The line below will download it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnWwdOK5k7yL",
        "outputId": "e83cfe3e-37a1-4bad-cf42-1e4743a39313"
      },
      "source": [
        "!git clone https://github.com/tlatkowski/st"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'st'...\n",
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Total 18 (delta 0), reused 0 (delta 0), pack-reused 18\u001b[K\n",
            "Unpacking objects: 100% (18/18), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErayClJcuKBw"
      },
      "source": [
        "Extract the .tgz archive from the cloned Github repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7JnBVezOiqs"
      },
      "source": [
        "!tar -xf /content/st/shakespeare.train.tgz\n",
        "!tar -xf /content/st/shakespeare.dev.tgz\n",
        "!tar -xf /content/st/shakespeare.test.tgz"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPc3DhoWzBhF"
      },
      "source": [
        "Load the data, tokenize it with the Spacy tokenizer and construct a vocabulary. This is word level tokenization and every unique word is assigned a token, along with special tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gzo6oMyzt653"
      },
      "source": [
        "train_filepaths = ['/content/train.modern', '/content/train.original']\n",
        "val_filepaths = ['/content/dev.modern', '/content/dev.original']\n",
        "test_filepaths = ['/content/test.modern', '/content/test.original']\n",
        "\n",
        "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "def build_vocab(filepath, tokenizer):\n",
        "  counter = Counter()\n",
        "  with io.open(filepath, encoding=\"utf8\") as f:\n",
        "    for string_ in f:\n",
        "      counter.update(tokenizer(string_))\n",
        "  return Vocab(counter, max_size=8188, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
        "\n",
        "modern_vocab = build_vocab('/content/train.modern', en_tokenizer)\n",
        "original_vocab = build_vocab('/content/train.original', en_tokenizer)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnnM2gJ30JU9"
      },
      "source": [
        "Convert the text data to Torch tensors using the vocabulary obtained at the previous step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv7ZwefVtP8U"
      },
      "source": [
        "def data_process(filepaths):\n",
        "  raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
        "  raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
        "  data = []\n",
        "  for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
        "    mo_tensor_ = torch.tensor([modern_vocab[token] for token in en_tokenizer(raw_de.rstrip(\"\\n\"))],\n",
        "                            dtype=torch.long)\n",
        "    og_tensor_ = torch.tensor([original_vocab[token] for token in en_tokenizer(raw_en.rstrip(\"\\n\"))],\n",
        "                            dtype=torch.long)\n",
        "    data.append((mo_tensor_, og_tensor_))\n",
        "  return data\n",
        "\n",
        "train_data = data_process(train_filepaths)\n",
        "val_data = data_process(val_filepaths)\n",
        "test_data = data_process(test_filepaths)\n",
        "\n",
        "PAD_IDX = modern_vocab['<pad>']\n",
        "BOS_IDX = modern_vocab['<bos>']\n",
        "EOS_IDX = modern_vocab['<eos>']"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxds3dCF0YIW"
      },
      "source": [
        "Set batch size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RtKIEdO0XNr"
      },
      "source": [
        "BATCH_SIZE = 32"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ornl_Fpk0gJs"
      },
      "source": [
        "Construct a Torch dataloader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnLNCst7t655"
      },
      "source": [
        "def generate_batch(data_batch):\n",
        "  de_batch, en_batch = [], []\n",
        "  for (de_item, en_item) in data_batch:\n",
        "    de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "    en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "  de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n",
        "  en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
        "  return de_batch, en_batch\n",
        "\n",
        "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=False, collate_fn=generate_batch)\n",
        "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=False, collate_fn=generate_batch)\n",
        "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
        "                       shuffle=False, collate_fn=generate_batch)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVzA046ayORJ"
      },
      "source": [
        "# The PHM Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGMY7TLQ0uSm"
      },
      "source": [
        "This is the PHM Layer, the heart of our code. It is adapted from the Torch linear layer. Please refer to the Github repo for more details. The A and S tensors are the same tensors as they are in the explanation in the repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htvjhR063fn0"
      },
      "source": [
        "class PHMLayer(nn.Module):\n",
        "\n",
        "  def __init__(self, n, in_features, out_features):\n",
        "    super(PHMLayer, self).__init__()\n",
        "    self.n = n\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "\n",
        "    self.bias = Parameter(torch.Tensor(out_features))\n",
        "\n",
        "    self.a = Parameter(torch.nn.init.xavier_uniform_(torch.zeros((n, n, n))))\n",
        "\n",
        "    self.s = Parameter(torch.nn.init.xavier_uniform_(torch.zeros((n, self.out_features//n, self.in_features//n))))\n",
        "\n",
        "    self.weight = torch.zeros((self.out_features, self.in_features))\n",
        "\n",
        "    fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
        "    bound = 1 / math.sqrt(fan_in)\n",
        "    init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "\n",
        "  def kronecker_product1(self, a, b): #adapted from Bayer Research's implementation\n",
        "    siz1 = torch.Size(torch.tensor(a.shape[-2:]) * torch.tensor(b.shape[-2:]))\n",
        "    res = a.unsqueeze(-1).unsqueeze(-3) * b.unsqueeze(-2).unsqueeze(-4)\n",
        "    siz0 = res.shape[:-4]\n",
        "    out = res.reshape(siz0 + siz1)\n",
        "    return out\n",
        "\n",
        "  def forward(self, input: Tensor) -> Tensor:\n",
        "    self.weight = torch.sum(self.kronecker_product1(self.a, self.s), dim=0)\n",
        "    input = input.type(dtype=self.weight.type())\n",
        "    return F.linear(input, weight=self.weight, bias=self.bias)\n",
        "\n",
        "  def extra_repr(self) -> str:\n",
        "    return 'in_features={}, out_features={}, bias={}'.format(\n",
        "      self.in_features, self.out_features, self.bias is not None)\n",
        "    \n",
        "  def reset_parameters(self) -> None:\n",
        "    init.kaiming_uniform_(self.a, a=math.sqrt(5))\n",
        "    init.kaiming_uniform_(self.s, a=math.sqrt(5))\n",
        "    fan_in, _ = init._calculate_fan_in_and_fan_out(self.placeholder)\n",
        "    bound = 1 / math.sqrt(fan_in)\n",
        "    init.uniform_(self.bias, -bound, bound)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HM6Xq1bA1UzS"
      },
      "source": [
        "The forward method of PHM multi head attention. It is adapted from Torch's multi_head_attention_forward. The part between '#'s are modified. The A and S tensors are used to create the weight matrix H for out projection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alyPyVkXSQoC"
      },
      "source": [
        "def phm_multi_head_attention_forward(\n",
        "    phm: int,\n",
        "    a: Tensor,\n",
        "    s: Tensor,\n",
        "    query: Tensor,\n",
        "    key: Tensor,\n",
        "    value: Tensor,\n",
        "    embed_dim_to_check: int,\n",
        "    num_heads: int,\n",
        "    in_proj_weight: Tensor,\n",
        "    in_proj_bias: Tensor,\n",
        "    bias_k: Optional[Tensor],\n",
        "    bias_v: Optional[Tensor],\n",
        "    add_zero_attn: bool,\n",
        "    dropout_p: float,\n",
        "    out_proj_weight: Tensor,\n",
        "    out_proj_bias: Tensor,\n",
        "    training: bool = True,\n",
        "    key_padding_mask: Optional[Tensor] = None,\n",
        "    need_weights: bool = True,\n",
        "    attn_mask: Optional[Tensor] = None,\n",
        "    use_separate_proj_weight: bool = False,\n",
        "    q_proj_weight: Optional[Tensor] = None,\n",
        "    k_proj_weight: Optional[Tensor] = None,\n",
        "    v_proj_weight: Optional[Tensor] = None,\n",
        "    static_k: Optional[Tensor] = None,\n",
        "    static_v: Optional[Tensor] = None,\n",
        ") -> Tuple[Tensor, Optional[Tensor]]:\n",
        "    r\"\"\"\n",
        "    Args:\n",
        "        query, key, value: map a query and a set of key-value pairs to an output.\n",
        "            See \"Attention Is All You Need\" for more details.\n",
        "        embed_dim_to_check: total dimension of the model.\n",
        "        num_heads: parallel attention heads.\n",
        "        in_proj_weight, in_proj_bias: input projection weight and bias.\n",
        "        bias_k, bias_v: bias of the key and value sequences to be added at dim=0.\n",
        "        add_zero_attn: add a new batch of zeros to the key and\n",
        "                       value sequences at dim=1.\n",
        "        dropout_p: probability of an element to be zeroed.\n",
        "        out_proj_weight, out_proj_bias: the output projection weight and bias.\n",
        "        training: apply dropout if is ``True``.\n",
        "        key_padding_mask: if provided, specified padding elements in the key will\n",
        "            be ignored by the attention. This is an binary mask. When the value is True,\n",
        "            the corresponding value on the attention layer will be filled with -inf.\n",
        "        need_weights: output attn_output_weights.\n",
        "        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n",
        "            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n",
        "        use_separate_proj_weight: the function accept the proj. weights for query, key,\n",
        "            and value in different forms. If false, in_proj_weight will be used, which is\n",
        "            a combination of q_proj_weight, k_proj_weight, v_proj_weight.\n",
        "        q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.\n",
        "        static_k, static_v: static key and value used for attention operators.\n",
        "\n",
        "\n",
        "    Shape:\n",
        "        Inputs:\n",
        "        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
        "          the embedding dimension.\n",
        "        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
        "          the embedding dimension.\n",
        "        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
        "          the embedding dimension.\n",
        "        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n",
        "          If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions\n",
        "          will be unchanged. If a BoolTensor is provided, the positions with the\n",
        "          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
        "        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n",
        "          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,\n",
        "          S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked\n",
        "          positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n",
        "          while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n",
        "          are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
        "          is provided, it will be added to the attention weight.\n",
        "        - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n",
        "          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n",
        "        - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n",
        "          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n",
        "\n",
        "        Outputs:\n",
        "        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
        "          E is the embedding dimension.\n",
        "        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n",
        "          L is the target sequence length, S is the source sequence length.\n",
        "    \"\"\"\n",
        "    tens_ops = (query, key, value, in_proj_weight, in_proj_bias, bias_k, bias_v, out_proj_weight, out_proj_bias)\n",
        "    if has_torch_function(tens_ops):\n",
        "        return handle_torch_function(\n",
        "            multi_head_attention_forward,\n",
        "            tens_ops,\n",
        "            query,\n",
        "            key,\n",
        "            value,\n",
        "            embed_dim_to_check,\n",
        "            num_heads,\n",
        "            in_proj_weight,\n",
        "            in_proj_bias,\n",
        "            bias_k,\n",
        "            bias_v,\n",
        "            add_zero_attn,\n",
        "            dropout_p,\n",
        "            out_proj_weight,\n",
        "            out_proj_bias,\n",
        "            training=training,\n",
        "            key_padding_mask=key_padding_mask,\n",
        "            need_weights=need_weights,\n",
        "            attn_mask=attn_mask,\n",
        "            use_separate_proj_weight=use_separate_proj_weight,\n",
        "            q_proj_weight=q_proj_weight,\n",
        "            k_proj_weight=k_proj_weight,\n",
        "            v_proj_weight=v_proj_weight,\n",
        "            static_k=static_k,\n",
        "            static_v=static_v,\n",
        "        )\n",
        "    tgt_len, bsz, embed_dim = query.size()\n",
        "    assert embed_dim == embed_dim_to_check\n",
        "    # allow MHA to have different sizes for the feature dimension\n",
        "    assert key.size(0) == value.size(0) and key.size(1) == value.size(1)\n",
        "\n",
        "    head_dim = embed_dim // num_heads\n",
        "    assert head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
        "    scaling = float(head_dim) ** -0.5\n",
        "\n",
        "    if not use_separate_proj_weight:\n",
        "        if (query is key or torch.equal(query, key)) and (key is value or torch.equal(key, value)):\n",
        "            # self-attention\n",
        "            q, k, v = linear(query, in_proj_weight, in_proj_bias).chunk(3, dim=-1)\n",
        "\n",
        "        elif key is value or torch.equal(key, value):\n",
        "            # encoder-decoder attention\n",
        "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
        "            _b = in_proj_bias\n",
        "            _start = 0\n",
        "            _end = embed_dim\n",
        "            _w = in_proj_weight[_start:_end, :]\n",
        "            if _b is not None:\n",
        "                _b = _b[_start:_end]\n",
        "            q = linear(query, _w, _b)\n",
        "\n",
        "            if key is None:\n",
        "                assert value is None\n",
        "                k = None\n",
        "                v = None\n",
        "            else:\n",
        "\n",
        "                # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
        "                _b = in_proj_bias\n",
        "                _start = embed_dim\n",
        "                _end = None\n",
        "                _w = in_proj_weight[_start:, :]\n",
        "                if _b is not None:\n",
        "                    _b = _b[_start:]\n",
        "                k, v = linear(key, _w, _b).chunk(2, dim=-1)\n",
        "\n",
        "        else:\n",
        "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
        "            _b = in_proj_bias\n",
        "            _start = 0\n",
        "            _end = embed_dim\n",
        "            _w = in_proj_weight[_start:_end, :]\n",
        "            if _b is not None:\n",
        "                _b = _b[_start:_end]\n",
        "            q = linear(query, _w, _b)\n",
        "\n",
        "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
        "            _b = in_proj_bias\n",
        "            _start = embed_dim\n",
        "            _end = embed_dim * 2\n",
        "            _w = in_proj_weight[_start:_end, :]\n",
        "            if _b is not None:\n",
        "                _b = _b[_start:_end]\n",
        "            k = linear(key, _w, _b)\n",
        "\n",
        "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
        "            _b = in_proj_bias\n",
        "            _start = embed_dim * 2\n",
        "            _end = None\n",
        "            _w = in_proj_weight[_start:, :]\n",
        "            if _b is not None:\n",
        "                _b = _b[_start:]\n",
        "            v = linear(value, _w, _b)\n",
        "    else:\n",
        "        q_proj_weight_non_opt = torch.jit._unwrap_optional(q_proj_weight)\n",
        "        len1, len2 = q_proj_weight_non_opt.size()\n",
        "        assert len1 == embed_dim and len2 == query.size(-1)\n",
        "\n",
        "        k_proj_weight_non_opt = torch.jit._unwrap_optional(k_proj_weight)\n",
        "        len1, len2 = k_proj_weight_non_opt.size()\n",
        "        assert len1 == embed_dim and len2 == key.size(-1)\n",
        "\n",
        "        v_proj_weight_non_opt = torch.jit._unwrap_optional(v_proj_weight)\n",
        "        len1, len2 = v_proj_weight_non_opt.size()\n",
        "        assert len1 == embed_dim and len2 == value.size(-1)\n",
        "\n",
        "        if in_proj_bias is not None:\n",
        "            q = linear(query, q_proj_weight_non_opt, in_proj_bias[0:embed_dim])\n",
        "            k = linear(key, k_proj_weight_non_opt, in_proj_bias[embed_dim : (embed_dim * 2)])\n",
        "            v = linear(value, v_proj_weight_non_opt, in_proj_bias[(embed_dim * 2) :])\n",
        "        else:\n",
        "            q = linear(query, q_proj_weight_non_opt, in_proj_bias)\n",
        "            k = linear(key, k_proj_weight_non_opt, in_proj_bias)\n",
        "            v = linear(value, v_proj_weight_non_opt, in_proj_bias)\n",
        "    q = q * scaling\n",
        "\n",
        "    if attn_mask is not None:\n",
        "        assert (\n",
        "            attn_mask.dtype == torch.float32\n",
        "            or attn_mask.dtype == torch.float64\n",
        "            or attn_mask.dtype == torch.float16\n",
        "            or attn_mask.dtype == torch.uint8\n",
        "            or attn_mask.dtype == torch.bool\n",
        "        ), \"Only float, byte, and bool types are supported for attn_mask, not {}\".format(attn_mask.dtype)\n",
        "        if attn_mask.dtype == torch.uint8:\n",
        "            warnings.warn(\"Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
        "            attn_mask = attn_mask.to(torch.bool)\n",
        "\n",
        "        if attn_mask.dim() == 2:\n",
        "            attn_mask = attn_mask.unsqueeze(0)\n",
        "            if list(attn_mask.size()) != [1, query.size(0), key.size(0)]:\n",
        "                raise RuntimeError(\"The size of the 2D attn_mask is not correct.\")\n",
        "        elif attn_mask.dim() == 3:\n",
        "            if list(attn_mask.size()) != [bsz * num_heads, query.size(0), key.size(0)]:\n",
        "                raise RuntimeError(\"The size of the 3D attn_mask is not correct.\")\n",
        "        else:\n",
        "            raise RuntimeError(\"attn_mask's dimension {} is not supported\".format(attn_mask.dim()))\n",
        "        # attn_mask's dim is 3 now.\n",
        "\n",
        "    # convert ByteTensor key_padding_mask to bool\n",
        "    if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n",
        "        warnings.warn(\n",
        "            \"Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\"\n",
        "        )\n",
        "        key_padding_mask = key_padding_mask.to(torch.bool)\n",
        "\n",
        "    if bias_k is not None and bias_v is not None:\n",
        "        if static_k is None and static_v is None:\n",
        "            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])\n",
        "            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])\n",
        "            if attn_mask is not None:\n",
        "                attn_mask = pad(attn_mask, (0, 1))\n",
        "            if key_padding_mask is not None:\n",
        "                key_padding_mask = pad(key_padding_mask, (0, 1))\n",
        "        else:\n",
        "            assert static_k is None, \"bias cannot be added to static key.\"\n",
        "            assert static_v is None, \"bias cannot be added to static value.\"\n",
        "    else:\n",
        "        assert bias_k is None\n",
        "        assert bias_v is None\n",
        "\n",
        "    q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
        "    if k is not None:\n",
        "        k = k.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
        "    if v is not None:\n",
        "        v = v.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
        "\n",
        "    if static_k is not None:\n",
        "        assert static_k.size(0) == bsz * num_heads\n",
        "        assert static_k.size(2) == head_dim\n",
        "        k = static_k\n",
        "\n",
        "    if static_v is not None:\n",
        "        assert static_v.size(0) == bsz * num_heads\n",
        "        assert static_v.size(2) == head_dim\n",
        "        v = static_v\n",
        "\n",
        "    src_len = k.size(1)\n",
        "\n",
        "    if key_padding_mask is not None:\n",
        "        assert key_padding_mask.size(0) == bsz\n",
        "        assert key_padding_mask.size(1) == src_len\n",
        "\n",
        "    if add_zero_attn:\n",
        "        src_len += 1\n",
        "        k = torch.cat([k, torch.zeros((k.size(0), 1) + k.size()[2:], dtype=k.dtype, device=k.device)], dim=1)\n",
        "        v = torch.cat([v, torch.zeros((v.size(0), 1) + v.size()[2:], dtype=v.dtype, device=v.device)], dim=1)\n",
        "        if attn_mask is not None:\n",
        "            attn_mask = pad(attn_mask, (0, 1))\n",
        "        if key_padding_mask is not None:\n",
        "            key_padding_mask = pad(key_padding_mask, (0, 1))\n",
        "\n",
        "    attn_output_weights = torch.bmm(q, k.transpose(1, 2))\n",
        "    assert list(attn_output_weights.size()) == [bsz * num_heads, tgt_len, src_len]\n",
        "\n",
        "    if attn_mask is not None:\n",
        "        if attn_mask.dtype == torch.bool:\n",
        "            attn_output_weights.masked_fill_(attn_mask, float(\"-inf\"))\n",
        "        else:\n",
        "            attn_output_weights += attn_mask\n",
        "\n",
        "    if key_padding_mask is not None:\n",
        "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
        "        attn_output_weights = attn_output_weights.masked_fill(\n",
        "            key_padding_mask.unsqueeze(1).unsqueeze(2),\n",
        "            float(\"-inf\"),\n",
        "        )\n",
        "        attn_output_weights = attn_output_weights.view(bsz * num_heads, tgt_len, src_len)\n",
        "\n",
        "    attn_output_weights = softmax(attn_output_weights, dim=-1)\n",
        "    attn_output_weights = dropout(attn_output_weights, p=dropout_p, training=training)\n",
        "\n",
        "    attn_output = torch.bmm(attn_output_weights, v)\n",
        "    assert list(attn_output.size()) == [bsz * num_heads, tgt_len, head_dim]\n",
        "    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
        "\n",
        "    ######################################################################################\n",
        "\n",
        "    def kronecker_product1(a, b):\n",
        "      siz1 = torch.Size(torch.tensor(a.shape[-2:]) * torch.tensor(b.shape[-2:]))\n",
        "      res = a.unsqueeze(-1).unsqueeze(-3) * b.unsqueeze(-2).unsqueeze(-4)\n",
        "      siz0 = res.shape[:-4]\n",
        "      out = res.reshape(siz0 + siz1)\n",
        "      return out\n",
        "\n",
        "    out_proj_weight2 = torch.sum(kronecker_product1(a, s), dim=0) #the weight matrix H is constructed from a and s tensors\n",
        "\n",
        "    attn_output = linear(attn_output, out_proj_weight2, out_proj_bias)\n",
        "\n",
        "    ######################################################################################\n",
        "\n",
        "    if need_weights:\n",
        "        # average attention weights over heads\n",
        "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
        "        return attn_output, attn_output_weights.sum(dim=1) / num_heads\n",
        "    else:\n",
        "        return attn_output, None"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeDKC5HCEPr2"
      },
      "source": [
        "The weights that are multiplied with the input to create Q, K, V in multiheaded attention are created from A and S. Also a PHM layer is defined to use its weights in the out proj."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-puq5PXxr6y"
      },
      "source": [
        "class PHMMultiheadAttention(Module):\n",
        "    r\"\"\"Allows the model to jointly attend to information\n",
        "    from different representation subspaces.\n",
        "    See `Attention Is All You Need <https://arxiv.org/abs/1706.03762>`_\n",
        "\n",
        "    .. math::\n",
        "        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n",
        "\n",
        "    where :math:`head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`.\n",
        "\n",
        "    Args:\n",
        "        embed_dim: total dimension of the model.\n",
        "        num_heads: parallel attention heads.\n",
        "        dropout: a Dropout layer on attn_output_weights. Default: 0.0.\n",
        "        bias: add bias as module parameter. Default: True.\n",
        "        add_bias_kv: add bias to the key and value sequences at dim=0.\n",
        "        add_zero_attn: add a new batch of zeros to the key and\n",
        "                       value sequences at dim=1.\n",
        "        kdim: total number of features in key. Default: None.\n",
        "        vdim: total number of features in value. Default: None.\n",
        "        batch_first: If ``True``, then the input and output tensors are provided\n",
        "            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
        "\n",
        "    Note that if :attr:`kdim` and :attr:`vdim` are None, they will be set\n",
        "    to :attr:`embed_dim` such that query, key, and value have the same\n",
        "    number of features.\n",
        "\n",
        "    Examples::\n",
        "\n",
        "        >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
        "    \"\"\"\n",
        "    __constants__ = ['batch_first']\n",
        "    bias_k: Optional[torch.Tensor]\n",
        "    bias_v: Optional[torch.Tensor]\n",
        "\n",
        "    def __init__(self, phm, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False,\n",
        "                 kdim=None, vdim=None, batch_first=False, device=None, dtype=None) -> None:\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        super(PHMMultiheadAttention, self).__init__()\n",
        "        ###########################################################################################################\n",
        "        self.phm = phm # this is n in the explanation\n",
        "        ###########################################################################################################\n",
        "        self.embed_dim = embed_dim\n",
        "        self.kdim = kdim if kdim is not None else embed_dim\n",
        "        self.vdim = vdim if vdim is not None else embed_dim\n",
        "        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.batch_first = batch_first\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        if self._qkv_same_embed_dim is False:\n",
        "            self.q_proj_weight = Parameter(torch.empty((embed_dim, embed_dim), **factory_kwargs))\n",
        "            self.k_proj_weight = Parameter(torch.empty((embed_dim, self.kdim), **factory_kwargs))\n",
        "            self.v_proj_weight = Parameter(torch.empty((embed_dim, self.vdim), **factory_kwargs))\n",
        "            self.register_parameter('in_proj_weight', None)\n",
        "        else:\n",
        "            ###########################################################################################################\n",
        "            self.a = Parameter(torch.nn.init.xavier_uniform_(torch.zeros((phm, phm, phm))))\n",
        "\n",
        "            self.s = Parameter(torch.nn.init.xavier_uniform_(torch.zeros((phm, (3*embed_dim)//phm, embed_dim//phm))))\n",
        "            ###########################################################################################################\n",
        "            self.register_parameter('q_proj_weight', None)\n",
        "            self.register_parameter('k_proj_weight', None)\n",
        "            self.register_parameter('v_proj_weight', None)\n",
        "\n",
        "        if bias:\n",
        "            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim, **factory_kwargs))\n",
        "        else:\n",
        "            self.register_parameter('in_proj_bias', None)\n",
        "        ###########################################################################################################\n",
        "        self.out_proj = PHMLayer(phm, embed_dim, embed_dim)\n",
        "        ###########################################################################################################\n",
        "\n",
        "        if add_bias_kv:\n",
        "            self.bias_k = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n",
        "            self.bias_v = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n",
        "        else:\n",
        "            self.bias_k = self.bias_v = None\n",
        "\n",
        "        self.add_zero_attn = add_zero_attn\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        if self._qkv_same_embed_dim:\n",
        "            #xavier_uniform_(self.in_proj_weight) #we initialize the weights ourself\n",
        "            pass\n",
        "        else:\n",
        "            xavier_uniform_(self.q_proj_weight)\n",
        "            xavier_uniform_(self.k_proj_weight)\n",
        "            xavier_uniform_(self.v_proj_weight)\n",
        "\n",
        "        if self.in_proj_bias is not None:\n",
        "            constant_(self.in_proj_bias, 0.)\n",
        "            constant_(self.out_proj.bias, 0.)\n",
        "        if self.bias_k is not None:\n",
        "            xavier_normal_(self.bias_k)\n",
        "        if self.bias_v is not None:\n",
        "            xavier_normal_(self.bias_v)\n",
        "    ###########################################################################################################\n",
        "    def kronecker_product1(self, a, b):\n",
        "        siz1 = torch.Size(torch.tensor(a.shape[-2:]) * torch.tensor(b.shape[-2:]))\n",
        "        res = a.unsqueeze(-1).unsqueeze(-3) * b.unsqueeze(-2).unsqueeze(-4)\n",
        "        siz0 = res.shape[:-4]\n",
        "        out = res.reshape(siz0 + siz1)\n",
        "        return out\n",
        "    ###########################################################################################################\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        # Support loading old MultiheadAttention checkpoints generated by v1.1.0\n",
        "        if '_qkv_same_embed_dim' not in state:\n",
        "            state['_qkv_same_embed_dim'] = True\n",
        "\n",
        "        super(PHMMultiheadAttention, self).__setstate__(state)\n",
        "\n",
        "    def forward(self, query: Tensor, key: Tensor, value: Tensor, key_padding_mask: Optional[Tensor] = None,\n",
        "                need_weights: bool = True, attn_mask: Optional[Tensor] = None):\n",
        "        r\"\"\"\n",
        "    Args:\n",
        "        query, key, value: map a query and a set of key-value pairs to an output.\n",
        "            See \"Attention Is All You Need\" for more details.\n",
        "        key_padding_mask: if provided, specified padding elements in the key will\n",
        "            be ignored by the attention. When given a binary mask and a value is True,\n",
        "            the corresponding value on the attention layer will be ignored. When given\n",
        "            a byte mask and a value is non-zero, the corresponding value on the attention\n",
        "            layer will be ignored\n",
        "        need_weights: output attn_output_weights.\n",
        "        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n",
        "            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n",
        "\n",
        "    Shapes for inputs:\n",
        "        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
        "          the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n",
        "        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
        "          the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n",
        "        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
        "          the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n",
        "        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n",
        "          If a ByteTensor is provided, the non-zero positions will be ignored while the position\n",
        "          with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the\n",
        "          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
        "        - attn_mask: if a 2D mask: :math:`(L, S)` where L is the target sequence length, S is the\n",
        "          source sequence length.\n",
        "\n",
        "          If a 3D mask: :math:`(N\\cdot\\text{num\\_heads}, L, S)` where N is the batch size, L is the target sequence\n",
        "          length, S is the source sequence length. ``attn_mask`` ensure that position i is allowed to attend\n",
        "          the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n",
        "          while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n",
        "          is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
        "          is provided, it will be added to the attention weight.\n",
        "\n",
        "    Shapes for outputs:\n",
        "        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
        "          E is the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n",
        "        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n",
        "          L is the target sequence length, S is the source sequence length.\n",
        "        \"\"\"\n",
        "        self.in_proj_weight = torch.sum(self.kronecker_product1(self.a, self.s), dim=0)\n",
        "\n",
        "        if self.batch_first:\n",
        "            query, key, value = [x.transpose(1, 0) for x in (query, key, value)]\n",
        "        ###########################################################################################################\n",
        "        if not self._qkv_same_embed_dim:\n",
        "            attn_output, attn_output_weights = phm_multi_head_attention_forward(\n",
        "                self.phm, self.out_proj.a, self.out_proj.s, query, key, value, self.embed_dim, self.num_heads,\n",
        "                self.in_proj_weight, self.in_proj_bias,\n",
        "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
        "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
        "                training=self.training,\n",
        "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
        "                attn_mask=attn_mask, use_separate_proj_weight=True,\n",
        "                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\n",
        "                v_proj_weight=self.v_proj_weight,\n",
        "                )\n",
        "        else:\n",
        "            attn_output, attn_output_weights = phm_multi_head_attention_forward(\n",
        "                self.phm, self.out_proj.a, self.out_proj.s, query, key, value, self.embed_dim, self.num_heads,\n",
        "                self.in_proj_weight, self.in_proj_bias,\n",
        "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
        "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
        "                training=self.training,\n",
        "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
        "                attn_mask=attn_mask)\n",
        "        ###########################################################################################################\n",
        "        if self.batch_first:\n",
        "            return attn_output.transpose(1, 0), attn_output_weights\n",
        "        else:\n",
        "            return attn_output, attn_output_weights"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2YiTvhPEpCd"
      },
      "source": [
        "The feed forward network and the attention mechanism is replaced with PHM counterparts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X50ThC5O109i"
      },
      "source": [
        "def _get_activation_fn(activation):\n",
        "    if activation == \"relu\":\n",
        "        return F.relu\n",
        "    elif activation == \"gelu\":\n",
        "        return F.gelu\n",
        "\n",
        "    raise RuntimeError(\"activation should be relu/gelu, not {}\".format(activation))\n",
        "\n",
        "class PHMTransformerEncoderLayer(Module):\n",
        "\n",
        "    __constants__ = ['batch_first']\n",
        "\n",
        "    def __init__(self, phm, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\",\n",
        "                 layer_norm_eps=1e-5, batch_first=False,\n",
        "                 device=None, dtype=None) -> None:\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        ###########################################################################################################\n",
        "        super(PHMTransformerEncoderLayer, self).__init__()\n",
        "        self.self_attn = PHMMultiheadAttention(phm, d_model, nhead, dropout=dropout)\n",
        "        self.linear1 = PHMLayer(phm, d_model, dim_feedforward)\n",
        "        self.dropout = Dropout(dropout)\n",
        "        self.linear2 = PHMLayer(phm, dim_feedforward, d_model)\n",
        "        ###########################################################################################################\n",
        "        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        self.dropout1 = Dropout(dropout)\n",
        "        self.dropout2 = Dropout(dropout)\n",
        "\n",
        "        self.activation = _get_activation_fn(activation)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        if 'activation' not in state:\n",
        "            state['activation'] = F.relu\n",
        "        super(PHMTransformerEncoderLayer, self).__setstate__(state)\n",
        "\n",
        "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
        " \n",
        "        src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2IdiLv7ExDo"
      },
      "source": [
        "The feed forward network and the attention mechanism is replaced with PHM counterparts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US8QRCVYtVVw"
      },
      "source": [
        "class PHMTransformerDecoderLayer(Module):\n",
        "\n",
        "    __constants__ = ['batch_first']\n",
        "\n",
        "    def __init__(self, phm, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\",\n",
        "                 layer_norm_eps=1e-5, batch_first=False, device=None, dtype=None) -> None:\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        ###########################################################################################################\n",
        "        super(PHMTransformerDecoderLayer, self).__init__()\n",
        "        self.self_attn = PHMMultiheadAttention(phm, d_model, nhead, dropout=dropout)\n",
        "        self.multihead_attn = PHMMultiheadAttention(phm, d_model, nhead, dropout=dropout)\n",
        "        self.linear1 = PHMLayer(phm, d_model, dim_feedforward)\n",
        "        self.dropout = Dropout(dropout)\n",
        "        self.linear2 = PHMLayer(phm, dim_feedforward, d_model)\n",
        "        ###########################################################################################################\n",
        "        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        self.norm3 = LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        self.dropout1 = Dropout(dropout)\n",
        "        self.dropout2 = Dropout(dropout)\n",
        "        self.dropout3 = Dropout(dropout)\n",
        "\n",
        "        self.activation = _get_activation_fn(activation)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        if 'activation' not in state:\n",
        "            state['activation'] = F.relu\n",
        "        super(PHMTransformerDecoderLayer, self).__setstate__(state)\n",
        "\n",
        "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None,\n",
        "                tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
        "\n",
        "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt = self.norm1(tgt)\n",
        "        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt = self.norm2(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        tgt = self.norm3(tgt)\n",
        "        return tgt\n",
        "\n",
        "    def _get_clones(module, N):\n",
        "        return ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "\n",
        "    def _get_activation_fn(activation):\n",
        "        if activation == \"relu\":\n",
        "            return F.relu\n",
        "        elif activation == \"gelu\":\n",
        "            return F.gelu\n",
        "\n",
        "        raise RuntimeError(\"activation should be relu/gelu, not {}\".format(activation))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boBnW4x0EzvT"
      },
      "source": [
        "The embedding layer, encoder, decoder and the generator are replaced with PHM counterparts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-2di7MD1o23"
      },
      "source": [
        "class PHMTransformer(Module):\n",
        "    def __init__(self, phm, nhead, num_encoder_layers: int, num_decoder_layers: int,\n",
        "                 emb_size: int, src_vocab_size: int, tgt_vocab_size: int,\n",
        "                 dim_feedforward:int = 512, dropout:float = 0.1):\n",
        "        ###########################################################################################################\n",
        "        super(PHMTransformer, self).__init__()\n",
        "        encoder_layer = PHMTransformerEncoderLayer(phm, d_model=emb_size, nhead=nhead,\n",
        "                                                dim_feedforward=dim_feedforward)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "        decoder_layer = PHMTransformerDecoderLayer(phm, d_model=emb_size, nhead=nhead,\n",
        "                                                dim_feedforward=dim_feedforward)\n",
        "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "                \n",
        "        self.generator = PHMLayer(phm, emb_size, tgt_vocab_size)   \n",
        "        self.src_tok_emb = PHMTokenEmbedding(src_vocab_size, emb_size, phm)\n",
        "        self.tgt_tok_emb = PHMTokenEmbedding(tgt_vocab_size, emb_size, phm)\n",
        "        ###########################################################################################################\n",
        "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self, src: Tensor, trg: Tensor, src_mask: Tensor,\n",
        "                tgt_mask: Tensor, src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor, memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        memory = self.transformer_encoder(src_emb, src_mask, src_padding_mask)\n",
        "        outs = self.transformer_decoder(tgt_emb, memory, tgt_mask, None,\n",
        "                                        tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer_encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer_decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZu2eJfoE8kr"
      },
      "source": [
        "This is the usual transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCCahYjmt656"
      },
      "source": [
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self, nhead: int, num_encoder_layers: int, num_decoder_layers: int,\n",
        "                 emb_size: int, src_vocab_size: int, tgt_vocab_size: int, \n",
        "                 dim_feedforward:int = 512, dropout:float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        encoder_layer = TransformerEncoderLayer(d_model=emb_size, nhead=nhead,\n",
        "                                                dim_feedforward=dim_feedforward)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "        decoder_layer = TransformerDecoderLayer(d_model=emb_size, nhead=nhead,\n",
        "                                                dim_feedforward=dim_feedforward)\n",
        "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "                \n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self, src: Tensor, trg: Tensor, src_mask: Tensor,\n",
        "                tgt_mask: Tensor, src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor, memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        memory = self.transformer_encoder(src_emb, src_mask, src_padding_mask)\n",
        "        outs = self.transformer_decoder(tgt_emb, memory, tgt_mask, None,\n",
        "                                        tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer_encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer_decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGp9atlkgnvV"
      },
      "source": [
        "class PHMEmbedding(Module):\n",
        "   \n",
        "    __constants__ = ['num_embeddings', 'embedding_dim', 'padding_idx', 'max_norm',\n",
        "                     'norm_type', 'scale_grad_by_freq', 'sparse']\n",
        "\n",
        "    def __init__(self, num_embeddings: int, embedding_dim: int, phm: int, padding_idx: Optional[int] = None,\n",
        "                 max_norm: Optional[float] = None, norm_type: float = 2., scale_grad_by_freq: bool = False,\n",
        "                 sparse: bool = False, _weight: Optional[Tensor] = None,\n",
        "                 device=None, dtype=None):\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        super(PHMEmbedding, self).__init__()\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.embedding_dim = embedding_dim\n",
        "        if padding_idx is not None:\n",
        "            if padding_idx > 0:\n",
        "                assert padding_idx < self.num_embeddings, 'Padding_idx must be within num_embeddings'\n",
        "            elif padding_idx < 0:\n",
        "                assert padding_idx >= -self.num_embeddings, 'Padding_idx must be within num_embeddings'\n",
        "                padding_idx = self.num_embeddings + padding_idx\n",
        "        self.padding_idx = padding_idx\n",
        "        self.max_norm = max_norm\n",
        "        self.norm_type = norm_type\n",
        "        self.scale_grad_by_freq = scale_grad_by_freq\n",
        "        if _weight is None:\n",
        "            ##########################################################################################\n",
        "            self.a = Parameter(torch.nn.init.xavier_uniform_(torch.zeros((phm, phm, phm))))\n",
        "\n",
        "            self.s = Parameter(torch.nn.init.xavier_uniform_(torch.zeros((phm, num_embeddings//phm, embedding_dim//phm))))\n",
        "\n",
        "            self.weight = torch.zeros((num_embeddings, embedding_dim))\n",
        "            ##########################################################################################\n",
        "        else:\n",
        "            assert list(_weight.shape) == [num_embeddings, embedding_dim], \\\n",
        "                'Shape of weight does not match num_embeddings and embedding_dim'\n",
        "            self.weight = Parameter(_weight)\n",
        "\n",
        "        self.sparse = sparse\n",
        "\n",
        "    def reset_parameters(self) -> None:\n",
        "        #init.normal_(self.weight)\n",
        "        #self._fill_padding_idx_with_zero() #not used\n",
        "        pass\n",
        "\n",
        "    def _fill_padding_idx_with_zero(self) -> None:\n",
        "        if self.padding_idx is not None:\n",
        "            with torch.no_grad():\n",
        "                self.weight[self.padding_idx].fill_(0)\n",
        "            \n",
        "    ##########################################################################################\n",
        "    def kronecker_product1(self, a, b):\n",
        "        siz1 = torch.Size(torch.tensor(a.shape[-2:]) * torch.tensor(b.shape[-2:]))\n",
        "        res = a.unsqueeze(-1).unsqueeze(-3) * b.unsqueeze(-2).unsqueeze(-4)\n",
        "        siz0 = res.shape[:-4]\n",
        "        out = res.reshape(siz0 + siz1)\n",
        "        return out\n",
        "    ##########################################################################################\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        ##########################################################################################\n",
        "        self.weight = torch.sum(self.kronecker_product1(self.a, self.s), dim=0)\n",
        "        ##########################################################################################\n",
        "        #self._fill_padding_idx_with_zero()\n",
        "        return F.embedding(\n",
        "            input, self.weight, self.padding_idx, self.max_norm,\n",
        "            self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        s = '{num_embeddings}, {embedding_dim}'\n",
        "        if self.padding_idx is not None:\n",
        "            s += ', padding_idx={padding_idx}'\n",
        "        if self.max_norm is not None:\n",
        "            s += ', max_norm={max_norm}'\n",
        "        if self.norm_type != 2:\n",
        "            s += ', norm_type={norm_type}'\n",
        "        if self.scale_grad_by_freq is not False:\n",
        "            s += ', scale_grad_by_freq={scale_grad_by_freq}'\n",
        "        if self.sparse is not False:\n",
        "            s += ', sparse=True'\n",
        "        return s.format(**self.__dict__)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTSESCTsFV0d"
      },
      "source": [
        "Token embedding is used in the usual transformer whereas PHM transformer uses PHMTokenEmbedding. Positional encoding is untouched."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5n_U0CjTt658"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, emb_size: int, dropout, maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + \n",
        "                            self.pos_embedding[:token_embedding.size(0),:])\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "class PHMTokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size, phm):\n",
        "        super(PHMTokenEmbedding, self).__init__()\n",
        "        self.embedding = PHMEmbedding(vocab_size, emb_size, phm)\n",
        "        self.emb_size = emb_size\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kjh1xctrFgIS"
      },
      "source": [
        "Attention masks are created so that the encoder and decoder only have access to the relevant tokens at a time step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QhCOolDt66A"
      },
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "  src_seq_len = src.shape[0]\n",
        "  tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "  tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "  src_mask = torch.zeros((src_seq_len, src_seq_len), device=device).type(torch.bool)\n",
        "\n",
        "  src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "  tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "  return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ViY1CROFyz8"
      },
      "source": [
        "The usual training eval loops with WandB reporting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLq0dLBCt66C"
      },
      "source": [
        "def train_epoch(model, train_iter, optimizer):\n",
        "  model.train()\n",
        "  losses = 0\n",
        "  for idx, (src, tgt) in enumerate(train_iter):\n",
        "      src = src.to(device)\n",
        "      tgt = tgt.to(device)\n",
        "            \n",
        "      tgt_input = tgt[:-1, :]\n",
        "\n",
        "      src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "      logits = model(src, tgt_input, src_mask, tgt_mask,\n",
        "                                src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      tgt_out = tgt[1:,:]\n",
        "      loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      losses += loss.item()\n",
        "\n",
        "      if idx % 50 == 0:\n",
        "        wandb.log({\"train_loss\": loss.item()})\n",
        "\n",
        "  return losses / len(train_iter)\n",
        "\n",
        "\n",
        "def evaluate(model, val_iter):\n",
        "  model.eval()\n",
        "  losses = 0\n",
        "  for idx, (src, tgt) in (enumerate(valid_iter)):\n",
        "    src = src.to(device)\n",
        "    tgt = tgt.to(device)\n",
        "\n",
        "    tgt_input = tgt[:-1, :]\n",
        "\n",
        "    src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "    logits = model(src, tgt_input, src_mask, tgt_mask,\n",
        "                              src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "    tgt_out = tgt[1:,:]\n",
        "    loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "    losses += loss.item()\n",
        "  return losses / len(val_iter)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhbcsaobB1-c"
      },
      "source": [
        "def bleu(model, src_lines, tgt_lines, src_vcb, tgt_vcb, src_tok):\n",
        "\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  assert len(src_lines) == len(tgt_lines)\n",
        "  n = len(src_lines)\n",
        "  scores = 0\n",
        "  \n",
        "  for mo, og in zip(src_lines, tgt_lines):\n",
        "    reference = [tokenizer.tokenize(og[:-3])]\n",
        "    candidate = tokenizer.tokenize(translate(model, mo[:-3], src_vcb, tgt_vcb, en_tokenizer))\n",
        "\n",
        "    try:\n",
        "      scores += sentence_bleu(reference, candidate, smoothing_function=smoothie)\n",
        "    except:\n",
        "      print('except') # the bleu function very rarely gives exceptions (once every 500 lines or so)\n",
        "      pass\n",
        "  return scores/n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsescJCJydXA"
      },
      "source": [
        "# Instantiate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfVA2zobt66B"
      },
      "source": [
        "SRC_VOCAB_SIZE = len(modern_vocab)\n",
        "TGT_VOCAB_SIZE = len(original_vocab)\n",
        "EMB_SIZE = 512 #embedding size\n",
        "NHEAD = 8 #number of heads in multihead attention\n",
        "FFN_HID_DIM = 512 #feed forward network hidden dimension\n",
        "NUM_ENCODER_LAYERS = 4\n",
        "NUM_DECODER_LAYERS = 4\n",
        "N = 2 #the 1/n scaling factor\n",
        "\n",
        "#this is the phm+ transformer\n",
        "\n",
        "transformer = PHMTransformer(N, NHEAD, NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, \n",
        "                                 EMB_SIZE, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE,\n",
        "                                 FFN_HID_DIM)\n",
        "\"\"\"\n",
        "#this is the usual transformer\n",
        "transformer = Seq2SeqTransformer(NHEAD, NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, \n",
        "                                 EMB_SIZE, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE,\n",
        "                                 FFN_HID_DIM)\n",
        "\"\"\"\n",
        "#one of the transformers above should be commented out\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1: \n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(device)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.05\n",
        ")\n",
        "# at every milestone the learning rate is multiplied with gamma to avoid overfitting\n",
        "scheduler = MultiStepLR(optimizer, milestones=[10, 20, 30], gamma=0.9)\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07yoyaJ-yMfl",
        "outputId": "0a70231b-39f3-4c08-b31c-5e62d1c000e6"
      },
      "source": [
        "print(transformer)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PHMTransformer(\n",
            "  (transformer_encoder): TransformerEncoder(\n",
            "    (layers): ModuleList(\n",
            "      (0): PHMTransformerEncoderLayer(\n",
            "        (self_attn): PHMMultiheadAttention(\n",
            "          (out_proj): PHMLayer(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (linear1): PHMLayer(in_features=512, out_features=512, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): PHMLayer(in_features=512, out_features=512, bias=True)\n",
            "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): PHMTransformerEncoderLayer(\n",
            "        (self_attn): PHMMultiheadAttention(\n",
            "          (out_proj): PHMLayer(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (linear1): PHMLayer(in_features=512, out_features=512, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): PHMLayer(in_features=512, out_features=512, bias=True)\n",
            "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (2): PHMTransformerEncoderLayer(\n",
            "        (self_attn): PHMMultiheadAttention(\n",
            "          (out_proj): PHMLayer(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (linear1): PHMLayer(in_features=512, out_features=512, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): PHMLayer(in_features=512, out_features=512, bias=True)\n",
            "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (3): PHMTransformerEncoderLayer(\n",
            "        (self_attn): PHMMultiheadAttention(\n",
            "          (out_proj): PHMLayer(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (linear1): PHMLayer(in_features=512, out_features=512, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): PHMLayer(in_features=512, out_features=512, bias=True)\n",
            "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (transformer_decoder): TransformerDecoder(\n",
            "    (layers): ModuleList(\n",
            "      (0): PHMTransformerDecoderLayer(\n",
            "        (self_attn): PHMMultiheadAttention(\n",
            "          (out_proj): PHMLayer(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (multihead_attn): PHMMultiheadAttention(\n",
            "          (out_proj): PHMLayer(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (linear1): PHMLayer(in_features=512, out_features=512, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): PHMLayer(in_features=512, out_features=512, bias=True)\n",
            "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        (dropout3): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): PHMTransformerDecoderLayer(\n",
            "        (self_attn): PHMMultiheadAttention(\n",
            "          (out_proj): PHMLayer(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (multihead_attn): PHMMultiheadAttention(\n",
            "          (out_proj): PHMLayer(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (linear1): PHMLayer(in_features=512, out_features=512, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): PHMLayer(in_features=512, out_features=512, bias=True)\n",
            "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        (dropout3): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (2): PHMTransformerDecoderLayer(\n",
            "        (self_attn): PHMMultiheadAttention(\n",
            "          (out_proj): PHMLayer(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (multihead_attn): PHMMultiheadAttention(\n",
            "          (out_proj): PHMLayer(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (linear1): PHMLayer(in_features=512, out_features=512, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): PHMLayer(in_features=512, out_features=512, bias=True)\n",
            "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        (dropout3): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (3): PHMTransformerDecoderLayer(\n",
            "        (self_attn): PHMMultiheadAttention(\n",
            "          (out_proj): PHMLayer(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (multihead_attn): PHMMultiheadAttention(\n",
            "          (out_proj): PHMLayer(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (linear1): PHMLayer(in_features=512, out_features=512, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): PHMLayer(in_features=512, out_features=512, bias=True)\n",
            "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        (dropout3): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): PHMLayer(in_features=512, out_features=8192, bias=True)\n",
            "  (src_tok_emb): PHMTokenEmbedding(\n",
            "    (embedding): PHMEmbedding(8192, 512)\n",
            "  )\n",
            "  (tgt_tok_emb): PHMTokenEmbedding(\n",
            "    (embedding): PHMEmbedding(8192, 512)\n",
            "  )\n",
            "  (positional_encoding): PositionalEncoding(\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVnO79YW6xxm"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3_RxyywIpXc",
        "outputId": "9c4e0dec-4964-467a-b8ae-2774d752a53a"
      },
      "source": [
        "count_parameters(transformer)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14741848"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KshZB0MBhD_6"
      },
      "source": [
        "Weights and Biases is used to track the experiment. By running this code you can also contribute to our experiment page, or you can track the runs in your own project page"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        },
        "id": "pCZT3IE6ZGx8",
        "outputId": "2b66d1f1-00cc-4641-d403-bdcbd2dc24d2"
      },
      "source": [
        "wandb.init(project='transformer', entity='demegire')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.11.0<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">avid-dream-103</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/demegire/transformer\" target=\"_blank\">https://wandb.ai/demegire/transformer</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/demegire/transformer/runs/2b8b7tmk\" target=\"_blank\">https://wandb.ai/demegire/transformer/runs/2b8b7tmk</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210728_103233-2b8b7tmk</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f4c2f71fdd0>"
            ],
            "text/html": [
              "<h1>Run(2b8b7tmk)</h1><iframe src=\"https://wandb.ai/demegire/transformer/runs/2b8b7tmk\" style=\"border:none;width:100%;height:400px\"></iframe>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMay1qm3ZXJ_"
      },
      "source": [
        "config = wandb.config\n",
        "config.learning_rate = optimizer.state_dict()['param_groups'][0]['lr']\n",
        "config.SRC_VOCAB_SIZE = SRC_VOCAB_SIZE\n",
        "config.TGT_VOCAB_SIZE = TGT_VOCAB_SIZE\n",
        "config.EMB_SIZE = EMB_SIZE\n",
        "config.NHEAD = NHEAD\n",
        "config.FFN_HID_DIM = FFN_HID_DIM\n",
        "config.BATCH_SIZE = BATCH_SIZE\n",
        "config.NUM_ENCODER_LAYERS = NUM_ENCODER_LAYERS\n",
        "config.NUM_DECODER_LAYERS = NUM_DECODER_LAYERS\n",
        "config.num_parameters = count_parameters(transformer)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLA5ReqjHd-h"
      },
      "source": [
        "Our beam search is not the most efficient but it works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wjaBUZ9t66E"
      },
      "source": [
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(device)\n",
        "    src_mask = src_mask.to(device)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n",
        "\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(device)\n",
        "        memory_mask = torch.zeros(ys.shape[0], memory.shape[0]).to(device).type(torch.bool)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                                    .type(torch.bool)).to(device)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim = 1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        \n",
        "        if next_word == EOS_IDX:\n",
        "          break\n",
        "    return ys\n",
        "\n",
        "def beam_search(model, src, src_mask, max_len, start_symbol, beam_size=2, alpha=0.7):\n",
        "  with torch.no_grad():\n",
        "    src = src.to(device)\n",
        "    src_mask = src_mask.to(device)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n",
        "\n",
        "    memory = memory.to(device)\n",
        "    memory_mask = torch.zeros(ys.shape[0], memory.shape[0]).to(device).type(torch.bool)\n",
        "    tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                                      .type(torch.bool)).to(device)\n",
        "    out = model.decode(ys, memory, tgt_mask)\n",
        "    out = out.transpose(0, 1)\n",
        "    probs = model.generator(out[:, -1])\n",
        "\n",
        "    soft_probs = softmax(probs, dim=1)\n",
        "          \n",
        "    tops = torch.topk(soft_probs, beam_size, dim = 1)\n",
        "\n",
        "    sentences = []\n",
        "\n",
        "    for i in range(beam_size):\n",
        "      sentences.append((tops.values[0][i].item(), \n",
        "                        torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(tops.indices[0][i].item())], dim=0)))\n",
        "    while True:      \n",
        "      candidate_sentences = []\n",
        "      eos_index = []\n",
        "\n",
        "      for i in range(beam_size):\n",
        "        eos_flag = False\n",
        "        if sentences[i][1][-1].item() != EOS_IDX:\n",
        "          memory = memory.to(device)\n",
        "          memory_mask = torch.zeros(sentences[i][1].shape[0], memory.shape[0]).to(device).type(torch.bool)\n",
        "          tgt_mask = (generate_square_subsequent_mask(sentences[i][1].size(0))\n",
        "                                            .type(torch.bool)).to(device)\n",
        "          out = model.decode(sentences[i][1], memory, tgt_mask)\n",
        "          out = out.transpose(0, 1)\n",
        "          probs = model.generator(out[:, -1])\n",
        "          soft_probs = softmax(probs, dim=1)\n",
        "\n",
        "          tops = torch.topk(soft_probs, beam_size, dim = 1)\n",
        "\n",
        "          for j in range(beam_size):\n",
        "            candidate_sentences.append((1/(len(torch.cat([sentences[i][1], torch.ones(1, 1).type_as(src.data).fill_(tops.indices[0][j].item())], dim=0))**alpha) * sentences[i][0] * tops.values[0][j].item(),\n",
        "                                        torch.cat([sentences[i][1], torch.ones(1, 1).type_as(src.data).fill_(tops.indices[0][j].item())], dim=0) ))\n",
        "        else:\n",
        "          eos_index.append(i)\n",
        "          eos_flag = True\n",
        "\n",
        "      if eos_flag:\n",
        "        for idx in eos_index:\n",
        "          candidate_sentences.append(sentences[idx])\n",
        "\n",
        "      candidate_sentences.sort(key=lambda x: x[0], reverse=True)\n",
        "      sentences = candidate_sentences[:beam_size]\n",
        "      candidate_sentences = []\n",
        "\n",
        "      end_flag = True\n",
        "      max_flag = False\n",
        "\n",
        "      for idx in range(beam_size):\n",
        "        if sentences[idx][1][-1].item() != EOS_IDX :\n",
        "          end_flag = False\n",
        "\n",
        "      for idx in range(beam_size):\n",
        "        if len(sentences[idx][1]) == max_len:\n",
        "          end_flag = True\n",
        "\n",
        "      if end_flag or max_flag:\n",
        "        return sorted(sentences,key=lambda x: x[0], reverse=True)[0][1]\n",
        "      \n",
        "\n",
        "\n",
        "def translate(model, src, src_vocab, tgt_vocab, src_tokenizer):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    tokens = [BOS_IDX] + [src_vocab.stoi[tok] for tok in src_tokenizer(src)]+ [EOS_IDX]\n",
        "    num_tokens = len(tokens)\n",
        "    src = (torch.LongTensor(tokens).reshape(num_tokens, 1) )\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = beam_search(model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX, beam_size=5, alpha = 0.6).flatten()\n",
        "    #tgt_tokens = greedy_decode(model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten() # greedy decode is faster\n",
        "  return \" \".join([tgt_vocab.itos[tok] for tok in tgt_tokens]).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp1hOx5IHrTM"
      },
      "source": [
        "Be sure to run this line to see if your model works. The model will output nonsense when it is not trained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "CpwLDHwbt66E",
        "outputId": "b228833d-37b2-414b-cea9-57f1ef0ac3df"
      },
      "source": [
        "translate(transformer, \"I have half a mind to hit you before you speak again\", modern_vocab, original_vocab, en_tokenizer)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" waters waters bootless Marry Marry bootless cases cases greatness greatness modesties faintly quicken augmented augmented Draw List'ning List'ning\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTZk0cWWHzcC"
      },
      "source": [
        "Smoothing function is explained in our repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZLle3ri_eh1"
      },
      "source": [
        "smoothie = SmoothingFunction().method4"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIq2lg90Mpjk"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPJTzQVtMKsO"
      },
      "source": [
        "Load the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYI50I9jBHwu"
      },
      "source": [
        "with open('test.original') as f:\n",
        "    test_lines_og = f.readlines()"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAmIKSY_Czhj"
      },
      "source": [
        "with open('test.modern') as g:\n",
        "    test_lines_mo = g.readlines()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyYOBSqXMOSE"
      },
      "source": [
        "There are 575 steps in every epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZDHKgui3SAG",
        "outputId": "f1659aba-3a80-485f-c402-ff2988090894"
      },
      "source": [
        "len(train_iter)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "575"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqWA2ayaMUHY"
      },
      "source": [
        "The model is trained for 9 epochs, notice range(1, num epochs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n6MO9ej3wlr"
      },
      "source": [
        "NUM_EPOCHS = 10"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EUBL4D5t66D",
        "outputId": "873ff0d3-a3b4-4917-9119-cfcbd0e4de5e"
      },
      "source": [
        "wandb.watch(transformer)\n",
        "for epoch in range(1, NUM_EPOCHS):\n",
        "  start_time = time.time()\n",
        "  train_loss = train_epoch(transformer, train_iter, optimizer)\n",
        "  val_loss = evaluate(transformer, valid_iter)\n",
        "  wandb.log({\"val_loss\": val_loss})\n",
        "  end_time = time.time()\n",
        "  scheduler.step()\n",
        "  print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"\n",
        "          f\"Epoch time = {(end_time - start_time):.3f}s\"))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Train loss: 5.859, Val loss: 4.828, Epoch time = 50.009s\n",
            "Epoch: 2, Train loss: 5.002, Val loss: 4.456, Epoch time = 51.349s\n",
            "Epoch: 3, Train loss: 4.667, Val loss: 4.263, Epoch time = 51.361s\n",
            "Epoch: 4, Train loss: 4.431, Val loss: 4.129, Epoch time = 51.691s\n",
            "Epoch: 5, Train loss: 4.229, Val loss: 4.035, Epoch time = 51.552s\n",
            "Epoch: 6, Train loss: 4.048, Val loss: 3.959, Epoch time = 51.531s\n",
            "Epoch: 7, Train loss: 3.882, Val loss: 3.894, Epoch time = 51.690s\n",
            "Epoch: 8, Train loss: 3.728, Val loss: 3.865, Epoch time = 51.559s\n",
            "Epoch: 9, Train loss: 3.581, Val loss: 3.844, Epoch time = 51.668s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510,
          "referenced_widgets": [
            "321b68039d3446bcaf4ad4f74be492dc",
            "609413c75891404dbcb13ecbb67ad69c",
            "c53dfea99a844785b86c3b990bd1d58a",
            "4ffed0daf0db4da7900412ee735ee457",
            "2a9af9b6fc9f42d5b3d67e2c05c6279f",
            "41b0171d39474d88ac61797ec06b71e5",
            "9ff0cbfb452543a2a9e33a2a6c3cad7c",
            "4179f9701091458a86efcfb70b792b0c"
          ]
        },
        "id": "EQDYIHXheSPu",
        "outputId": "11ea412f-7b26-4dcb-aecd-e0a06542e265"
      },
      "source": [
        "wandb.finish()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 255<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "321b68039d3446bcaf4ad4f74be492dc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210728_103233-2b8b7tmk/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210728_103233-2b8b7tmk/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>train_loss</td><td>3.80807</td></tr><tr><td>_runtime</td><td>622</td></tr><tr><td>_timestamp</td><td>1627468975</td></tr><tr><td>_step</td><td>116</td></tr><tr><td>val_loss</td><td>3.84409</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>train_loss</td><td>█▅▄▃▄▄▃▃▃▃▃▃▂▃▄▃▂▃▃▂▂▂▂▂▂▁▂▃▁▂▂▂▂▂▁▁▃▂▁▂</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▅▄▃▂▂▁▁▁</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">avid-dream-103</strong>: <a href=\"https://wandb.ai/demegire/transformer/runs/2b8b7tmk\" target=\"_blank\">https://wandb.ai/demegire/transformer/runs/2b8b7tmk</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIyaL4vpMthT"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxGUNFoXMd9X"
      },
      "source": [
        "Lastly, the model is evaluated on the first 500 lines of the test set, which is approx 1500 lines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVgbN1NQ4xzK",
        "outputId": "020c19a4-024f-44e3-afd9-ce3edbd2e719"
      },
      "source": [
        "start_time = time.time()\n",
        "score = bleu(transformer, test_lines_mo[:500], test_lines_og[:500], modern_vocab, original_vocab, en_tokenizer)\n",
        "end_time = time.time()\n",
        "print(score)\n",
        "print(end_time-start_time)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.171657864712889\n",
            "344.24534487724304\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}